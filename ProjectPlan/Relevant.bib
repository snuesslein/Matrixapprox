
@article{parekh_improved_2017,
	title = {Improved sparse low-rank matrix estimation},
	volume = {139},
	issn = {0165-1684},
	url = {https://www.sciencedirect.com/science/article/pii/S0165168417301494},
	doi = {10.1016/j.sigpro.2017.04.011},
	abstract = {We address the problem of estimating a sparse low-rank matrix from its noisy observation. We propose an objective function consisting of a data-fidelity term and two parameterized non-convex penalty functions. Further, we show how to set the parameters of the non-convex penalty functions, in order to ensure that the objective function is strictly convex. The proposed objective function better estimates sparse low-rank matrices than a convex method which utilizes the sum of the nuclear norm and the ℓ1 norm. We derive an algorithm (as an instance of {ADMM}) to solve the proposed problem, and guarantee its convergence provided the scalar augmented Lagrangian parameter is set appropriately. We demonstrate the proposed method for denoising an audio signal and an adjacency matrix representing protein interactions in the ‘Escherichia coli’ bacteria.},
	pages = {62--69},
	journaltitle = {Signal Processing},
	shortjournal = {Signal Processing},
	author = {Parekh, Ankit and Selesnick, Ivan W.},
	urldate = {2021-11-15},
	date = {2017-10-01},
	langid = {english},
	keywords = {Convex optimization, Low-rank matrix, Non-convex regularization, Sparse matrix, Speech denoising},
	file = {ScienceDirect Snapshot:/home/stephan/snap/zotero-snap/common/Zotero/storage/SX38CPFA/S0165168417301494.html:text/html;Eingereichte Version:/home/stephan/snap/zotero-snap/common/Zotero/storage/RB7K6CF7/Parekh und Selesnick - 2017 - Improved sparse low-rank matrix estimation.pdf:application/pdf},
}

@inproceedings{ulfarsson_sparse_2015,
	title = {Sparse and low rank decomposition using l0 penalty},
	doi = {10.1109/ICASSP.2015.7178584},
	abstract = {High dimensional data is often modeled as a linear combination of a sparse component, a low-rank component, and noise. An example is a video sequence of a busy scene where the background is the low-rank part and the foreground, e.g. moving pedestrians, is the sparse part. Sparse and low rank ({SLR}) matrix decomposition is a recent method that estimates those components. In this paper we develop an l0 based {SLR} method and an associated tuning parameter selection method based on the extended Bayesian information criterion ({EBIC}) method. In simulations the new algorithm is compared with state of the art algorithms from the literature.},
	eventtitle = {2015 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	pages = {3312--3316},
	booktitle = {2015 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	author = {Ulfarsson, M.O. and Solo, V. and Marjanovic, G.},
	date = {2015-04},
	note = {{ISSN}: 2379-190X},
	keywords = {Matrix decomposition, Sparse matrices, Cyclic Descent, Extended {BIC}, Indexes, l0 penalty, Noise, Noise measurement, Principal component analysis, Sparse and Low Rank Matrix Decomposition, Tuning},
	file = {IEEE Xplore Abstract Record:/home/stephan/snap/zotero-snap/common/Zotero/storage/TKYI37MD/7178584.html:text/html;Ulfarsson et al. - 2015 - Sparse and low rank decomposition using l0 penalty.pdf:/home/stephan/snap/zotero-snap/common/Zotero/storage/IX4ADKG8/Ulfarsson et al. - 2015 - Sparse and low rank decomposition using l0 penalty.pdf:application/pdf},
}
@article{martin_implicit_2018,
	title = {Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning},
	url = {http://arxiv.org/abs/1810.01075},
	shorttitle = {Implicit Self-Regularization in Deep Neural Networks},
	abstract = {Random Matrix Theory ({RMT}) is applied to analyze weight matrices of Deep Neural Networks ({DNNs}), including both production quality, pre-trained models such as {AlexNet} and Inception, and smaller models trained from scratch, such as {LeNet}5 and a miniature-{AlexNet}. Empirical and theoretical results clearly indicate that the {DNN} training process itself implicitly implements a form of Self-Regularization. The empirical spectral density ({ESD}) of {DNN} layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of explicit regularization. Building on relatively recent results in {RMT}, most notably its extension to Universality classes of Heavy-Tailed matrices, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization. These phases can be observed during the training process as well as in the final learned {DNNs}. For smaller and/or older {DNNs}, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a "size scale" separating signal from noise. For state-of-the-art {DNNs}, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems. This results from correlations arising at all size scales, which arises implicitly due to the training process itself. This implicit Self-Regularization can depend strongly on the many knobs of the training process. By exploiting the generalization gap phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size. This demonstrates that---all else being equal---{DNN} optimization with larger batch sizes leads to less-well implicitly-regularized models, and it provides an explanation for the generalization gap phenomena.},
	journaltitle = {{arXiv}:1810.01075 [cs, stat]},
	author = {Martin, Charles H. and Mahoney, Michael W.},
	urldate = {2021-11-09},
	date = {2018-10-02},
	eprinttype = {arxiv},
	eprint = {1810.01075},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/38UWI6K6/Martin und Mahoney - 2018 - Implicit Self-Regularization in Deep Neural Networ.pdf:application/pdf;arXiv.org Snapshot:/home/stephan/snap/zotero-snap/common/Zotero/storage/7Q3PLBAT/1810.html:text/html;1810.01075.pdf:/home/stephan/snap/zotero-snap/common/Zotero/storage/YQ4HREQU/1810.01075.pdf:application/pdf},
}
@article{chandrasekaran_sparse_2009,
	title = {Sparse and Low-Rank Matrix Decompositions},
	volume = {42},
	issn = {1474-6670},
	url = {https://www.sciencedirect.com/science/article/pii/S1474667016388632},
	doi = {10.3182/20090706-3-FR-2004.00249},
	series = {15th {IFAC} Symposium on System Identification},
	abstract = {Suppose we are given a matrix that is formed by adding an unknown sparse matrix to an unknown low-rank matrix. Our goal is to decompose the given matrix into its sparse and low-rank components. Such a problem arises in a number of applications in model and system identification, but obtaining an exact solution is {NP}-hard in general. In this paper we consider a convex optimization formulation to splitting the specified matrix into its components; in fact our approach reduces to solving a semidefinite program. We provide sufficient conditions that guarantee exact recovery of the components by solving the semidefinite program. We also show that when the sparse and low-rank matrices are drawn from certain natural random ensembles, these sufficient conditions are satisfied with high probability. We conclude with simulation results on synthetic matrix decomposition problems.},
	pages = {1493--1498},
	number = {10},
	journaltitle = {{IFAC} Proceedings Volumes},
	shortjournal = {{IFAC} Proceedings Volumes},
	author = {Chandrasekaran, Venkat and Sanghavi, Sujay and Parrilo, Pablo A. and Willsky, Alan S.},
	urldate = {2021-11-15},
	date = {2009-01-01},
	langid = {english},
	file = {ScienceDirect Full Text PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/NEHZA5HV/Chandrasekaran et al. - 2009 - Sparse and Low-Rank Matrix Decompositions.pdf:application/pdf;ScienceDirect Snapshot:/home/stephan/snap/zotero-snap/common/Zotero/storage/PSM4XQFU/S1474667016388632.html:text/html},
}

@article{ong_beyond_2016,
	title = {Beyond Low Rank + Sparse: Multiscale Low Rank Matrix Decomposition},
	volume = {10},
	issn = {1941-0484},
	doi = {10.1109/JSTSP.2016.2545518},
	shorttitle = {Beyond Low Rank + Sparse},
	abstract = {We present a natural generalization of the recent low rank + sparse matrix decomposition and consider the decomposition of matrices into components of multiple scales. Such decomposition is well motivated in practice as data matrices often exhibit local correlations in multiple scales. Concretely, we propose a multiscale low rank modeling that represents a data matrix as a sum of block-wise low rank matrices with increasing scales of block sizes. We then consider the inverse problem of decomposing the data matrix into its multiscale low rank components and approach the problem via a convex formulation. Theoretically, we show that under various incoherence conditions, the convex program recovers the multiscale low rank components either exactly or approximately. Practically, we provide guidance on selecting the regularization parameters and incorporate cycle spinning to reduce blocking artifacts. Experimentally, we show that the multiscale low rank decomposition provides a more intuitive decomposition than conventional low rank methods and demonstrate its effectiveness in four applications, including illumination normalization for face images, motion separation for surveillance videos, multiscale modeling of the dynamic contrast enhanced magnetic resonance imaging, and collaborative filtering exploiting age information.},
	pages = {672--687},
	number = {4},
	journaltitle = {{IEEE} Journal of Selected Topics in Signal Processing},
	author = {Ong, Frank and Lustig, Michael},
	date = {2016-06},
	note = {Conference Name: {IEEE} Journal of Selected Topics in Signal Processing},
	keywords = {Data models, Convex Relaxation, Correlation, Face, Low Rank Modeling, Matrix decomposition, Multi-scale Modeling, Signal Decomposition, Sparse matrices, Structured Matrix, Transforms, Videos},
	file = {IEEE Xplore Full Text PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/QVP6LF3E/Ong und Lustig - 2016 - Beyond Low Rank + Sparse Multiscale Low Rank Matr.pdf:application/pdf;IEEE Xplore Abstract Record:/home/stephan/snap/zotero-snap/common/Zotero/storage/2QTYFERC/7439735.html:text/html},
}
