%================================================================================
%=============================== DOCUMENT SETUP =================================
%================================================================================

%\documentclass[lang=ngerman,inputenc=utf8,fontsize=10pt]{ldvarticle}
\documentclass[lang=ngerman,inputenc=utf8,fontsize=10pt]{ldvarticle}
%PACKAGES

\usepackage{parskip}
\usepackage{subfigure}
\usepackage{ifthen}
\usepackage{comment}
\usepackage{color}
\usepackage{colortbl}
\usepackage{soul}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usepackage{tabularx}
\usepackage{lipsum}
\usepackage{pgfgantt}
\usepackage{amsmath}

\usepackage{color}

\usepackage[backend=biber,style=ieee,
doi=false,isbn=false,url=false,eprint=false]{biblatex}
\addbibresource{Relevant.bib}

\definecolor{lightgray}{rgb}{0.75,0.75,0.75}

\input{kalender.tex}

\usetikzlibrary{matrix}


\DeclareMathOperator{\vect}{vec}
\DeclareMathOperator{\rank}{rank}

%================================================================================
%================================= TITLE PAGE ===================================
%================================================================================

\title{Combining Matrix Representations for Structured Approximation of Neural Network Weight Matrices}
\subtitle{Projectplan}
\author{Stepahn Nüßlein}

\date{\today}

\begin{document}


	\maketitle
	\thispagestyle{empty}
	\vspace*{2cm}
	\hrule

\section*{Motivation}

As the matrices for fully connected layers get larger the evaluation of the neuronal nets get computationally more expensive. This is often problematic as the computational resources are a limiting factor, especially on embedded or mobile systems.
Therefore different approaches to reduce the computational cost are being explored.

A possible representation are Sequentially Semiseperable Matrices. These have a certain underling structure.
The structure of the sequentially semiseperabel matrix is often informed by a underlying physical system or the construction of the matrix.
The matrices in Neuronal Networks do not necessarily have the same underling structure.
Even if the Matrix is close the to a Sequentially Semiseperable Matrix the structural Parameters might not be know.

To increase the number of Matrices that can be represented we also allow combinations of Sequentially Semiseperable Matrices.

To represent the matrices, possible algorithms to derive the representation are investigated and tested.
%To represent the matrices, an algorithm to derive the representation is needed.
	%\item Both approxiamtion of Matrix in some Norm ($\|\|_F$,$\|\|_1$,$\|\|_\infty$ or similar) also Accuracy on Data set




\vspace*{1cm}
\hrule

\newpage

\section{Project Description}

\subsection*{Goals}

\begin{itemize}

	\item\textbf{Matrix approximation with reduced cost} Find a Matrix approximation. This requires a formal definition, as well as a reference implementation.
	\item\textbf{Algorithm to construct said approximation} Development of an algorithm to calculate the approximation. Also implement an reference implementation
	\item\textbf{Theoretical description of description} Compile the theoretical properties that can be derived.
	\item\textbf{Evaluate approximation in practical examples} Evaluate the performance on weight matrices from pretrained models form Pytorch. This includes the behaviour of the approximation algorithm, the computational cost as well as the performance of the neuronal net
\end{itemize}





\subsection*{Approach}

These assumptions and properties are underlying the idea:
\begin{itemize}
	\item Matrices in fully connected layers often have full rank. This means that a rank reduction is not easily possible (with some caveats, as it is not certain that this is the relevant ingredient) \cite{martin_implicit_2018}
	\item To get the optimal trade of between accuracy and cost it should be possible to adapt the accuracy of the approximation by changing hyper parameters of the approximation.
\end{itemize}

State-Space representation have many hyperparameters like input and output dimensions, these have to be chosen. By setting the input and output dimensions representation we fix an segmentation of the representable matrices.

The matrices in a neuronal net might not have an underling structure that can be represented.
Therefore it might be interesting if the Matrices in an neural net can be better described with combinations of different representations.
This might be the sum of state-space representations. These might have the same or different input and output dimensions.

It might be also interesting to use permutations of the input and output to obtain segments with low rank.

A similar representation with products of SSS Representations with non matching output and input dimensions might also be possible, but is harder to describe.

\subsubsection*{Segmentation}
One prerequisite is finding appropriate segmentation of the Matrix. This either includes finding a existing way of finding a segmentation and implementing it.
Or coming up with a an algorithm to segment a matrix.

\subsubsection*{Computational cost}
The computational cost of evaluation a matrix vector product is related to the number of parameters.
For the SSS structure the number of multiplications is equal to the number of nonzero entries in the A,B,C and D matrices. These depend on the dimensions of the input and output and on the number of states.

\subsubsection*{Properties}
The properties of the representations should be tested in terms of computational cost, storage requirements and performance in neuronal nets.
The computational stability of the resulting algorithm might be tested numerically. This might be for further interest if the evaluation is done on an 32-bit architecture.

\section*{Some Results}

\subsection*{Addition of Systems}
One Idea was that the sum of two systems with different input and output dimensions might be able to represent a wider class of matrices.
It is questionable that this class is far bigger as the relation 

\begin{equation}
	\rank(A) \leq \rank(H_1) + \rank(H_2) 
\end{equation}
holds true for all sub matrices that do not contain $D$-matrices.
There might be some benefits in the close vicinity to the diagonal but it is unclear if this is relevant.

TODO: question: decompose same dims into towo systems -> parallel computational cost?

\subsection*{Changes to size of the input and output dimensions}
I already programmed code to change the size of the inputs and outputs. The code also preserves minimality.
In some cases this results in a increasing/decreasing state dimensions. This depends on the parts of $D$ that are included or excluded. 
This does not depend on the rest of the matrix (Similar to the shift invariance principal).
This might be different for cases where an later approximation is done.

\section{Some Questions that came up}

\subsection*{Approximation}
An approximation of an system can be done using the hankel rank approximation. It is not clear how to represent an general Matrix using an aproximated system. 
TODO: read about hankel rank approximation.
If the implementation is balanced and the states are ordered with respect to the according $\sigma$s the state dimension can be reduced by simply dropping columns and rows from the matrices.   

\subsection*{Importance of $D$}
As there is no restriction of the $D$-Matrix it is quite versatile.
It might be possible to permute them in a way that reveals a compatible structure.

\section{Further Ideas:}

\subsection*{Segmentation of Matrix}
Derive an first informed guess on the segmentation based on a model of the expected hankerrank. This might first include a model with constant input and output dimensions. 
%Maybe later different dimenstions
After this move boundaries if this gives an computational advantage.

\subsection*{Derive Permuataions}
Use algorithm from todo to get a sparse + low rank representation.
Then group the sparse elements and move them into the $D$ matrices.
This also results in a segmentation (the order is still not determined)
And the represent the permuted matrix using a system 


%\subsubsection*{Calculation of Representation}
%There are two possible ideas to get the wanted representation:
%
%\paragraph{Optimization Problem}
%Convert the given properties into manifolds and objective functions and use optimization techniques.
%This might be easier to define, but will possibly result in a problem with a very high number of dimensions. Maybe it is possible to express it as some standard problem, then it would be possible to use standard solvers and use existing guarantees and runtime estimates.
%
%\paragraph{Iterative Methods}
%Use an approach similar to Matching pursuit. Here the dictionary $D$ would be a subset of all Sequentially Semiseperable Matrices. This requires some description of a qasi-inner-product and an way to find an approximation of the closest element in $D$.
%As $D$ will have infinitely many dimensions, this might be quite involved.



\section{Workpackages}

\begin{itemize}
	\item \textbf{Recherche:}
	\begin{itemize}
			\item \textbf{Existing Decompositions:} Low Rank+Sparse, Multiscale Low rank \cite{chandrasekaran_sparse_2009,ong_beyond_2016,yu_compressing_2017}
			\item \textbf{Sparsity:} Appropriate Sparsity Measures \cite{ulfarsson_sparse_2015,parekh_improved_2017}
			\item \textbf{Existing Algorithms:} Look into the structure of related algorithms
		\end{itemize}
	\item \textbf{Evaluierung:} Analyse der bisherigen Recherce
		\begin{itemize}
			\item \textbf{Auswahl:} choose appropriate assumptions
			\item \textbf{Theoretical considerations:} Try the theoretical questiosn outlined above.
		\end{itemize}
	\item \textbf{Implementierung:} Implement Algorithm
		\begin{itemize}
			\item \textbf{Implement Algorithm(s):} Based on previous exploration implement tests and algorithms to construct the representation.
			\item \textbf{Generate Tests:} Generate Testsdata and pipeline to test them, including meta-analysis (speed of convergence...)
		\end{itemize}
	\item \textbf{Analyse:} Entwurf und Durchführung praxisnaher Tests
		\begin{itemize}
			\item \textbf{Setup Test}
			\item \textbf{Evaluation I:} Evaluation on tailor-made examples: Combiantion of Sequentially Semiseperable Matricies %(I think we need more than $\min(n,m)$ rank sub matrices until the standard SVD not be able to recover the structure?, but even then it might be interesting, as we can reduce the computations)
			\item \textbf{Evaluation II:} Random matrices with different parameters
			\item \textbf{Evaluation III:} Evaluation on AI matrices
		\end{itemize}
	\item \textbf{Auswertung und Diskussion:} Ergebnisse zusammentragen und vergleichen
		\begin{itemize}
			\item \textbf{Revisiting the Theory:} Explanations for experienced behavior?
			\item \textbf{Description of Performance:} Beschreibung der durchgeführten Tests und Visualisierung der Ergebnisse
			\item \textbf{Diskussion:} Auswertung der Ergebnisse und kritische Betrachtung des Nutzens für das Gesamtsystem
		\end{itemize}
	\item \textbf{Ausarbeitung:} Abschließende schriftliche Darstellung der durchgeführten Arbeiten
\end{itemize}

\section{Time Table}



	\begin{tikzpicture}
	\GanttHeader{2021-11-21}{2022-05-29}{\textwidth}


	\Task{1}{Recherchen}{0}{4}
	\Task{2}{Evaluierung}{4}{3}
	\Task{3}{Implementierung}{7}{6}
	\Task{4}{Analyse}{11}{6}
	\Task{5}{Auswertung}{17}{5}
	\Task{6}{Ausarbeitung}{20}{7}
	\end{tikzpicture}






\section{Risk Analysis}

Computation effort might not be lower than the effort for regular matrix vector product.
\newline\textbf{Likelihood:} Risk is hard to determine upfront as it depends on how many fast the approximation converges.
\newline\textbf{Mitigation:} Test practical examples at an early stage of the thesis and.

Unable to find an appropriate algorithm
\newline\textbf{Likelihood:} Quite likely as it is a very underdetermined problem with many parameters
\newline\textbf{Mitigation:} use more computation-time, reduction of degrees of freedom by requiring some artificial condition (e.g. set the structure of the submatrices)


\printbibliography

\end{document}
