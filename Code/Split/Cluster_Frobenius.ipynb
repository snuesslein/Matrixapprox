{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering collums and rows\n",
    "\n",
    "The idea is to cluster collumns and rows\n",
    "\n",
    "Here we have the objective function \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tvsclib.utils as utils\n",
    "import Split\n",
    "import matplotlib.pyplot as plt\n",
    "from tvsclib.strict_system import StrictSystem\n",
    "\n",
    "from tvsclib.approximation import Approximation\n",
    "\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import scipy.stats \n",
    "\n",
    "import graphs\n",
    "\n",
    "import scipy.linalg as linalg\n",
    "\n",
    "import plot_permutations as perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mobilenet_target_mats():\n",
    "    target_mats = []\n",
    "    # Load the model\n",
    "    model = models.mobilenet_v2(pretrained=True)\n",
    "    # Put moel into eval mode\n",
    "    model.eval()\n",
    "    for layer in model.classifier:\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "            # Obtain the weights of this layer\n",
    "            weights = layer.weight.detach().numpy()\n",
    "            target_mats.append(weights)\n",
    "    return target_mats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.random.rand(64,64)\n",
    "#T = mats = get_mobilenet_target_mats()[0]\n",
    "sys = Split.initial_mixed(T)\n",
    "utils.show_system(sys,mark_D=False)\n",
    "utils.check_dims(sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X,s_col,s_row):\n",
    "    return 0\n",
    "\n",
    "def compute_sigmasf(X,s_col,s_row):\n",
    "    return np.linalg.svd(X[s_row][:,s_col],compute_uv=False), np.linalg.svd(X[~s_row][:,~s_col],compute_uv=False)\n",
    "\n",
    "def show_matrices(X,s_col,s_row):\n",
    "    plt.matshow(X[s_row][:,s_col],fignum=1)\n",
    "\n",
    "    plt.matshow(X[~s_row][:,~s_col],fignum=2)\n",
    "\n",
    "f_reg_row = lambda x: -gamma*x**3\n",
    "f_reg_col = lambda x: -gamma*x**3\n",
    "\n",
    "gamma = 1e4 #maxbee have two different regularizations for rows and collumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def segment_matrix(stage_causal,stage_anticausal,N=70,initla_spectral=True):\n",
    "    \n",
    "    Ac = stage_causal.A_matrix\n",
    "    Bc = stage_causal.B_matrix\n",
    "    Cc = stage_causal.C_matrix\n",
    "\n",
    "    Aa = stage_anticausal.A_matrix\n",
    "    Ba = stage_anticausal.B_matrix\n",
    "    Ca = stage_anticausal.C_matrix\n",
    "\n",
    "    D = stage_causal.D_matrix\n",
    "    \n",
    "    #regularization vector\n",
    "    v_reg_col = f_reg_col(np.linspace(-1,1,D.shape[1]))\n",
    "    v_reg_row = f_reg_row(np.linspace(-1,1,D.shape[0]))\n",
    "\n",
    "    #dims of states\n",
    "    (d_out_c,d_in_c) = Ac.shape\n",
    "    (d_out_a,d_in_a) = Aa.shape\n",
    "\n",
    "    #setup matrix\n",
    "    X = np.block([[np.zeros((d_out_a,d_in_c)),Ba,Aa ],\n",
    "                  [Cc,D,Ca],\n",
    "                  [Ac,Bc,np.zeros((d_out_c,d_in_a))]\n",
    "    \n",
    "    ])\n",
    "\n",
    "    if initla_spectral:\n",
    "        #get initial using spectral \n",
    "        s_c = graphs.segment_matrix(X)\n",
    "        if np.count_nonzero(~s_c[:d_in_c])+np.count_nonzero(s_c[X.shape[1]-d_in_a:]) > (d_in_a+d_in_c)/2:\n",
    "            # if more fixed nodes are incorrect flip\n",
    "            s_c = ~s_c\n",
    "            \n",
    "        #set the fixed\n",
    "        s_c[:d_in_c]=1\n",
    "        s_c[X.shape[1]-d_in_a:]=0\n",
    "        \n",
    "        s_r = graphs.segment_matrix(X.T)\n",
    "        if np.count_nonzero(s_r[:d_out_a])+np.count_nonzero(~s_r[X.shape[0]-d_out_c:]) > (d_out_a+d_out_c)/2:\n",
    "            # if more fixed nodes are incorrect flip\n",
    "            s_r = ~s_r\n",
    "        s_r[:d_out_a]=0\n",
    "        s_r[X.shape[0]-d_out_c:]=1\n",
    "    else:\n",
    "        #initialize segmentation\n",
    "        s_c = np.zeros(X.shape[1],dtype=bool)\n",
    "        s_c[:d_in_c+D.shape[1]//2]=1\n",
    "        s_r = np.zeros(X.shape[0],dtype=bool)\n",
    "        s_r[d_out_a+D.shape[0]//2:]=1\n",
    "    \n",
    "    \n",
    "    fs = np.zeros(N+1)\n",
    "    s_cols=np.zeros((N+1,X.shape[1]),dtype=bool) \n",
    "    s_rows=np.zeros((N+1,X.shape[0]),dtype=bool) \n",
    "    \n",
    "\n",
    "    s_cols[0]=s_c\n",
    "    s_rows[0]=s_r\n",
    "    fs[0]=f(X,s_c,s_r)\n",
    "    \n",
    "    for n in range(N):\n",
    "        #for test \n",
    "        normalize =\"F\"\n",
    "        \n",
    "        #columns:\n",
    "        X_t = X[~s_r]\n",
    "        X_b = X[s_r]\n",
    "        \n",
    "        n_xt =np.linalg.norm(X_t,axis=0)\n",
    "        n_xb =np.linalg.norm(X_b,axis=0)\n",
    "        \n",
    "        #weights unnormalized\n",
    "        W_t = n_xt.reshape(-1,1)*n_xt.reshape(1,-1)-np.abs(X_t.T@X_t)\n",
    "        W_b = n_xb.reshape(-1,1)*n_xb.reshape(1,-1)-np.abs(X_b.T@X_b)\n",
    "        \n",
    "        #nomalize\n",
    "        if normalize ==\"F\": #Forbenius norm\n",
    "            W_t = W_t/(n_xt.reshape(-1,1)*n_xt.reshape(1,-1))\n",
    "            W_b = W_b/(n_xb.reshape(-1,1)*n_xb.reshape(1,-1))\n",
    "        elif normalize ==\"L\": #length\n",
    "            W_t = W_t/(np.count_nonzero(~s_r)**2)\n",
    "            W_b = W_b/(np.count_nonzero(s_r)**2)\n",
    "        elif normalize ==\"M\": #mixed: reference with norm, current with length\n",
    "            W_t = W_t/(n_xt.reshape(-1,1)*np.count_nonzero(~s_r))#\n",
    "            W_b = W_b/(n_xb.reshape(-1,1)*np.count_nonzero(s_r))\n",
    "            \n",
    "        #rows:\n",
    "        X_r = X[:,~s_c]\n",
    "        X_l = X[:,s_c]\n",
    "        \n",
    "        n_xr = np.linalg.norm(X_r,axis=1)\n",
    "        n_xl = np.linalg.norm(X_l,axis=1)\n",
    "        \n",
    "        #Weights unnormalized\n",
    "        W_r =n_xr.reshape(-1,1)*n_xr.reshape(1,-1)-np.abs(X_r@X_r.T)\n",
    "        W_l =n_xl.reshape(-1,1)*n_xl.reshape(1,-1)-np.abs(X_l@X_l.T)\n",
    "        \n",
    "        #normlaize\n",
    "        if normalize ==\"F\": #Forbenius norm\n",
    "            W_r = W_r/(n_xr.reshape(-1,1)*n_xr.reshape(1,-1))\n",
    "            W_l = W_l/(n_xl.reshape(-1,1)*n_xl.reshape(1,-1))\n",
    "        elif normalize ==\"L\": #length\n",
    "            W_r = W_r/(np.count_nonzero(~s_c)**2)#\n",
    "            W_l = W_l/(np.count_nonzero(s_c)**2)\n",
    "        elif normalize ==\"M\": #mixed: reference with norm, current with length\n",
    "            W_r = W_r/(n_xr.reshape(-1,1)*np.count_nonzero(~s_c))#\n",
    "            W_l = W_l/(n_xl.reshape(-1,1)*np.count_nonzero(s_c))\n",
    "\n",
    "        S_col = np.sum(W_t[~s_c],axis=0) -np.sum(W_b[s_c],axis=0)\n",
    "        S_row = np.sum(W_r[~s_r],axis=0) -np.sum(W_l[s_r],axis=0)\n",
    "        \n",
    "        ord_c = d_in_c +np.argsort(S_col[d_in_c:X.shape[1]-d_in_a])\n",
    "        ord_r = d_out_a+np.argsort(S_row[d_out_a:X.shape[0]-d_out_c])\n",
    "        \n",
    "        \n",
    "        v_c = ord_c[v_reg_col<S_col[ord_c]]\n",
    "        v_r = ord_r[v_reg_row<S_row[ord_r]]\n",
    "        \n",
    "        s_c[d_in_c:s_c.size-d_in_a]=0\n",
    "        s_r[d_out_a:s_r.size-d_out_c]=0\n",
    "        \n",
    "        s_c[v_c] = 1\n",
    "        s_r[v_r] = 1\n",
    "        \n",
    "        \n",
    "        fs[n+1] = f(X,s_c,s_r)\n",
    "        s_cols[n+1]=s_c\n",
    "        s_rows[n+1]=s_r\n",
    "        \n",
    "        if np.any(np.logical_and(np.all(s_cols[:n+1]==s_c,axis=1),np.all(s_rows[:n+1]==s_r,axis=1))):\n",
    "            print(\"converged at n=\",n)\n",
    "            break\n",
    "        \n",
    "    report ={\"s_cols\":s_cols[:n+2],\"s_rows\":s_rows[:n+2],\"X\":X}\n",
    "        \n",
    "    return s_c[d_in_c:X.shape[1]-d_in_a],s_r[d_out_a:X.shape[0]-d_out_c],report\n",
    "        \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def segment_matrix(stage_causal,stage_anticausal,N=70,initla_spectral=True):#maximize angle\n",
    "    \n",
    "    Ac = stage_causal.A_matrix\n",
    "    Bc = stage_causal.B_matrix\n",
    "    Cc = stage_causal.C_matrix\n",
    "\n",
    "    Aa = stage_anticausal.A_matrix\n",
    "    Ba = stage_anticausal.B_matrix\n",
    "    Ca = stage_anticausal.C_matrix\n",
    "\n",
    "    D = stage_causal.D_matrix\n",
    "    \n",
    "    #regularization vector\n",
    "    v_reg_col = f_reg_col(np.linspace(-1,1,D.shape[1]))\n",
    "    v_reg_row = f_reg_row(np.linspace(-1,1,D.shape[0]))\n",
    "\n",
    "    #dims of states\n",
    "    (d_out_c,d_in_c) = Ac.shape\n",
    "    (d_out_a,d_in_a) = Aa.shape\n",
    "\n",
    "    #setup matrix\n",
    "    X = np.block([[np.zeros((d_out_a,d_in_c)),Ba,Aa ],\n",
    "                  [Cc,D,Ca],\n",
    "                  [Ac,Bc,np.zeros((d_out_c,d_in_a))]\n",
    "    \n",
    "    ])\n",
    "\n",
    "    if initla_spectral:\n",
    "        #get initial using spectral \n",
    "        s_c = graphs.segment_matrix(X)\n",
    "        if np.count_nonzero(~s_c[:d_in_c])+np.count_nonzero(s_c[X.shape[1]-d_in_a:]) > (d_in_a+d_in_c)/2:\n",
    "            # if more fixed nodes are incorrect flip\n",
    "            s_c = ~s_c\n",
    "            \n",
    "        #set the fixed\n",
    "        s_c[:d_in_c]=1\n",
    "        s_c[X.shape[1]-d_in_a:]=0\n",
    "        \n",
    "        s_r = graphs.segment_matrix(X.T)\n",
    "        if np.count_nonzero(s_r[:d_out_a])+np.count_nonzero(~s_r[X.shape[0]-d_out_c:]) > (d_out_a+d_out_c)/2:\n",
    "            # if more fixed nodes are incorrect flip\n",
    "            s_r = ~s_r\n",
    "        s_r[:d_out_a]=0\n",
    "        s_r[X.shape[0]-d_out_c:]=1\n",
    "    else:\n",
    "        #initialize segmentation\n",
    "        s_c = np.zeros(X.shape[1],dtype=bool)\n",
    "        s_c[:d_in_c+D.shape[1]//2]=1\n",
    "        s_r = np.zeros(X.shape[0],dtype=bool)\n",
    "        s_r[d_out_a+D.shape[0]//2:]=1\n",
    "    \n",
    "    \n",
    "    fs = np.zeros(N+1)\n",
    "    s_cols=np.zeros((N+1,X.shape[1]),dtype=bool) \n",
    "    s_rows=np.zeros((N+1,X.shape[0]),dtype=bool) \n",
    "    \n",
    "\n",
    "    s_cols[0]=s_c\n",
    "    s_rows[0]=s_r\n",
    "    fs[0]=f(X,s_c,s_r)\n",
    "    \n",
    "    for n in range(N):\n",
    "        #for test \n",
    "        normalize =\"F\"\n",
    "        \n",
    "        #columns:\n",
    "        X_t = X[~s_r]\n",
    "        X_b = X[s_r]\n",
    "        \n",
    "        n_xt =np.linalg.norm(X_t,axis=0)\n",
    "        n_xb =np.linalg.norm(X_b,axis=0)\n",
    "        \n",
    "        #weights unnormalized\n",
    "        W_t = n_xt.reshape(-1,1)*n_xt.reshape(1,-1)-np.abs(X_t.T@X_t)\n",
    "        W_b = n_xb.reshape(-1,1)*n_xb.reshape(1,-1)-np.abs(X_b.T@X_b)\n",
    "        \n",
    "        #nomalize\n",
    "        if normalize ==\"F\": #Forbenius norm\n",
    "            W_t = W_t/(n_xt.reshape(-1,1)*n_xt.reshape(1,-1))\n",
    "            W_b = W_b/(n_xb.reshape(-1,1)*n_xb.reshape(1,-1))\n",
    "        elif normalize ==\"L\": #length\n",
    "            W_t = W_t/(np.count_nonzero(~s_r)**2)\n",
    "            W_b = W_b/(np.count_nonzero(s_r)**2)\n",
    "        elif normalize ==\"M\": #mixed: reference with norm, current with length\n",
    "            W_t = W_t/(n_xt.reshape(-1,1)*np.count_nonzero(~s_r))#\n",
    "            W_b = W_b/(n_xb.reshape(-1,1)*np.count_nonzero(s_r))\n",
    "            \n",
    "        #rows:\n",
    "        X_r = X[:,~s_c]\n",
    "        X_l = X[:,s_c]\n",
    "        \n",
    "        n_xr = np.linalg.norm(X_r,axis=1)\n",
    "        n_xl = np.linalg.norm(X_l,axis=1)\n",
    "        \n",
    "        #Weights unnormalized\n",
    "        W_r =n_xr.reshape(-1,1)*n_xr.reshape(1,-1)-np.abs(X_r@X_r.T)\n",
    "        W_l =n_xl.reshape(-1,1)*n_xl.reshape(1,-1)-np.abs(X_l@X_l.T)\n",
    "        \n",
    "        #normlaize\n",
    "        if normalize ==\"F\": #Forbenius norm\n",
    "            W_r = W_r/(n_xr.reshape(-1,1)*n_xr.reshape(1,-1))\n",
    "            W_l = W_l/(n_xl.reshape(-1,1)*n_xl.reshape(1,-1))\n",
    "        elif normalize ==\"L\": #length\n",
    "            W_r = W_r/(np.count_nonzero(~s_c)**2)#\n",
    "            W_l = W_l/(np.count_nonzero(s_c)**2)\n",
    "        elif normalize ==\"M\": #mixed: reference with norm, current with length\n",
    "            W_r = W_r/(n_xr.reshape(-1,1)*np.count_nonzero(~s_c))#\n",
    "            W_l = W_l/(n_xl.reshape(-1,1)*np.count_nonzero(s_c))\n",
    "\n",
    "        S_col = np.sum(W_t[~s_c],axis=0) -np.sum(W_b[s_c],axis=0)\n",
    "        S_row = np.sum(W_r[~s_r],axis=0) -np.sum(W_l[s_r],axis=0)\n",
    "        \n",
    "        ord_c = d_in_c +np.argsort(S_col[d_in_c:X.shape[1]-d_in_a])\n",
    "        ord_r = d_out_a+np.argsort(S_row[d_out_a:X.shape[0]-d_out_c])\n",
    "        \n",
    "        \n",
    "        v_c = ord_c[v_reg_col>S_col[ord_c]]\n",
    "        v_r = ord_r[v_reg_row>S_row[ord_r]]\n",
    "        \n",
    "        s_c[d_in_c:s_c.size-d_in_a]=0\n",
    "        s_r[d_out_a:s_r.size-d_out_c]=0\n",
    "        \n",
    "        s_c[v_c] = 1\n",
    "        s_r[v_r] = 1\n",
    "        \n",
    "        \n",
    "        fs[n+1] = f(X,s_c,s_r)\n",
    "        s_cols[n+1]=s_c\n",
    "        s_rows[n+1]=s_r\n",
    "        \n",
    "        if np.any(np.logical_and(np.all(s_cols[:n+1]==s_c,axis=1),np.all(s_rows[:n+1]==s_r,axis=1))):\n",
    "            print(\"converged at n=\",n)\n",
    "            break\n",
    "        \n",
    "    report ={\"s_cols\":s_cols[:n+2],\"s_rows\":s_rows[:n+2],\"X\":X}\n",
    "        \n",
    "    return s_c[d_in_c:X.shape[1]-d_in_a],s_r[d_out_a:X.shape[0]-d_out_c],report\n",
    "        \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def w(s):\n",
    "    return s>1.5#np.logical_and(s>3,s<5)\n",
    "\n",
    "def segment_matrix(stage_causal,stage_anticausal,N=200,initla_spectral=True):\n",
    "    \n",
    "    Ac = stage_causal.A_matrix\n",
    "    Bc = stage_causal.B_matrix\n",
    "    Cc = stage_causal.C_matrix\n",
    "\n",
    "    Aa = stage_anticausal.A_matrix\n",
    "    Ba = stage_anticausal.B_matrix\n",
    "    Ca = stage_anticausal.C_matrix\n",
    "\n",
    "    D = stage_causal.D_matrix\n",
    "    \n",
    "    #regularization vector\n",
    "    v_reg_col = f_reg_col(np.linspace(-1,1,D.shape[1]))\n",
    "    v_reg_row = f_reg_row(np.linspace(-1,1,D.shape[0]))\n",
    "\n",
    "    #dims of states\n",
    "    (d_out_c,d_in_c) = Ac.shape\n",
    "    (d_out_a,d_in_a) = Aa.shape\n",
    "\n",
    "    #setup matrix\n",
    "    X = np.block([[np.zeros((d_out_a,d_in_c)),Ba,Aa ],\n",
    "                  [Cc,D,Ca],\n",
    "                  [Ac,Bc,np.zeros((d_out_c,d_in_a))]\n",
    "    \n",
    "    ])\n",
    "\n",
    "    if initla_spectral:\n",
    "        #get initial using spectral \n",
    "        s_c = graphs.segment_matrix(X)\n",
    "        if np.count_nonzero(~s_c[:d_in_c])+np.count_nonzero(s_c[X.shape[1]-d_in_a:]) > (d_in_a+d_in_c)/2:\n",
    "            # if more fixed nodes are incorrect flip\n",
    "            s_c = ~s_c\n",
    "            \n",
    "        #set the fixed\n",
    "        s_c[:d_in_c]=1\n",
    "        s_c[X.shape[1]-d_in_a:]=0\n",
    "        \n",
    "        s_r = graphs.segment_matrix(X.T)\n",
    "        if np.count_nonzero(s_r[:d_out_a])+np.count_nonzero(~s_r[X.shape[0]-d_out_c:]) > (d_out_a+d_out_c)/2:\n",
    "            # if more fixed nodes are incorrect flip\n",
    "            s_r = ~s_r\n",
    "        s_r[:d_out_a]=0\n",
    "        s_r[X.shape[0]-d_out_c:]=1\n",
    "    else:\n",
    "        #initialize segmentation\n",
    "        s_c = np.zeros(X.shape[1],dtype=bool)\n",
    "        s_c[:d_in_c+D.shape[1]//2]=1\n",
    "        s_r = np.zeros(X.shape[0],dtype=bool)\n",
    "        s_r[d_out_a+D.shape[0]//2:]=1\n",
    "    \n",
    "    \n",
    "    fs = np.zeros(N+1)\n",
    "    s_cols=np.zeros((N+1,X.shape[1]),dtype=bool) \n",
    "    s_rows=np.zeros((N+1,X.shape[0]),dtype=bool) \n",
    "    \n",
    "\n",
    "    s_cols[0]=s_c\n",
    "    s_rows[0]=s_r\n",
    "    fs[0]=f(X,s_c,s_r)\n",
    "    \n",
    "    for n in range(N):\n",
    "        #compute svds of Hankel matrices\n",
    "        U_c,s_caus,Vt_c = np.linalg.svd(X[s_r][:,s_c],full_matrices=False)#TODO false is only correct if w(0)=0\n",
    "        U_a,s_anti,Vt_a = np.linalg.svd(X[~s_r][:,~s_c],full_matrices=False)\n",
    "        \n",
    "        #colums: range of matrices\n",
    "        X_t = X[~s_r]\n",
    "        X_b = X[s_r]\n",
    "        W_t = (U_a.T@X_t)*w(s_anti).reshape(-1,1)\n",
    "        W_b = (U_c.T@X_b)*w(s_caus).reshape(-1,1)\n",
    "        \n",
    "        #columns: corange of matrices\n",
    "        X_r = X[:,~s_c]\n",
    "        X_l = X[:,s_c]    \n",
    "        W_r = (Vt_a@X_r.T)*w(s_anti).reshape(-1,1)\n",
    "        W_l = (Vt_c@X_l.T)*w(s_caus).reshape(-1,1)\n",
    "        \n",
    "        S_col = np.sum(W_t,axis=0) -np.sum(W_b,axis=0)\n",
    "        S_row = np.sum(W_r,axis=0) -np.sum(W_l,axis=0)\n",
    "        \n",
    "        ord_c = d_in_c +np.argsort(S_col[d_in_c:X.shape[1]-d_in_a])\n",
    "        ord_r = d_out_a+np.argsort(S_row[d_out_a:X.shape[0]-d_out_c])\n",
    "        \n",
    "        offset_reg = 3000\n",
    "        \n",
    "        f_offset = lambda s: offset_reg*2*(0.5-s)\n",
    "        \n",
    "        v_c = ord_c[v_reg_col+f_offset(s_c[ord_c])<S_col[ord_c]]\n",
    "        v_r = ord_r[v_reg_row+f_offset(s_r[ord_r])<S_row[ord_r]]\n",
    "        \n",
    "        s_c[d_in_c:s_c.size-d_in_a]=0\n",
    "        s_r[d_out_a:s_r.size-d_out_c]=0\n",
    "        \n",
    "        s_c[v_c] = 1\n",
    "        s_r[v_r] = 1\n",
    "        \n",
    "        \n",
    "        fs[n+1] = f(X,s_c,s_r)\n",
    "        s_cols[n+1]=s_c\n",
    "        s_rows[n+1]=s_r\n",
    "        \n",
    "        if np.any(np.logical_and(np.all(s_cols[:n+1]==s_c,axis=1),np.all(s_rows[:n+1]==s_r,axis=1))):\n",
    "            print(\"converged at n=\",n)\n",
    "            break\n",
    "        \n",
    "    report ={\"s_cols\":s_cols[:n+2],\"s_rows\":s_rows[:n+2],\"X\":X}\n",
    "        \n",
    "    return s_c[d_in_c:X.shape[1]-d_in_a],s_r[d_out_a:X.shape[0]-d_out_c],report\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_matrix(stage_causal,stage_anticausal,N=70,initla_spectral=True): #minimize frobenius norm\n",
    "    \n",
    "    Ac = stage_causal.A_matrix\n",
    "    Bc = stage_causal.B_matrix\n",
    "    Cc = stage_causal.C_matrix\n",
    "\n",
    "    Aa = stage_anticausal.A_matrix\n",
    "    Ba = stage_anticausal.B_matrix\n",
    "    Ca = stage_anticausal.C_matrix\n",
    "\n",
    "    D = stage_causal.D_matrix\n",
    "    \n",
    "    #regularization vector\n",
    "    v_reg_col = f_reg_col(np.linspace(-1,1,D.shape[1]))\n",
    "    v_reg_row = f_reg_row(np.linspace(-1,1,D.shape[0]))\n",
    "\n",
    "    #dims of states\n",
    "    (d_out_c,d_in_c) = Ac.shape\n",
    "    (d_out_a,d_in_a) = Aa.shape\n",
    "\n",
    "    #setup matrix\n",
    "    X = np.block([[np.zeros((d_out_a,d_in_c)),Ba,Aa ],\n",
    "                  [Cc,D,Ca],\n",
    "                  [Ac,Bc,np.zeros((d_out_c,d_in_a))]\n",
    "    \n",
    "    ])\n",
    "\n",
    "    s_c = np.zeros(X.shape[1],dtype=bool)\n",
    "    s_r = np.zeros(X.shape[0],dtype=bool)\n",
    "\n",
    "    #initialize based on the Bs and Cs\n",
    "    order = np.argsort(np.linalg.norm(Bc,axis=0)-np.linalg.norm(Ba,axis=0))\n",
    "    s_c[d_in_c+order[:len(order)//2]]=1\n",
    "    order = np.argsort(np.linalg.norm(Cc,axis=1)-np.linalg.norm(Ca,axis=1))\n",
    "    s_r[d_out_a+order[:len(order)//2]]=1\n",
    "    \n",
    "            \n",
    "    #set the fixed\n",
    "    s_c[:d_in_c]=1\n",
    "    s_c[X.shape[1]-d_in_a:]=0\n",
    "    s_r[:d_out_a]=0\n",
    "    s_r[X.shape[0]-d_out_c:]=1\n",
    "\n",
    "    \n",
    "    \n",
    "    fs = np.zeros(N+1)\n",
    "    s_cols=np.zeros((N+1,X.shape[1]),dtype=bool) \n",
    "    s_rows=np.zeros((N+1,X.shape[0]),dtype=bool) \n",
    "    \n",
    "    \n",
    "    Xs = X**2\n",
    "\n",
    "    s_cols[0]=s_c\n",
    "    s_rows[0]=s_r\n",
    "    fs[0]=np.sum(Xs[s_r][:,s_c])+ np.sum(Xs[~s_r][:,~s_c])\n",
    "    \n",
    "    q = int(np.ceil(min(D.shape)/2e2))\n",
    "    n_restart = -1e5\n",
    "    \n",
    "    for n in range(N):\n",
    "        \n",
    "        #columns:\n",
    "        #X_t = X[~s_r]\n",
    "        #X_b = X[s_r]\n",
    "        \n",
    "        #n_xt =np.linalg.norm(X_t,axis=0)\n",
    "        #n_xb =np.linalg.norm(X_b,axis=0)\n",
    "        n_xt = np.sum(Xs[~s_r],axis=0)\n",
    "        n_xb = np.sum(Xs[s_r],axis=0)\n",
    "        \n",
    "        #rows:\n",
    "        #X_r = X[:,~s_c]\n",
    "        #X_l = X[:,s_c]\n",
    "        \n",
    "        #n_xr = np.linalg.norm(X_r,axis=1)\n",
    "        #n_xl = np.linalg.norm(X_l,axis=1)\n",
    "        n_xr = np.sum(Xs[:,~s_c],axis=1)\n",
    "        n_xl = np.sum(Xs[:,s_c],axis=1)\n",
    "        \n",
    "        #only the ones we can change\n",
    "        s_c_int = s_c[d_in_c:X.shape[1]-d_in_a] \n",
    "        s_r_int = s_r[d_out_a:X.shape[0]-d_out_c]\n",
    "        \n",
    "        S_col = n_xt/np.count_nonzero(~s_r) -n_xb/np.count_nonzero(s_r_int)\n",
    "        S_row = n_xr/np.count_nonzero(~s_c) -n_xl/np.count_nonzero(s_c_int)\n",
    "        \n",
    "        #ord_c = d_in_c +np.argsort(S_col[d_in_c:X.shape[1]-d_in_a])\n",
    "        #ord_r = d_out_a+np.argsort(S_row[d_out_a:X.shape[0]-d_out_c])\n",
    "        \n",
    "        S_col_int = S_col[d_in_c:X.shape[1]-d_in_a] \n",
    "        S_row_int = S_row[d_out_a:X.shape[0]-d_out_c]\n",
    "        \n",
    "        \n",
    "        \n",
    "        if q ==1:\n",
    "            i_n = -1\n",
    "            i_p = -1\n",
    "            v_reg = gamma*(np.count_nonzero(s_c_int)/len(s_c_int)-0.5)**3\n",
    "            if np.any(S_col_int[s_c_int]<v_reg):\n",
    "                i_n= np.nonzero(s_c_int)[0][np.argmin(S_col_int[s_c_int])]\n",
    "                s_c[d_in_c+i_n]=0\n",
    "            if np.any(S_col_int[~s_c_int]>v_reg):\n",
    "                i_p= np.nonzero(~s_c_int)[0][np.argmax(S_col_int[~s_c_int])]\n",
    "                s_c[d_in_c+i_p]=1\n",
    "            if i_n==-1 and i_p==-1:\n",
    "                i_n= np.nonzero(s_c_int)[0][np.argmin(S_col_int[s_c_int])]\n",
    "                s_c[d_in_c+i_n]=0\n",
    "                i_p= np.nonzero(~s_c_int)[0][np.argmax(S_col_int[~s_c_int])]\n",
    "                s_c[d_in_c+i_p]=1\n",
    "\n",
    "            i_n = -1\n",
    "            i_p = -1\n",
    "            v_reg = gamma*(np.count_nonzero(s_r_int)/len(s_r_int)-0.5)**3\n",
    "            if np.any(S_row_int[s_r_int]<v_reg):\n",
    "                i_n = np.nonzero(s_r_int)[0][np.argmin(S_row_int[s_r_int])]\n",
    "                s_r[d_out_a+i_n]=0\n",
    "            if np.any(S_row_int[~s_r_int]>v_reg):\n",
    "                i_p = np.nonzero(~s_r_int)[0][np.argmax(S_row_int[~s_r_int])]\n",
    "                s_r[d_out_a+i_p]=1\n",
    "            if i_n==-1 and i_p==-1:\n",
    "                i_n = np.nonzero(s_r_int)[0][np.argmin(S_row_int[s_r_int])]\n",
    "                s_r[d_out_a+i_n]=0\n",
    "                i_p = np.nonzero(~s_r_int)[0][np.argmax(S_row_int[~s_r_int])]\n",
    "                s_r[d_out_a+i_p]=1\n",
    "                \n",
    "                \n",
    "        else:\n",
    "            v_reg = gamma*(np.count_nonzero(s_c_int)/len(s_c_int)-0.5)**3\n",
    "            \n",
    "            #if np.any(S_col_int[s_c_int]<v_reg):\n",
    "            i_n= np.arange(len(s_c_int))[s_c_int][np.argsort(S_col_int[s_c_int])[:q]]#arange ist trick to recover index\n",
    "            i_nf = i_n[S_col_int[i_n]<v_reg]\n",
    "            s_c[d_in_c+i_nf]=0\n",
    "            #if np.any(S_col_int[~s_c_int]>v_reg):\n",
    "            i_p= np.arange(len(s_c_int))[~s_c_int][np.argsort(S_col_int[~s_c_int])[-q:]]\n",
    "            i_pf = i_p[S_col_int[i_p]>v_reg]\n",
    "            s_c[d_in_c+i_pf]=1\n",
    "\n",
    "            if len(i_nf)==0 and len(i_pf)==0:\n",
    "                print(\"flip\")\n",
    "                s_c[d_in_c+i_n[0]]=0\n",
    "                s_c[d_in_c+i_p[-1]]=1\n",
    "            \n",
    "            v_reg = gamma*(np.count_nonzero(s_r_int)/len(s_r_int)-0.5)**3\n",
    "            #if np.any(S_row_int[s_r_int]<v_reg):\n",
    "            i_n = np.arange(len(s_r_int))[s_r_int][np.argsort(S_row_int[s_r_int])[:q]]\n",
    "            i_nf = i_n[S_row_int[i_n]<v_reg]\n",
    "            s_r[d_out_a+i_nf]=0\n",
    "            #if np.any(S_row_int[~s_r_int]>v_reg):\n",
    "            i_p = np.arange(len(s_r_int))[~s_r_int][np.argsort(S_row_int[~s_r_int])[-q:]]\n",
    "            i_pf = i_p[S_row_int[i_p]>v_reg]\n",
    "            s_r[d_out_a+i_pf]=1\n",
    "      \n",
    "            if len(i_nf)==0 and len(i_pf)==0:\n",
    "                print(\"flip\")\n",
    "                s_r[d_out_a+i_n[0]]=0\n",
    "                s_r[d_out_a+i_p[-1]]=1\n",
    "        #v_c = ord_c[v_reg_col>S_col[ord_c]]\n",
    "        #v_r = ord_r[v_reg_row>S_row[ord_r]]\n",
    "        \n",
    "        #s_c[d_in_c:s_c.size-d_in_a]=0\n",
    "        #s_r[d_out_a:s_r.size-d_out_c]=0\n",
    "        \n",
    "        #s_c[v_c] = 1\n",
    "        #s_r[v_r] = 1\n",
    "        \n",
    "        f = np.sum(Xs[s_r][:,s_c])+ np.sum(Xs[~s_r][:,~s_c])\n",
    "        if f > fs[0] and n > n_restart + 50: #worse than initial -> do restart with other initial\n",
    "            print(\"restart at n=\",n)\n",
    "            n_restart= n\n",
    "        #    s_c[d_in_c:X.shape[1]-d_in_a]=np.random.permutation(s_c[d_in_c:X.shape[1]-d_in_a])\n",
    "        #    s_r[d_out_a:X.shape[0]-d_out_c] = np.random.permutation(s_r[d_out_a:X.shape[0]-d_out_c])\n",
    "            i_min = np.argmin(fs[:n+1])\n",
    "            s_c = s_cols[i_min].copy()\n",
    "            s_r = s_rows[i_min].copy()\n",
    "            v = np.random.randint(d_in_c,X.shape[1]-d_in_a,3*q)\n",
    "            s_c[v] = ~s_c[v]\n",
    "            print(v)\n",
    "            v = np.random.randint(d_out_a,X.shape[0]-d_out_c,3*q)\n",
    "            print(v)\n",
    "            s_r[v] = ~s_r[v]            \n",
    "        \n",
    "        fs[n+1] = np.sum(Xs[s_r][:,s_c])+ np.sum(Xs[~s_r][:,~s_c])\n",
    "        s_cols[n+1]=s_c\n",
    "        s_rows[n+1]=s_r\n",
    "        \n",
    "        if np.any(np.logical_and(np.all(s_cols[:n+1]==s_c,axis=1),np.all(s_rows[:n+1]==s_r,axis=1))):\n",
    "            print(\"converged at n=\",n)\n",
    "            break\n",
    "            \n",
    "    #get minimum f\n",
    "    i_min = np.argmin(fs[:n+2])\n",
    "    s_c = s_cols[i_min]\n",
    "    s_r = s_rows[i_min]\n",
    "    print(\"frac cols:\",np.count_nonzero(s_c[d_in_c:X.shape[1]-d_in_a])/D.shape[1])\n",
    "    print(\"frac rows:\",np.count_nonzero(s_r[d_out_a:X.shape[0]-d_out_c])/D.shape[0])\n",
    "        \n",
    "    report ={\"s_cols\":s_cols[:n+2],\"s_rows\":s_rows[:n+2],\"X\":0,\"f\":fs[:n+2],\"q\":q}\n",
    "        \n",
    "    return s_c[d_in_c:X.shape[1]-d_in_a],s_r[d_out_a:X.shape[0]-d_out_c],report\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma =1e2\n",
    "q =25\n",
    "sys = Split.initial_mixed(T)\n",
    "stage_c=sys.causal_system.stages[0]\n",
    "stage_a=sys.anticausal_system.stages[0]\n",
    "s_c,s_r,report = segment_matrix(stage_c,stage_a,N=50)\n",
    "s_cols = report[\"s_cols\"]\n",
    "s_rows = report[\"s_rows\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_start_c,s_start_a = compute_sigmasf(T,s_cols[0],s_rows[0])\n",
    "s_end_c,s_end_a = compute_sigmasf(T,s_cols[-1],s_rows[-1])\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(s_start_c,label=\"start\")\n",
    "plt.plot(s_end_c,label=\"end\")\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(s_start_a,label=\"start\")\n",
    "plt.plot(s_end_a,label=\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.count_nonzero(s_start_a>2)+np.count_nonzero(s_start_c>1.5))\n",
    "print(np.count_nonzero(s_end_a>2)+np.count_nonzero(s_end_c>1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.spy(s_cols)\n",
    "print(np.count_nonzero(s_cols[-1])/len(s_cols[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.spy(s_rows)\n",
    "print(np.count_nonzero(s_cols[-1])/len(s_cols[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(report[\"f\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(s_cols[0]==s_cols[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helping function to apply permutation and store it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_permutations(s_c,s_r):\n",
    "    p_col = np.hstack([np.nonzero(s_c),np.nonzero(~s_c)]).reshape(-1)\n",
    "    p_row = np.hstack([np.nonzero(~s_r),np.nonzero(s_r)]).reshape(-1)\n",
    "    i_in =np.count_nonzero(s_c)\n",
    "    i_out=np.count_nonzero(s_r)\n",
    "    return p_col,p_row,i_in,i_out\n",
    "\n",
    "def permute_stage(stage,p_col,p_row):\n",
    "    stage.B_tilde = stage.B_tilde[:,p_col]\n",
    "    stage.C_tilde = stage.C_tilde[p_row,:]\n",
    "    stage.D_matrix = stage.D_matrix[:,p_col][p_row]\n",
    "    \n",
    "def collect_permutations(P_col,P_row,k,p_col,p_row,system):\n",
    "    \"\"\"\n",
    "    Function to collect the permutations in P_col and P_row\n",
    "    \n",
    "    P_col:    total permutation of columns\n",
    "    P_row:    total permutation of columns\n",
    "    l:        index of stage\n",
    "    p_col:    new collumn permuation\n",
    "    p_row:    new row permutation\n",
    "    \"\"\"\n",
    "    \n",
    "    dims_in = system.dims_in\n",
    "    dims_out = system.dims_out\n",
    "    \n",
    "    I = np.sum(dims_in[:k]).astype(int)\n",
    "    P_col[I:I+dims_in[k]]=P_col[I:I+dims_in[k]][p_col]\n",
    "    I = np.sum(dims_out[:k]).astype(int)\n",
    "    P_row[I:I+dims_out[k]]=P_row[I:I+dims_out[k]][p_row]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get test matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims_in =  np.array([4, 4, 4, 4])*6\n",
    "dims_out = np.array([4, 4, 4, 4])*6\n",
    "\n",
    "#dims_in =  np.array([9, 7, 7, 9])*3\n",
    "#dims_out = np.array([7, 9, 9, 7])*3\n",
    "\n",
    "\n",
    "n = 2\n",
    "#create orthogonal vectors and normalize them to the size of the matix (i.e. norm(block)/size(block) = const\n",
    "Us =np.vstack([scipy.stats.ortho_group.rvs(dims_out[i])[:,:3*n]*dims_out[i] for i in range(len(dims_in))])\n",
    "Vts=np.hstack([scipy.stats.ortho_group.rvs(dims_in[i])[:3*n,:]*dims_in[i] for i in range(len(dims_in))])\n",
    "\n",
    "s = np.linspace(1,0.75,n)\n",
    "\n",
    "lower = Us[:,:n]@np.diag(s)@Vts[:n,:]\n",
    "diag = Us[:,n:2*n]@np.diag(s)@Vts[n:2*n,:]\n",
    "upper = Us[:,2*n:3*n]@np.diag(s)@Vts[2*n:3*n,:]\n",
    "matrix = np.zeros_like(diag)\n",
    "a=0;b=0\n",
    "for i in range(len(dims_in)):\n",
    "    matrix[a:a+dims_out[i],:b]            =lower[a:a+dims_out[i],:b]\n",
    "    matrix[a:a+dims_out[i],b:b+dims_in[i]]=diag[a:a+dims_out[i],b:b+dims_in[i]]\n",
    "    matrix[a:a+dims_out[i],b+dims_in[i]:] =upper[a:a+dims_out[i],b+dims_in[i]:]\n",
    "    a+=dims_out[i];b+=dims_in[i]\n",
    "plt.figure()\n",
    "\n",
    "P_in_ref = np.random.permutation(np.arange(matrix.shape[1]))\n",
    "P_out_ref= np.random.permutation(np.arange(matrix.shape[0]))\n",
    "\n",
    "T = matrix[P_out_ref][:,P_in_ref]\n",
    "plt.matshow(T)\n",
    "print(T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#T = np.random.rand(32,32)\n",
    "#T =np.arange(1,32).reshape(1,-1)*np.arange(1,32).reshape(-1,1)\n",
    "#T = matrix.T\n",
    "#T = mats = get_mobilenet_target_mats()[0]\n",
    "sys = Split.initial_sigmas_mixed(T)\n",
    "utils.show_system(sys,mark_D=False)\n",
    "utils.check_dims(sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combine it to algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identification_split_system(sys,N):\n",
    "    \n",
    "    P_col = np.arange(np.sum(sys.dims_in) ,dtype=int)\n",
    "    P_row = np.arange(np.sum(sys.dims_out),dtype=int)\n",
    "    \n",
    "    Ps_col =np.zeros((N,P_col.size),dtype=int)\n",
    "    Ps_row =np.zeros((N,P_row.size),dtype=int)\n",
    "    for n in range(N):\n",
    "        print(n)\n",
    "        for k in range(len(sys.causal_system.stages)-1,-1,-1): #reverse ordering makes indexing easier \n",
    "            i_in =sys.causal_system.stages[k].dim_in//2\n",
    "            i_out=sys.causal_system.stages[k].dim_out//2\n",
    "            Split.split_sigmas_mixed(sys,k,i_in,i_out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys = Split.initial_sigmas_mixed(T)\n",
    "identification_split_system(sys,2)\n",
    "utils.check_dims(sys)\n",
    "utils.show_system(sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas_causal =[stage.s_in for stage in sys.causal_system.stages][1:]\n",
    "sigmas_anticausal =[stage.s_in for stage in sys.anticausal_system.stages][:-1]\n",
    "#print(sigmas_causal)\n",
    "#print(sigmas_anticausal)\n",
    "plt.subplot(1,2,1)\n",
    "for sig in sigmas_causal:\n",
    "    plt.scatter(np.arange(len(sig)),sig)\n",
    "plt.subplot(1,2,2)\n",
    "for sig in sigmas_anticausal:\n",
    "    plt.scatter(np.arange(len(sig)),sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identification_split_clustering(sys,N,N_split = 50):\n",
    "    \n",
    "    P_col = np.arange(np.sum(sys.dims_in) ,dtype=int)\n",
    "    P_row = np.arange(np.sum(sys.dims_out),dtype=int)\n",
    "    \n",
    "    Ps_col =np.zeros((N+1,P_col.size),dtype=int)\n",
    "    Ps_row =np.zeros((N+1,P_row.size),dtype=int)\n",
    "    Ps_col[0]=P_col\n",
    "    Ps_row[0]=P_row\n",
    "    reports = []\n",
    "    for n in range(N):\n",
    "        print(n)\n",
    "        for k in range(len(sys.causal_system.stages)-1,-1,-1): #reverse ordering makes indexing easier \n",
    "            stage_c=sys.causal_system.stages[k]\n",
    "            stage_a=sys.anticausal_system.stages[k]\n",
    "            s_c,s_r,report = segment_matrix(stage_c,stage_a,N=N_split)\n",
    "            reports.append(report)\n",
    "            \n",
    "            assert len(s_c)==stage_c.dim_in ,\"dims_in causal do not match s_c\"\n",
    "            assert len(s_r)==stage_c.dim_out,\"dims_out causal do not match s_r\"\n",
    "            assert len(s_c)==stage_a.dim_in ,\"dims_in antic do not match s_c\"\n",
    "            assert len(s_r)==stage_a.dim_out,\"dims_out antic do not match s_r\"\n",
    "            p_col,p_row,i_in,i_out = get_permutations(s_c,s_r)\n",
    "            permute_stage(stage_c,p_col,p_row)\n",
    "            permute_stage(stage_a,p_col,p_row)\n",
    "            collect_permutations(P_col,P_row,k,p_col,p_row,sys)\n",
    "            \n",
    "            Split.split_sigmas_mixed(sys,k,i_in,i_out)\n",
    "        #save the Permutations collected for all stages\n",
    "        Ps_col[n+1]=P_col\n",
    "        Ps_row[n+1]=P_row\n",
    "    \n",
    "    return Ps_col,Ps_row,reports"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Transpose to check for assymetries in algorithm\n",
    "T = T.T\n",
    "P_in_ref,P_out_ref = (P_out_ref,P_in_ref )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1e2\n",
    "sys = Split.initial_sigmas_mixed(T)\n",
    "Ps_col,Ps_row,reports = identification_split_clustering(sys,2)\n",
    "P_col = Ps_col[-1]\n",
    "P_row = Ps_row[-1]\n",
    "utils.check_dims(sys)\n",
    "utils.show_system(sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.scatter(np.arange(len(P_col)),P_col)\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(np.arange(len(P_row)),P_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas_causal =[stage.s_in for stage in sys.causal_system.stages][1:]\n",
    "sigmas_anticausal =[stage.s_in for stage in sys.anticausal_system.stages][:-1]\n",
    "#print(sigmas_causal)\n",
    "#print(sigmas_anticausal)\n",
    "plt.subplot(1,2,1)\n",
    "for i,sig in enumerate(sigmas_causal):\n",
    "    plt.scatter(np.arange(len(sig)),sig,label=str(i))\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "for i,sig in enumerate(sigmas_anticausal):\n",
    "    plt.scatter(np.arange(len(sig)),sig,label=str(i))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(T[P_row][:,P_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(T[Ps_row[-1]][:,Ps_col[-1]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.matshow(T[P_row][:,P_col]-sys.to_matrix())\n",
    "np.max(np.abs(T[P_row][:,P_col]-sys.to_matrix()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_dims(sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Some assymetry here\n",
    "\n",
    "The algorithm usually recvers the row clustering, but is unable to recover the column clustering. \n",
    "This is reagerdless of wheater the matrix is transposed or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.get_cmap('tab20')\n",
    "colors = np.repeat(cmap((1/20)*np.arange(4)+0.001),dims_in,axis=0)[P_in_ref]\n",
    "perm.multiple_connection_plot(perm.invert_permutations(Ps_col),colors=colors,start=0,end=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.get_cmap('tab20')\n",
    "colors = np.repeat(cmap((1/20)*np.arange(4)+0.001),dims_in,axis=0)[P_in_ref]\n",
    "perm.multiple_connection_plot(perm.invert_permutations(Ps_row),colors=colors,start=0,end=2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.matshow(reports[0][\"X\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight matrix form Mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = get_mobilenet_target_mats()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys = Split.initial_sigmas_mixed(T)\n",
    "identification_split_system(sys,3)\n",
    "utils.check_dims(sys)\n",
    "utils.show_system(sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas_causal =[stage.s_in for stage in sys.causal_system.stages][1:]\n",
    "sigmas_anticausal =[stage.s_in for stage in sys.anticausal_system.stages][:-1]\n",
    "#print(sigmas_causal)\n",
    "#print(sigmas_anticausal)\n",
    "plt.subplot(1,2,1)\n",
    "for sig in sigmas_causal:\n",
    "    plt.scatter(np.arange(len(sig)),sig)\n",
    "plt.subplot(1,2,2)\n",
    "for sig in sigmas_anticausal:\n",
    "    plt.scatter(np.arange(len(sig)),sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1e1\n",
    "#q = 20\n",
    "sys_per = Split.initial_sigmas_mixed(T)\n",
    "Ps_col,Ps_row,reports = identification_split_clustering(sys_per,3,N_split=200)\n",
    "utils.check_dims(sys_per)\n",
    "utils.show_system(sys_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas_causal =[stage.s_in for stage in sys_per.causal_system.stages][1:]\n",
    "sigmas_anticausal =[stage.s_in for stage in sys_per.anticausal_system.stages][:-1]\n",
    "#print(sigmas_causal)\n",
    "#print(sigmas_anticausal)\n",
    "plt.subplot(1,2,1)\n",
    "for sig in sigmas_causal:\n",
    "    plt.scatter(np.arange(len(sig)),sig)\n",
    "plt.subplot(1,2,2)\n",
    "for sig in sigmas_anticausal:\n",
    "    plt.scatter(np.arange(len(sig)),sig)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "perm.multiple_connection_plot(perm.invert_permutations(Ps_col),start=0,end=4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "perm.multiple_connection_plot(perm.invert_permutations(Ps_row),start=0,end=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sigmas_causal_per =[stage.s_in for stage in sys_per.causal_system.stages][1:]\n",
    "sigmas_anticausal_per =[stage.s_in for stage in sys_per.anticausal_system.stages][:-1]\n",
    "\n",
    "sigmas_causal =[stage.s_in for stage in sys.causal_system.stages][1:]\n",
    "sigmas_anticausal =[stage.s_in for stage in sys.anticausal_system.stages][:-1]\n",
    "\n",
    "plt.figure(figsize=[12,8])\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.grid()\n",
    "for sig in sigmas_causal:\n",
    "    plt.plot(np.arange(len(sig)),sig,color='C0')\n",
    "for sig in sigmas_causal_per:\n",
    "    plt.plot(np.arange(len(sig)),sig,color='C1')\n",
    "plt.subplot(1,2,2)\n",
    "for sig in sigmas_anticausal:\n",
    "    plt.plot(np.arange(len(sig)),sig,color='C0')\n",
    "for sig in sigmas_anticausal_per:\n",
    "    plt.plot(np.arange(len(sig)),sig,color='C1')\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_per = T[Ps_row[-1]][:,Ps_col[-1]]\n",
    "np.max(np.abs(T_per-sys_per.to_matrix()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_max = max([np.max(sig)for sig in sigmas_causal]+[np.max(sig)for sig in sigmas_anticausal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx =Approximation(sys,(sigmas_causal,sigmas_anticausal))\n",
    "approx_per=Approximation(sys_per,(sigmas_causal_per,sigmas_anticausal_per))\n",
    "\n",
    "\n",
    "N = 9 #number of points\n",
    "alpha = np.linspace(0,1,N)\n",
    "\n",
    "err_move =np.zeros_like(alpha)\n",
    "\n",
    "eps = eps_max*alpha\n",
    "\n",
    "def calc_values(approx,eps,matrix):\n",
    "    costs =np.zeros_like(eps)\n",
    "    err =np.zeros_like(eps)\n",
    "    for i in range(len(eps)):\n",
    "        approx_system=approx.get_approxiamtion(eps[i])\n",
    "        matrix_approx = approx_system.to_matrix()\n",
    "        err[i] = np.linalg.norm(matrix_approx-matrix,ord=2)\n",
    "        costs[i] = approx_system.cost()\n",
    "    return err,costs\n",
    "\n",
    "err_orig,cost_orig = calc_values(approx,eps,T)\n",
    "err_per,cost_per = calc_values(approx_per,eps,T_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cost_orig,err_orig,label=\"orig\")\n",
    "plt.plot(cost_per,err_per,label=\"per\")\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_orig[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_per[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_orig[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_per[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "print(alpha[i])\n",
    "print(1-cost_per[i]/cost_orig[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(err_per[i]/err_orig[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alpha,cost_per/cost_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for report in reports:\n",
    "    plt.figure()\n",
    "    plt.plot(report[\"f\"])\n",
    "#    plt.figure()\n",
    "#    plt.spy(report[\"s_cols\"])\n",
    "#    plt.figure()\n",
    "#    plt.spy(report[\"s_rows\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.spy(reports[4][\"s_rows\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.spy(reports[4][\"s_cols\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports[4][\"q\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports[4][\"s_cols\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports[4][\"s_cols\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_AlexNet_target_mats():\n",
    "    target_mats = []\n",
    "    # Load the model\n",
    "    model = models.alexnet(pretrained=True)\n",
    "    # Put moel into eval mode\n",
    "    model.eval()\n",
    "    for layer in model.classifier:\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "            # Obtain the weights of this layer\n",
    "            weights = layer.weight.detach().numpy()\n",
    "            target_mats.append(weights)\n",
    "    return target_mats\n",
    "mat_AlexNet = get_AlexNet_target_mats()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = mat_AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
