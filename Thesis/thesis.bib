
@article{vandebril_bibliography_2005,
	title = {A bibliography on semiseparable matrices*},
	volume = {42},
	issn = {1126-5434},
	url = {https://doi.org/10.1007/s10092-005-0107-z},
	doi = {10.1007/s10092-005-0107-z},
	abstract = {Currently there is a growing interest in semiseparable matrices and generalized semiseparable matrices. To gain an appreciation of the historical evolution of this concept, we present in this paper an extensive list of publications related to the field of semiseparable matrices. It is interesting to see that semiseparable matrices were investigated in different fields, e.g., integral equations, statistics, vibrational analysis, independently of each other. Also notable is the fact that leading statisticians at that time used semiseparable matrices without knowing their inverses to be tridiagonal. During this historical evolution the definition of semiseparable matrices has always been a difficult point leading to misunderstandings; sometimes they were defined as the inverses of irreducible tridiagonal matrices leading to generator representable matrices, while in other cases they were defined as matrices having low rank blocks below the diagonal.},
	pages = {249--270},
	number = {3},
	journaltitle = {{CALCOLO}},
	shortjournal = {Calcolo},
	author = {Vandebril, R. and Barel, M. Van and Golub, G. and Mastronardi, N.},
	urldate = {2021-09-14},
	date = {2005-12-01},
	langid = {english},
}

@book{vandebril_matrix_2007,
	location = {Baltimore, {UNITED} {STATES}},
	title = {Matrix Computations and Semiseparable Matrices},
	isbn = {978-0-8018-9679-8},
	url = {http://ebookcentral.proquest.com/lib/munchentech/detail.action?docID=3318415},
	shorttitle = {Matrix Computations and Semiseparable Matrices},
	abstract = {In recent years several new classes of matrices have been discovered and their structure exploited to design fast and accurate algorithms. In this new reference work, Raf Vandebril, Marc Van Barel, and Nicola Mastronardi present the first comprehensive overview of the mathematical and numerical properties of the family's newest member: semiseparable matrices. The text is divided into three parts. The first provides some historical background and introduces concepts and definitions concerning structured rank matrices. The second offers some traditional methods for solving systems of equations involving the basic subclasses of these matrices. The third section discusses structured rank matrices in a broader context, presents algorithms for solving higher-order structured rank matrices, and examines hybrid variants such as block quasiseparable matrices. An accessible case study clearly demonstrates the general topic of each new concept discussed. Many of the routines featured are implemented in Matlab and can be downloaded from the Web for further exploration.},
	publisher = {Johns Hopkins University Press},
	author = {Vandebril, Raf and Mastronardi, Nicola and Van Barel, Marc},
	urldate = {2021-09-14},
	date = {2007},
	keywords = {Matrices -- Data processing., Numerical analysis., Semiseparable matrices.},
	file = {Vandebril et al. - 2007 - Matrix Computations and Semiseparable Matrices Li.pdf:/home/stephan/snap/zotero-snap/common/Zotero/storage/N72WNGWM/Vandebril et al. - 2007 - Matrix Computations and Semiseparable Matrices Li.pdf:application/pdf},
}

@article{grasedyck_theorie_2001,
	title = {Theorie und Anwendungen Hierarchischer Matrizen},
	rights = {https://rightsstatements.org/page/{InC}/1.0/},
	url = {https://macau.uni-kiel.de/receive/diss_mods_00000454},
	abstract = {The modeling of physical properties often leads to the task of solving partial differential equations or integral equations. The results of some discretisation and linearisation process are matrix equations or linear systems of equations with special features. In the case of partial differential equations one exploits the local character of the differentiation by using some finite element method or finite difference scheme and gains a sparse system matrix. In the case of (nonlocal) integral operators low rank approximations seem to be the method of choice. These are either given explicitly by some multipole method or panel clustering technique or implicitly by rank revealing decompositions. Both types of matrices can be represented as so-called H-matrices. In this thesis we investigate algorithms that perform the addition, multiplication and inversion of H-matrices approximately. Under moderate assumptions the complexity of these new arithmetics is almost linear (linear up to logarithmic terms of order 1 to 3). The arithmetic operations can be performed adaptively, that is up to some given accuracy epsilon the relative error of the operations is zero. The question arises under which circumstances the inverse of an H-matrix can be approximated by an H-matrix. For the techniques used in this thesis we need very restrictive assumptions, but the numerical examples in the last part indicate that the approximability does not depend on these assumptions.},
	author = {Grasedyck, Lars},
	urldate = {2022-01-14},
	date = {2001-08-27},
	langid = {german},
	note = {Accepted: 2001-07-20},
}

@book{hackbusch_hierarchische_2009,
	location = {Berlin, Heidelberg},
	title = {Hierarchische Matrizen: Algorithmen und Analysis},
	isbn = {978-3-642-00222-9},
	url = {https://doi.org/10.1007/978-3-642-00222-9},
	publisher = {Springer},
	author = {Hackbusch, Wolfgang},
	editor = {Hackbusch, Wolfgang},
	urldate = {2022-01-14},
	date = {2009},
	langid = {german},
	doi = {10.1007/978-3-642-00222-9},
}


@book{dewilde_time-varying_1998,
	title = {Time-Varying Systems and Computations},
	isbn = {978-0-7923-8189-1},
	abstract = {Preface. 1. Introduction. Part I: Realization. 2. Notation and Properties of Non-Uniform Spaces. 3. Time-Varying State Space Realizations. 4. Diagonal Algebra. 5. Operator Realization Theory. 6. Isometric and Inner Operators. 7. Inner-Outer Factorization and Operator Inversion. Part {II}: Interpolation and Approximation. 8. J-Unitary Operators. 9. Algebraic Interpolation. 10. Hankel-Norm Model Reduction. 11. Low-Rank Matrix Approximation and Subspace Tracking. Part {III}: Factorization. 12. Orthogonal Embedding. 13. Spectral Factorization. 14. Lossless Cascade Factorizations. 15. Conclusions. Appendices: A. Hilbert Space Definitions and Properties. References. Glossary of Notation. Index.},
	author = {Dewilde, P. and Veen, Alle-Jan},
	date = {1998-01-01},
	doi = {10.1007/978-1-4757-2817-0},
	file = {Full Text PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/ZSF9ZQA2/Dewilde und Veen - 1998 - Time-Varying Systems and Computations.pdf:application/pdf},
}


@inproceedings{yu_compressing_2017,
	title = {On Compressing Deep Models by Low Rank and Sparse Decomposition},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Yu_On_Compressing_Deep_CVPR_2017_paper.html},
	eventtitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {7370--7379},
	author = {Yu, Xiyu and Liu, Tongliang and Wang, Xinchao and Tao, Dacheng},
	urldate = {2021-11-23},
	date = {2017},
	file = {Snapshot:/home/stephan/snap/zotero-snap/common/Zotero/storage/QQT5PMEQ/Yu_On_Compressing_Deep_CVPR_2017_paper.html:text/html;Full Text PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/2ABSW4SD/Yu et al. - 2017 - On Compressing Deep Models by Low Rank and Sparse .pdf:application/pdf},
}

@article{fan_multiscale_2019,
	title = {A Multiscale Neural Network Based on Hierarchical Matrices},
	volume = {17},
	issn = {1540-3459},
	url = {https://epubs.siam.org/doi/abs/10.1137/18M1203602},
	doi = {10.1137/18M1203602},
	abstract = {In this work we introduce a new multiscale artificial neural network based on the structure of \${\textbackslash}mathcal\{H\}\$-matrices. This network generalizes the latter to the nonlinear case by introducing a local deep neural network at each spatial scale. Numerical results indicate that the network is able to efficiently approximate discrete nonlinear maps obtained from discretized nonlinear partial differential equations, such as those arising from nonlinear Schrödinger equations and the Kohn--Sham density functional theory.},
	pages = {1189--1213},
	number = {4},
	journaltitle = {Multiscale Modeling \& Simulation},
	shortjournal = {Multiscale Model. Simul.},
	author = {Fan, Yuwei and Lin, Lin and Ying, Lexing and Zepeda-Nún͂ez, Leonardo},
	urldate = {2022-02-18},
	date = {2019-01},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {\${\textbackslash}mathcal\{H\}\$-matrix, 60H35, 65F30, 65M75, convolutional neural network, locally connected neural network, multiscale neural network},
	file = {Eingereichte Version:/home/stephan/snap/zotero-snap/common/Zotero/storage/N99YILHC/Fan et al. - 2019 - A Multiscale Neural Network Based on Hierarchical .pdf:application/pdf;18m1203602.pdf:/home/stephan/snap/zotero-snap/common/Zotero/storage/IHIZNB3F/18m1203602.pdf:application/pdf},
}

@article{wu_hybrid_2020,
	title = {Hybrid tensor decomposition in neural network compression},
	volume = {132},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608020303294},
	doi = {10.1016/j.neunet.2020.09.006},
	abstract = {Deep neural networks ({DNNs}) have enabled impressive breakthroughs in various artificial intelligence ({AI}) applications recently due to its capability of learning high-level features from big data. However, the current demand of {DNNs} for computational resources especially the storage consumption is growing due to that the increasing sizes of models are being required for more and more complicated applications. To address this problem, several tensor decomposition methods including tensor-train ({TT}) and tensor-ring ({TR}) have been applied to compress {DNNs} and shown considerable compression effectiveness. In this work, we introduce the hierarchical Tucker ({HT}), a classical but rarely-used tensor decomposition method, to investigate its capability in neural network compression. We convert the weight matrices and convolutional kernels to both {HT} and {TT} formats for comparative study, since the latter is the most widely used decomposition method and the variant of {HT}. We further theoretically and experimentally discover that the {HT} format has better performance on compressing weight matrices, while the {TT} format is more suited for compressing convolutional kernels. Based on this phenomenon we propose a strategy of hybrid tensor decomposition by combining {TT} and {HT} together to compress convolutional and fully connected parts separately and attain better accuracy than only using the {TT} or {HT} format on convolutional neural networks ({CNNs}). Our work illuminates the prospects of hybrid tensor decomposition for neural network compression.},
	pages = {309--320},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Wu, Bijiao and Wang, Dingheng and Zhao, Guangshe and Deng, Lei and Li, Guoqi},
	urldate = {2022-02-18},
	date = {2020-12-01},
	langid = {english},
	keywords = {Balanced structure, Hierarchical Tucker, Hybrid tensor decomposition, Neural network compression, Tensor-train},
	file = {ScienceDirect Snapshot:/home/stephan/snap/zotero-snap/common/Zotero/storage/DPG9GGZZ/S0893608020303294.html:text/html;Eingereichte Version:/home/stephan/snap/zotero-snap/common/Zotero/storage/PYE8ISCU/Wu et al. - 2020 - Hybrid tensor decomposition in neural network comp.pdf:application/pdf},
}

@article{pati_analysis_1993,
	title = {Analysis and synthesis of feedforward neural networks using discrete affine wavelet transformations},
	volume = {4},
	issn = {1941-0093},
	doi = {10.1109/72.182697},
	abstract = {A representation of a class of feedforward neural networks in terms of discrete affine wavelet transforms is developed. It is shown that by appropriate grouping of terms, feedforward neural networks with sigmoidal activation functions can be viewed as architectures which implement affine wavelet decompositions of mappings. It is shown that the wavelet transform formalism provides a mathematical framework within which it is possible to perform both analysis and synthesis of feedforward networks. For the purpose of analysis, the wavelet formulation characterizes a class of mappings which can be implemented by feedforward networks as well as reveals an exact implementation of a given mapping in this class. Spatio-spectral localization properties of wavelets can be exploited in synthesizing a feedforward network to perform a given approximation task. Two synthesis procedures based on spatio-spectral localization that reduce the training problem to one of convex optimization are outlined.{\textless}{\textgreater}},
	pages = {73--85},
	number = {1},
	journaltitle = {{IEEE} Transactions on Neural Networks},
	author = {Pati, Y.C. and Krishnaprasad, P.S.},
	date = {1993-01},
	note = {Conference Name: {IEEE} Transactions on Neural Networks},
	keywords = {Neural networks, Biology computing, Control systems, Discrete wavelet transforms, Feedforward neural networks, Motion control, Network synthesis, Performance analysis, Wavelet analysis, Wavelet transforms},
	file = {Eingereichte Version:/home/stephan/snap/zotero-snap/common/Zotero/storage/85UZBJBU/Pati und Krishnaprasad - 1993 - Analysis and synthesis of feedforward neural netwo.pdf:application/pdf;IEEE Xplore Abstract Record:/home/stephan/snap/zotero-snap/common/Zotero/storage/IH976WXW/182697.html:text/html},
}

@article{postalcioglu_wavelet_2007,
	title = {Wavelet networks for nonlinear system modeling},
	volume = {16},
	issn = {1433-3058},
	url = {https://doi.org/10.1007/s00521-006-0069-3},
	doi = {10.1007/s00521-006-0069-3},
	abstract = {This study presents a nonlinear systems and function learning by using wavelet network. Wavelet networks are as neural network for training and structural approach. But, training algorithms of wavelet networks is required a smaller number of iterations when the compared with neural networks. Gaussian-based mother wavelet function is used as an activation function. Wavelet networks have three main parameters; dilation, translation, and connection parameters (weights). Initial values of these parameters are randomly selected. They are optimized during training (learning) phase. Because of random selection of all initial values, it may not be suitable for process modeling. Because wavelet functions are rapidly vanishing functions. For this reason heuristic procedure has been used. In this study serial-parallel identification model has been applied to system modeling. This structure does not utilize feedback. Real system outputs have been exercised for prediction of the future system outputs. So that stability and approximation of the network is guaranteed. Gradient methods have been applied for parameters updating with momentum term. Quadratic cost function is used for error minimization. Three example problems have been examined in the simulation. They are static nonlinear functions and discrete dynamic nonlinear system.},
	pages = {433--441},
	number = {4},
	journaltitle = {Neural Computing and Applications},
	shortjournal = {Neural Comput \& Applic},
	author = {Postalcioglu, Seda and Becerikli, Yasar},
	urldate = {2022-02-18},
	date = {2007-05-01},
	langid = {english},
	file = {Springer Full Text PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/PJ9FYUV2/Postalcioglu und Becerikli - 2007 - Wavelet networks for nonlinear system modeling.pdf:application/pdf},
}

@inproceedings{hassibi_optimal_1993,
	title = {Optimal Brain Surgeon and general network pruning},
	doi = {10.1109/ICNN.1993.298572},
	abstract = {The use of information from all second-order derivatives of the error function to perform network pruning (i.e., removing unimportant weights from a trained network) in order to improve generalization, simplify networks, reduce hardware or storage requirements, increase the speed of further training, and, in some cases, enable rule extraction, is investigated. The method, Optimal Brain Surgeon ({OBS}), is significantly better than magnitude-based methods and Optimal Brain Damage, which often remove the wrong weights. {OBS}, permits pruning of more weights than other methods (for the same error on the training set), and thus yields better generalization on test data. Crucial to {OBS} is a recursion relation for calculating the inverse Hessian matrix H/sup -1/ from training data and structural information of the set. {OBS} deletes the correct weights from a trained {XOR} network in every case.{\textless}{\textgreater}},
	eventtitle = {{IEEE} International Conference on Neural Networks},
	pages = {293--299 vol.1},
	booktitle = {{IEEE} International Conference on Neural Networks},
	author = {Hassibi, B. and Stork, D.G. and Wolff, G.J.},
	date = {1993-03},
	keywords = {Machine learning, Training data, Backpropagation, Benchmark testing, Biological neural networks, Data mining, Hardware, Pattern recognition, Statistics, Surges},
	file = {Akzeptierte Version:/home/stephan/snap/zotero-snap/common/Zotero/storage/5NUFGW2D/Hassibi et al. - 1993 - Optimal Brain Surgeon and general network pruning.pdf:application/pdf;IEEE Xplore Abstract Record:/home/stephan/snap/zotero-snap/common/Zotero/storage/P7944RPW/298572.html:text/html},
}

@inproceedings{dao_kaleidoscope_2020,
	title = {Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps},
	url = {https://iclr.cc/virtual_2020/poster_BkgrBgSYDS.html},
	shorttitle = {Kaleidoscope},
	abstract = {Modern neural network architectures use structured linear transformations, such as low-rank matrices, sparse matrices, permutations, and the Fourier transform, to improve inference speed and reduce memory usage compared to general linear maps. However, choosing which of the myriad structured transformations to use (and its associated parameterization) is a laborious task that requires trading off speed, space, and accuracy. We consider a different approach: we introduce a family of matrices called kaleidoscope matrices (K-matrices) that provably capture any structured matrix with near-optimal space (parameter) and time (arithmetic operation) complexity. We empirically validate that K-matrices can be automatically learned within end-to-end pipelines to replace hand-crafted procedures, in order to improve model quality. For example, replacing channel shuffles in {ShuffleNet} improves classification accuracy on {ImageNet} by up to 5\%. K-matrices can also simplify hand-engineered pipelines---we replace filter bank feature computation in speech data preprocessing with a learnable kaleidoscope layer, resulting in only 0.4\% loss in accuracy on the {TIMIT} speech recognition task. In addition, K-matrices can capture latent structure in models: for a challenging permuted image classification task, adding a K-matrix to a standard convolutional architecture can enable learning the latent permutation and improve accuracy by over 8 points. We provide a practically efficient implementation of our approach, and use K-matrices in a Transformer network to attain 36\% faster end-to-end inference speed on a language translation task.},
	eventtitle = {Eighth International Conference on Learning Representations},
	author = {Dao, Tri and Sohoni, Nimit and Gu, Albert and Eichhorn, Matthew and Blonder, Amit and Leszczynski, Megan and Rudra, Atri and Ré, Christopher},
	urldate = {2022-02-18},
	date = {2020-04},
	langid = {english},
	file = {Full Text PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/NNEJ8H7Z/Dao et al. - 2020 - Kaleidoscope An Efficient, Learnable Representati.pdf:application/pdf;Snapshot:/home/stephan/snap/zotero-snap/common/Zotero/storage/CP5X4RFB/poster_BkgrBgSYDS.html:text/html},
}

@article{li_butterfly-net_2020,
	title = {Butterfly-Net: Optimal Function Representation Based on Convolutional Neural Networks},
	volume = {28},
	issn = {1815-2406, 1991-7120},
	url = {http://global-sci.org/intro/article_detail/cicp/18398.html},
	doi = {10.4208/cicp.OA-2020-0214},
	shorttitle = {Butterfly-Net},
	pages = {1838--1885},
	number = {5},
	journaltitle = {Communications in Computational Physics},
	shortjournal = {{CiCP}},
	author = {Li, Yingzhou and Cheng, Xiuyuan and Lu, Jianfeng},
	urldate = {2022-02-18},
	date = {2020-06},
	file = {Eingereichte Version:/home/stephan/snap/zotero-snap/common/Zotero/storage/VSC87ETK/Li - 2020 - Butterfly-Net Optimal Function Representation Bas.pdf:application/pdf;1805.07451.pdf:/home/stephan/snap/zotero-snap/common/Zotero/storage/PK5NHZ4F/1805.07451.pdf:application/pdf},
}

@article{dettmers_sparse_2019,
	title = {Sparse Networks from Scratch: Faster Training without Losing Performance},
	url = {http://arxiv.org/abs/1907.04840},
	shorttitle = {Sparse Networks from Scratch},
	abstract = {We demonstrate the possibility of what we call sparse learning: accelerated training of deep neural networks that maintain sparse weights throughout training while achieving dense performance levels. We accomplish this by developing sparse momentum, an algorithm which uses exponentially smoothed gradients (momentum) to identify layers and weights which reduce the error efficiently. Sparse momentum redistributes pruned weights across layers according to the mean momentum magnitude of each layer. Within a layer, sparse momentum grows weights according to the momentum magnitude of zero-valued weights. We demonstrate state-of-the-art sparse performance on {MNIST}, {CIFAR}-10, and {ImageNet}, decreasing the mean error by a relative 8\%, 15\%, and 6\% compared to other sparse algorithms. Furthermore, we show that sparse momentum reliably reproduces dense performance levels while providing up to 5.61x faster training. In our analysis, ablations show that the benefits of momentum redistribution and growth increase with the depth and size of the network. Additionally, we find that sparse momentum is insensitive to the choice of its hyperparameters suggesting that sparse momentum is robust and easy to use.},
	journaltitle = {{arXiv}:1907.04840 [cs, stat]},
	author = {Dettmers, Tim and Zettlemoyer, Luke},
	urldate = {2022-02-18},
	date = {2019-08-23},
	eprinttype = {arxiv},
	eprint = {1907.04840},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/N243G9EP/Dettmers und Zettlemoyer - 2019 - Sparse Networks from Scratch Faster Training with.pdf:application/pdf;arXiv.org Snapshot:/home/stephan/snap/zotero-snap/common/Zotero/storage/Q9SNTJLT/1907.html:text/html},
}

@article{li_butterfly_2015,
	title = {Butterfly Factorization},
	volume = {13},
	issn = {1540-3459},
	url = {https://epubs.siam.org/doi/abs/10.1137/15M1007173},
	doi = {10.1137/15M1007173},
	abstract = {The paper introduces the butterfly factorization as a data-sparse approximation for the matrices that satisfy a complementary low-rank property. The factorization can be constructed efficiently if either fast algorithms for applying the matrix and its adjoint are available or the entries of the matrix can be sampled individually. For an \$N{\textbackslash}times N\$ matrix, the resulting factorization is a product of \$O({\textbackslash}log N)\$ sparse matrices, each with \$O(N)\$ nonzero entries. Hence, it can be applied rapidly in \$O(N{\textbackslash}log N)\$ operations. Numerical results are provided to demonstrate the effectiveness of the butterfly factorization and its construction algorithms.},
	pages = {714--732},
	number = {2},
	journaltitle = {Multiscale Modeling \& Simulation},
	shortjournal = {Multiscale Model. Simul.},
	author = {Li, Yingzhou and Yang, Haizhao and Martin, Eileen R. and Ho, Kenneth L. and Ying, Lexing},
	urldate = {2022-02-18},
	date = {2015-01},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {44A55, 65R10, 65T50, butterfly algorithm, data-sparse matrix, Fourier integral operators, matrix factorization, operator compression, randomized algorithm, special functions},
	file = {Eingereichte Version:/home/stephan/snap/zotero-snap/common/Zotero/storage/N6HLAF8X/Li et al. - 2015 - Butterfly Factorization.pdf:application/pdf},
}

@report{parker_random_1995,
	title = {Random Butterfly Transformations with Applications in Computational Linear Algebra},
	url = {http://web.cs.ucla.edu/~stott/},
	abstract = {Theory and practice of computational linear algebra differ over the issue of degeneracy. Block matrix decompositions are used heavily in theory, but less in practice, since even when a matrix is nondegenerate (has full rank) its block submatrices can be degenerate. The potential degeneracy of block submatrices can completely prevent practical use of block matrix algorithms. Gaussian elimination is an important example of an algorithm affected by the possibility of degeneracy. While the basic elimination procedure is simple to state and implement, it becomes more complicated with the addition of a pivoting procedure, which handles degenerate matrices having zeros on the diagonal. Pivoting can significantly complicate the algorithm, increase data movement, and reduce speed, particularly on high-performance computers. We propose a randomization scheme that preconditions an input matrix by multiplying it with random matrices, where this multiplication can be performed efficiently. At the e...},
	author = {Parker, D. Stott},
	date = {1995},
	file = {CSD-950023.pdf:/home/stephan/snap/zotero-snap/common/Zotero/storage/AWMRRXRE/CSD-950023.pdf:application/pdf},
}

@inproceedings{louizos_learning_2018,
	title = {Learning Sparse Neural Networks through L\_0 Regularization},
	url = {https://openreview.net/forum?id=H1Y8hhg0b},
	abstract = {We show how to optimize the expected L\_0 norm of parametric models with gradient descent and introduce a new distribution that facilitates hard gating.},
	eventtitle = {International Conference on Learning Representations},
	author = {Louizos, Christos and Welling, Max and Kingma, Diederik P.},
	urldate = {2022-02-21},
	date = {2018-02-15},
	langid = {english},
	file = {Full Text PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/EA9QZ2PR/Louizos et al. - 2018 - Learning Sparse Neural Networks through L_0 Regula.pdf:application/pdf;Snapshot:/home/stephan/snap/zotero-snap/common/Zotero/storage/5XND2X2A/forum.html:text/html},
}

@inproceedings{wen_learning_2016,
	title = {Learning Structured Sparsity in Deep Neural Networks},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/hash/41bfd20a38bb1b0bec75acf0845530a7-Abstract.html},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
	urldate = {2022-02-21},
	date = {2016},
	file = {Full Text PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/P8C4LBEC/Wen et al. - 2016 - Learning Structured Sparsity in Deep Neural Networ.pdf:application/pdf},
}

@inproceedings{ailon_sparse_2021,
	title = {Sparse linear networks with a fixed butterfly structure: theory and practice},
	url = {https://proceedings.mlr.press/v161/ailon21a.html},
	shorttitle = {Sparse linear networks with a fixed butterfly structure},
	abstract = {A butterfly network consists of logarithmically many layers, each with a linear number of non-zero weights (pre-specified). The fast Johnson-Lindenstrauss transform ({FJLT}) can be represented as a butterfly network followed by a projection onto a random subset of the coordinates. Moreover, a random matrix based on {FJLT} with high probability approximates the action of any matrix on a vector. Motivated by these facts, we propose to replace a dense linear layer in any neural network by an architecture based on the butterfly network. The proposed architecture significantly improves upon the quadratic number of weights required in a standard dense layer to nearly linear with little compromise in expressibility of the resulting operator. In a collection of wide variety of experiments, including supervised prediction on both the {NLP} and vision data, we show that this not only produces results that match and at times outperform existing well-known architectures, but it also offers faster training and prediction in deployment. To understand the optimization problems posed by neural networks with a butterfly network, we also study the optimization landscape of the encoder-decoder network, where the encoder is replaced by a butterfly network followed by a dense linear layer in smaller dimension. Theoretical result presented in the paper explains why the training speed and outcome are not compromised by our proposed approach.},
	eventtitle = {Uncertainty in Artificial Intelligence},
	pages = {1174--1184},
	booktitle = {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},
	publisher = {{PMLR}},
	author = {Ailon, Nir and Leibovitch, Omer and Nair, Vineet},
	urldate = {2022-02-21},
	date = {2021-12-01},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/HMPMZ8E4/Ailon et al. - 2021 - Sparse linear networks with a fixed butterfly stru.pdf:application/pdf;Supplementary PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/3K9F8MBG/Ailon et al. - 2021 - Sparse linear networks with a fixed butterfly stru.pdf:application/pdf},
}

@inproceedings{jaderberg_speeding_2014,
	title = {Speeding up Convolutional Neural Networks with Low Rank Expansions},
	doi = {http://dx.doi.org/10.5244/C.28.88},
	booktitle = {Proceedings of the British Machine Vision Conference},
	publisher = {{BMVA} Press},
	author = {Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
	date = {2014},
	file = {Jaderberg et al. - 2014 - Speeding up Convolutional Neural Networks with Low.pdf:/home/stephan/snap/zotero-snap/common/Zotero/storage/82EBBFY4/Jaderberg et al. - 2014 - Speeding up Convolutional Neural Networks with Low.pdf:application/pdf;paper073.pdf:/home/stephan/snap/zotero-snap/common/Zotero/storage/7SAHQC2J/paper073.pdf:application/pdf},
}

@inproceedings{ioannou_training_2016,
	title = {Training {CNNs} with Low-Rank Filters for Efficient Image Classification},
	doi = {10.13140/RG.2.1.3727.8163},
	abstract = {Published as a conference paper at {ICLR} 2016

Trained Models at http://dx.doi.org/10.5281/zenodo.53189},
	author = {Ioannou, Yani and Robertson, Duncan and Shotton, Jamie and Cipolla, Roberto and Criminisi, Antonio},
	date = {2016-05-02},
	file = {Full Text PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/HJPXHIUB/Ioannou et al. - 2016 - Training CNNs with Low-Rank Filters for Efficient .pdf:application/pdf},
}

@article{blalock_what_2020,
	title = {What is the State of Neural Network Pruning?},
	volume = {2},
	url = {https://proceedings.mlsys.org/paper/2020/hash/d2ddea18f00665ce8623e36bd4e3c7c5-Abstract.html},
	pages = {129--146},
	journaltitle = {Proceedings of Machine Learning and Systems},
	author = {Blalock, Davis and Gonzalez Ortiz, Jose Javier and Frankle, Jonathan and Guttag, John},
	urldate = {2022-02-21},
	date = {2020-03-15},
	langid = {english},
	file = {Full Text PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/HEQQYRFR/Blalock et al. - 2020 - What is the State of Neural Network Pruning.pdf:application/pdf;Snapshot:/home/stephan/snap/zotero-snap/common/Zotero/storage/IJPBTXPB/d2ddea18f00665ce8623e36bd4e3c7c5-Abstract.html:text/html},
}
