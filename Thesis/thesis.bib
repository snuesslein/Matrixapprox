%=====================================================================================================================
%matrix structures
%=====================================================================================================================
@article{vandebril_bibliography_2005,
	title = {A bibliography on semiseparable matrices*},
	volume = {42},
	issn = {1126-5434},
	url = {https://doi.org/10.1007/s10092-005-0107-z},
	doi = {10.1007/s10092-005-0107-z},
	abstract = {Currently there is a growing interest in semiseparable matrices and generalized semiseparable matrices. To gain an appreciation of the historical evolution of this concept, we present in this paper an extensive list of publications related to the field of semiseparable matrices. It is interesting to see that semiseparable matrices were investigated in different fields, e.g., integral equations, statistics, vibrational analysis, independently of each other. Also notable is the fact that leading statisticians at that time used semiseparable matrices without knowing their inverses to be tridiagonal. During this historical evolution the definition of semiseparable matrices has always been a difficult point leading to misunderstandings; sometimes they were defined as the inverses of irreducible tridiagonal matrices leading to generator representable matrices, while in other cases they were defined as matrices having low rank blocks below the diagonal.},
	pages = {249--270},
	number = {3},
	journaltitle = {{CALCOLO}},
	shortjournal = {Calcolo},
	author = {Vandebril, R. and Barel, M. Van and Golub, G. and Mastronardi, N.},
	urldate = {2021-09-14},
	date = {2005-12-01},
	langid = {english},
}

@book{vandebril_matrix_2007,
	location = {Baltimore, {UNITED} {STATES}},
	title = {Matrix Computations and Semiseparable Matrices},
	isbn = {978-0-8018-9679-8},
	url = {http://ebookcentral.proquest.com/lib/munchentech/detail.action?docID=3318415},
	shorttitle = {Matrix Computations and Semiseparable Matrices},
	abstract = {In recent years several new classes of matrices have been discovered and their structure exploited to design fast and accurate algorithms. In this new reference work, Raf Vandebril, Marc Van Barel, and Nicola Mastronardi present the first comprehensive overview of the mathematical and numerical properties of the family's newest member: semiseparable matrices. The text is divided into three parts. The first provides some historical background and introduces concepts and definitions concerning structured rank matrices. The second offers some traditional methods for solving systems of equations involving the basic subclasses of these matrices. The third section discusses structured rank matrices in a broader context, presents algorithms for solving higher-order structured rank matrices, and examines hybrid variants such as block quasiseparable matrices. An accessible case study clearly demonstrates the general topic of each new concept discussed. Many of the routines featured are implemented in Matlab and can be downloaded from the Web for further exploration.},
	publisher = {Johns Hopkins University Press},
	author = {Vandebril, Raf and Mastronardi, Nicola and Van Barel, Marc},
	urldate = {2021-09-14},
	date = {2007},
	keywords = {Matrices -- Data processing., Numerical analysis., Semiseparable matrices.},
	file = {Vandebril et al. - 2007 - Matrix Computations and Semiseparable Matrices Li.pdf:/home/stephan/snap/zotero-snap/common/Zotero/storage/N72WNGWM/Vandebril et al. - 2007 - Matrix Computations and Semiseparable Matrices Li.pdf:application/pdf},
}

@article{grasedyck_theorie_2001,
	title = {Theorie und Anwendungen Hierarchischer Matrizen},
	rights = {https://rightsstatements.org/page/{InC}/1.0/},
	url = {https://macau.uni-kiel.de/receive/diss_mods_00000454},
	abstract = {The modeling of physical properties often leads to the task of solving partial differential equations or integral equations. The results of some discretisation and linearisation process are matrix equations or linear systems of equations with special features. In the case of partial differential equations one exploits the local character of the differentiation by using some finite element method or finite difference scheme and gains a sparse system matrix. In the case of (nonlocal) integral operators low rank approximations seem to be the method of choice. These are either given explicitly by some multipole method or panel clustering technique or implicitly by rank revealing decompositions. Both types of matrices can be represented as so-called H-matrices. In this thesis we investigate algorithms that perform the addition, multiplication and inversion of H-matrices approximately. Under moderate assumptions the complexity of these new arithmetics is almost linear (linear up to logarithmic terms of order 1 to 3). The arithmetic operations can be performed adaptively, that is up to some given accuracy epsilon the relative error of the operations is zero. The question arises under which circumstances the inverse of an H-matrix can be approximated by an H-matrix. For the techniques used in this thesis we need very restrictive assumptions, but the numerical examples in the last part indicate that the approximability does not depend on these assumptions.},
	author = {Grasedyck, Lars},
	urldate = {2022-01-14},
	date = {2001-08-27},
	langid = {german},
	note = {Accepted: 2001-07-20},
}

@book{hackbusch_hierarchische_2009,
	location = {Berlin, Heidelberg},
	title = {Hierarchische Matrizen: Algorithmen und Analysis},
	isbn = {978-3-642-00222-9},
	url = {https://doi.org/10.1007/978-3-642-00222-9},
	publisher = {Springer},
	author = {Hackbusch, Wolfgang},
	editor = {Hackbusch, Wolfgang},
	urldate = {2022-01-14},
	date = {2009},
	langid = {german},
	doi = {10.1007/978-3-642-00222-9},
}


@book{dewilde_time-varying_1998,
	title = {Time-Varying Systems and Computations},
	isbn = {978-0-7923-8189-1},
	abstract = {Preface. 1. Introduction. Part I: Realization. 2. Notation and Properties of Non-Uniform Spaces. 3. Time-Varying State Space Realizations. 4. Diagonal Algebra. 5. Operator Realization Theory. 6. Isometric and Inner Operators. 7. Inner-Outer Factorization and Operator Inversion. Part {II}: Interpolation and Approximation. 8. J-Unitary Operators. 9. Algebraic Interpolation. 10. Hankel-Norm Model Reduction. 11. Low-Rank Matrix Approximation and Subspace Tracking. Part {III}: Factorization. 12. Orthogonal Embedding. 13. Spectral Factorization. 14. Lossless Cascade Factorizations. 15. Conclusions. Appendices: A. Hilbert Space Definitions and Properties. References. Glossary of Notation. Index.},
	author = {Dewilde, P. and Veen, Alle-Jan},
	date = {1998-01-01},
	doi = {10.1007/978-1-4757-2817-0},
	file = {Full Text PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/ZSF9ZQA2/Dewilde und Veen - 1998 - Time-Varying Systems and Computations.pdf:application/pdf},
}

%=====================================================================================================================
%Sequentially semiseperable
%=====================================================================================================================

@article{chandrasekaran_fast_2005,
	title = {Some Fast Algorithms for Sequentially Semiseparable Representations},
	volume = {27},
	issn = {0895-4798},
	url = {https://epubs.siam.org/doi/abs/10.1137/S0895479802405884},
	doi = {10.1137/S0895479802405884},
	abstract = {An extended sequentially semiseparable ({SSS}) representation derived from time-varying system theory is used to capture, on the one hand, the low-rank of the off-diagonal blocks of a matrix for the purposes of efficient computations and, on the other, to provide for sufficient descriptive richness to allow for backward stability in the computations. We present (i) a fast algorithm (linear in the number of equations) to solve least squares problems in which the coefficient matrix is in {SSS} form, (ii) a fast algorithm to find the {SSS} form of X such that \${AX}=B\$, where A and B are in {SSS} form, and (iii) a fast model reduction technique to improve the {SSS} form.},
	pages = {341--364},
	number = {2},
	journaltitle = {{SIAM} Journal on Matrix Analysis and Applications},
	shortjournal = {{SIAM} J. Matrix Anal. Appl.},
	author = {Chandrasekaran, S. and Dewilde, P. and Gu, M. and Pals, T. and Sun, X. and van der Veen, A. J. and White, D.},
	urldate = {2022-03-05},
	date = {2005-01},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {65F30, 65F05, 65F20, backward stability, fast direct solver, fast multipole method, integral equations, method of moments, Moore--Penrose inverses, scattering theory, semiseparable matrices, spectral methods, time-varying systems},
	file = {Chandrasekaran et al. - 2005 - Some Fast Algorithms for Sequentially Semiseparabl.pdf:/home/stephan/snap/zotero-snap/common/Zotero/storage/FL2T83XW/Chandrasekaran et al. - 2005 - Some Fast Algorithms for Sequentially Semiseparabl.pdf:application/pdf},
}

@article{qiu_efficient_2015,
	title = {Efficient preconditioners for {PDE}-constrained optimization problems with a multi-level sequentially semi-separable matrix structure},
	volume = {44},
	pages = {3},
	number = {367},
	journaltitle = {Electronic Transactions on Numerical Analysis},
	author = {Qiu, Yue and Van Gijzen, Martin B and Van Wingerden, Jan-Willem and Verhaegen, Michel and Vuik, Cornelis},
	date = {2015},
	note = {Publisher: Institute of Computational Mathematics},
	file = {PDE_Cons_Opt.pdf:/home/stephan/snap/zotero-snap/common/Zotero/storage/UFM5AD7D/PDE_Cons_Opt.pdf:application/pdf;Qiu et al. - 2015 - Efficient preconditioners for PDE-constrained opti.pdf:/home/stephan/snap/zotero-snap/common/Zotero/storage/ZPERZ9WY/Qiu et al. - 2015 - Efficient preconditioners for PDE-constrained opti.pdf:application/pdf},
}

@thesis{rice_efficient_2010,
	title = {Efficient Algorithms for Distributed Control: A Structured Matrix Approach},
	url = {https://repository.tudelft.nl/islandora/object/uuid%3A50460cc9-2aba-4db5-b602-5373bb0bc609},
	shorttitle = {Efficient Algorithms for Distributed Control},
	abstract = {Distributed systems are all around us, and they are fascinating, and have an enormous potential to improve our lives, if their complexity can be properly harnessed. All scientists and engineers are aware of the great potential of this subject, since we witness fantastic distributed control systems every day, in the flocks of birds in the sky and fish in the sea. However, the collective behavior of millions of dynamically-coupled heterogeneous subsystems is hard to analyze and control for computational reasons. Our approach to the problems of analysis and control of distributed systems is to exploit the matrix structure of array-interconnected systems in fast iterative algorithms. Since these algorithms preserve the original matrix structure of the system, the resulting centrally optimal controller realizations have the same structure, which can conveniently be ‘redistributed’ into a set of subcontrollers linked in the same interconnection topology as the original system. For P interconnected subsystems, traditional analysis and control synthesis methods are O(P 3 ) computational complexity, but for N heterogeneous subsystems on a line, the above method is only O(N ) complexity. If the system is homogeneous with only heterogeneous boundary conditions, the complexity can be reduced to O(1), independent of the size of the homogeneous section. These results also extend to multiple spatial dimensions: for heterogeneous or homogeneous subsystems on an N ×M 2-D cartesian grid, the complexity is reduced to O(M N ) or O(1) complexity respectively, as compared to O(M 3 N 3 ) complexity of traditional techniques, an impressive improvement for very large systems N, M \&gt; 1000. Furthermore, due to the special form of the structured matrix arithmetic, the computations can actually be performed in a distributed way, on a distributed processor and memory system, with only linear complexity communication and memory requirements. Using these efficient structured techniques, one can perform stability and H2 and H? analysis to an arbitrary degree of accuracy, and sub-optimally upper-bound the structured singular value for robustness analysis. For synthesis, controllers with H2 and H? performance arbitrarily close to optimal are possible, and D-K iterations can be performed for robust design. Structure preserving model order reduction, and even system identification are also possible. It is also possible to apply this approach to analysis and synthesis of controllers for linear parameter varying({LPV}) systems, and of repetitive controllers for trials with many time steps, T ,in only O(T ) complexity which would otherwise be O(T 3 ).},
	institution = {Technische Universiteit Delft},
	type = {phdthesis},
	author = {Rice, J. K.},
	urldate = {2022-03-05},
	date = {2010},
	langid = {english},
	file = {Full Text PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/AZ7AY7DS/Rice - 2010 - Efficient Algorithms for Distributed Control A St.pdf:application/pdf;Snapshot:/home/stephan/snap/zotero-snap/common/Zotero/storage/INIANNV2/uuid50460cc9-2aba-4db5-b602-5373bb0bc609.html:text/html},
}

@inproceedings{rice_structure_2009,
	title = {Structure preserving model order reduction of heterogeneous 1-D distributed systems},
	doi = {10.1109/ACC.2009.5159801},
	abstract = {We consider the problem of model order reduction for spatially-varying interconnected systems distributed in one spatial dimension. The sequentially semi-separable matrix structure of such systems can be exploited to allow efficient structure preserving model order reduction using the matrix sign function. Iterative algorithms are provided for fast computation, which is demonstrated on an example.},
	eventtitle = {2009 American Control Conference},
	pages = {4109--4114},
	booktitle = {2009 American Control Conference},
	author = {Rice, Justin K. and Verhaegen, Michel},
	date = {2009-06},
	note = {{ISSN}: 2378-5861},
	keywords = {Iterative algorithms, Matrix decomposition, Control systems, Distributed computing, Distributed control, Interconnected systems, Linear matrix inequalities, Optimal control, Riccati equations, Stability},
	file = {IEEE Xplore Abstract Record:/home/stephan/snap/zotero-snap/common/Zotero/storage/A2FR9HKS/5159801.html:text/html},
}

@inproceedings{rice_distributed_2008,
	title = {Distributed control: A sequentially semi-separable approach},
	doi = {10.1109/CDC.2008.4738590},
	shorttitle = {Distributed control},
	abstract = {We consider the problem of designing controllers for spatially-varying interconnected systems distributed in one spatial dimension. The matrix structure of such systems can be exploited to allow fast analysis and design of centralized controllers with simple distributed implementations. Iterative algorithms are provided for stability analysis, H2 analysis, and sub-optimal controller synthesis. For practical implementation of the algorithms, approximations can be used, and the computational efficiency and accuracy of the algorithms incorporating these approximations are demonstrated on an example.},
	eventtitle = {2008 47th {IEEE} Conference on Decision and Control},
	pages = {2913--2919},
	booktitle = {2008 47th {IEEE} Conference on Decision and Control},
	author = {Rice, Justin K. and Verhaegen, Michel},
	date = {2008-12},
	note = {{ISSN}: 0191-2216},
	keywords = {Iterative algorithms, Control systems, Distributed control, Interconnected systems, Optimal control, Riccati equations, Computational efficiency, Control system synthesis, Hydrogen, Iterative methods},
	file = {IEEE Xplore Full Text PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/YCZ2QXS2/Rice und Verhaegen - 2008 - Distributed control A sequentially semi-separable.pdf:application/pdf},
}

@inproceedings{rice_structure_2009-1,
	title = {Structure preserving model order reduction of heterogeneous 1-D distributed systems},
	doi = {10.1109/ACC.2009.5159801},
	abstract = {We consider the problem of model order reduction for spatially-varying interconnected systems distributed in one spatial dimension. The sequentially semi-separable matrix structure of such systems can be exploited to allow efficient structure preserving model order reduction using the matrix sign function. Iterative algorithms are provided for fast computation, which is demonstrated on an example.},
	eventtitle = {2009 American Control Conference},
	pages = {4109--4114},
	booktitle = {2009 American Control Conference},
	author = {Rice, Justin K. and Verhaegen, Michel},
	date = {2009-06},
	note = {{ISSN}: 2378-5861},
	keywords = {Iterative algorithms, Matrix decomposition, Control systems, Distributed computing, Distributed control, Interconnected systems, Linear matrix inequalities, Optimal control, Riccati equations, Stability},
	file = {IEEE Xplore Full Text PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/Z5NWQ2YJ/Rice und Verhaegen - 2009 - Structure preserving model order reduction of hete.pdf:application/pdf},
}

@article{sandberg_balanced_2004,
	title = {Balanced truncation of linear time-varying systems},
	volume = {49},
	issn = {1558-2523},
	doi = {10.1109/TAC.2003.822862},
	abstract = {In this paper, balanced truncation of linear time-varying systems is studied in discrete and continuous time. Based on relatively basic calculations with time-varying Lyapunov equations/inequalities we are able to derive both upper and lower error bounds for the truncated models. These results generalize well-known time-invariant formulas. The case of time-varying state dimension is considered. Input-output stability of all truncated balanced realizations is also proven. The method is finally successfully applied to a high-order model.},
	pages = {217--229},
	number = {2},
	journaltitle = {{IEEE} Transactions on Automatic Control},
	author = {Sandberg, H. and Rantzer, A.},
	date = {2004-02},
	note = {Conference Name: {IEEE} Transactions on Automatic Control},
	keywords = {Riccati equations, Asymptotic stability, Councils, Linear approximation, Linear systems, Nonlinear systems, Reduced order systems, Robust control, Sufficient conditions, Time varying systems},
	file = {IEEE Xplore Abstract Record:/home/stephan/snap/zotero-snap/common/Zotero/storage/GLIGRZ36/1266776.html:text/html;Volltext:/home/stephan/snap/zotero-snap/common/Zotero/storage/G2QJMPI2/Sandberg und Rantzer - 2004 - Balanced truncation of linear time-varying systems.pdf:application/pdf},
}

@inproceedings{chandrasekaran_fast_2002,
	location = {Berlin, Heidelberg},
	title = {Fast Stable Solver for Sequentially Semi-separable Linear Systems of Equations},
	isbn = {978-3-540-36265-4},
	doi = {10.1007/3-540-36265-7_51},
	abstract = {In this paper we will present a fast backward stable algorithm for the solution of certain structured matrices which can be either sparse or dense. It essentially combines the fast solution techniques for banded plus semi-separable linear systems of equations of Chandrasekaran and Gu [4] with similar techniques of Dewilde and van der Veen for time-varying systems [12].},
	pages = {545--554},
	booktitle = {High Performance Computing — {HiPC} 2002},
	publisher = {Springer},
	author = {Chandrasekaran, S. and Dewilde, P. and Gu, M. and Pals, T. and van der Veen, A. J.},
	editor = {Sahni, Sartaj and Prasanna, Viktor K. and Shukla, Uday},
	date = {2002},
	langid = {english},
	file = {Springer Full Text PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/GKX6QPZN/Chandrasekaran et al. - 2002 - Fast Stable Solver for Sequentially Semi-separable.pdf:application/pdf},
}

@article{tong_blind_2003,
	title = {Blind decorrelating {RAKE} receivers for long-code {WCDMA}},
	volume = {51},
	issn = {1941-0476},
	doi = {10.1109/TSP.2003.811230},
	abstract = {The problem of blind and semiblind channel estimation and symbol detection is considered for long-code wideband code division multiple access ({CDMA}) systems, including systems with multirate and multicode transmissions. A decorrelating matched filter, implemented efficiently in state-space, eliminates multiaccess interference and produces a bank of vector processes. Each vector process spans a one-dimensional (1-D) subspace from which channel parameters and data symbols of one user are estimated jointly by least squares. A new identifiability condition is established, which suggests that channels unidentifiable, in short-code {CDMA} systems are almost surely identifiable when aperiodic spreading codes are used. The decorrelating matched filter is implemented efficiently based on time-varying state-space realizations that exploit the structure of sparsity of the code matrix. The mean square error of the estimated channel is compared to the Cramer-Rao bound, and a bit error rate ({BER}) expression for the proposed algorithm is presented.},
	pages = {1642--1655},
	number = {6},
	journaltitle = {{IEEE} Transactions on Signal Processing},
	author = {Tong, Lang and van der Veen, A.-J. and Dewilde, P. and Sung, Youngchul},
	date = {2003-06},
	note = {Conference Name: {IEEE} Transactions on Signal Processing},
	keywords = {Bit error rate, Channel estimation, Decorrelation, Fading, Interference elimination, Least squares approximation, Matched filters, Multiaccess communication, Multipath channels, Wideband},
	file = {Volltext:/home/stephan/snap/zotero-snap/common/Zotero/storage/IIMNMUNC/Tong et al. - 2003 - Blind decorrelating RAKE receivers for long-code W.pdf:application/pdf},
}


%=====================================================================================================================
%Entries for literature review
%=====================================================================================================================

@inproceedings{yu_compressing_2017,
	title = {On Compressing Deep Models by Low Rank and Sparse Decomposition},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Yu_On_Compressing_Deep_CVPR_2017_paper.html},
	eventtitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {7370--7379},
	author = {Yu, Xiyu and Liu, Tongliang and Wang, Xinchao and Tao, Dacheng},
	urldate = {2021-11-23},
	date = {2017},
	file = {Snapshot:/home/stephan/snap/zotero-snap/common/Zotero/storage/QQT5PMEQ/Yu_On_Compressing_Deep_CVPR_2017_paper.html:text/html;Full Text PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/2ABSW4SD/Yu et al. - 2017 - On Compressing Deep Models by Low Rank and Sparse .pdf:application/pdf},
}

@article{fan_multiscale_2019,
	title = {A Multiscale Neural Network Based on Hierarchical Matrices},
	volume = {17},
	issn = {1540-3459},
	url = {https://epubs.siam.org/doi/abs/10.1137/18M1203602},
	doi = {10.1137/18M1203602},
	abstract = {In this work we introduce a new multiscale artificial neural network based on the structure of \${\textbackslash}mathcal\{H\}\$-matrices. This network generalizes the latter to the nonlinear case by introducing a local deep neural network at each spatial scale. Numerical results indicate that the network is able to efficiently approximate discrete nonlinear maps obtained from discretized nonlinear partial differential equations, such as those arising from nonlinear Schrödinger equations and the Kohn--Sham density functional theory.},
	pages = {1189--1213},
	number = {4},
	journaltitle = {Multiscale Modeling \& Simulation},
	shortjournal = {Multiscale Model. Simul.},
	author = {Fan, Yuwei and Lin, Lin and Ying, Lexing and Zepeda-Núñez, Leonardo},
	urldate = {2022-02-18},
	date = {2019-01},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {\${\textbackslash}mathcal\{H\}\$-matrix, 60H35, 65F30, 65M75, convolutional neural network, locally connected neural network, multiscale neural network},
	file = {Eingereichte Version:/home/stephan/snap/zotero-snap/common/Zotero/storage/N99YILHC/Fan et al. - 2019 - A Multiscale Neural Network Based on Hierarchical .pdf:application/pdf;18m1203602.pdf:/home/stephan/snap/zotero-snap/common/Zotero/storage/IHIZNB3F/18m1203602.pdf:application/pdf},
}

@article{wu_hybrid_2020,
	title = {Hybrid tensor decomposition in neural network compression},
	volume = {132},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608020303294},
	doi = {10.1016/j.neunet.2020.09.006},
	abstract = {Deep neural networks ({DNNs}) have enabled impressive breakthroughs in various artificial intelligence ({AI}) applications recently due to its capability of learning high-level features from big data. However, the current demand of {DNNs} for computational resources especially the storage consumption is growing due to that the increasing sizes of models are being required for more and more complicated applications. To address this problem, several tensor decomposition methods including tensor-train ({TT}) and tensor-ring ({TR}) have been applied to compress {DNNs} and shown considerable compression effectiveness. In this work, we introduce the hierarchical Tucker ({HT}), a classical but rarely-used tensor decomposition method, to investigate its capability in neural network compression. We convert the weight matrices and convolutional kernels to both {HT} and {TT} formats for comparative study, since the latter is the most widely used decomposition method and the variant of {HT}. We further theoretically and experimentally discover that the {HT} format has better performance on compressing weight matrices, while the {TT} format is more suited for compressing convolutional kernels. Based on this phenomenon we propose a strategy of hybrid tensor decomposition by combining {TT} and {HT} together to compress convolutional and fully connected parts separately and attain better accuracy than only using the {TT} or {HT} format on convolutional neural networks ({CNNs}). Our work illuminates the prospects of hybrid tensor decomposition for neural network compression.},
	pages = {309--320},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Wu, Bijiao and Wang, Dingheng and Zhao, Guangshe and Deng, Lei and Li, Guoqi},
	urldate = {2022-02-18},
	date = {2020-12-01},
	langid = {english},
	keywords = {Balanced structure, Hierarchical Tucker, Hybrid tensor decomposition, Neural network compression, Tensor-train},
	file = {ScienceDirect Snapshot:/home/stephan/snap/zotero-snap/common/Zotero/storage/DPG9GGZZ/S0893608020303294.html:text/html;Eingereichte Version:/home/stephan/snap/zotero-snap/common/Zotero/storage/PYE8ISCU/Wu et al. - 2020 - Hybrid tensor decomposition in neural network comp.pdf:application/pdf},
}

@inproceedings{hassibi_optimal_1993,
	title = {Optimal Brain Surgeon and general network pruning},
	doi = {10.1109/ICNN.1993.298572},
	abstract = {The use of information from all second-order derivatives of the error function to perform network pruning (i.e., removing unimportant weights from a trained network) in order to improve generalization, simplify networks, reduce hardware or storage requirements, increase the speed of further training, and, in some cases, enable rule extraction, is investigated. The method, Optimal Brain Surgeon ({OBS}), is significantly better than magnitude-based methods and Optimal Brain Damage, which often remove the wrong weights. {OBS}, permits pruning of more weights than other methods (for the same error on the training set), and thus yields better generalization on test data. Crucial to {OBS} is a recursion relation for calculating the inverse Hessian matrix H/sup -1/ from training data and structural information of the set. {OBS} deletes the correct weights from a trained {XOR} network in every case.{\textless}{\textgreater}},
	eventtitle = {{IEEE} International Conference on Neural Networks},
	pages = {293--299 vol.1},
	booktitle = {{IEEE} International Conference on Neural Networks},
	author = {Hassibi, B. and Stork, D.G. and Wolff, G.J.},
	date = {1993-03},
	keywords = {Machine learning, Training data, Backpropagation, Benchmark testing, Biological neural networks, Data mining, Hardware, Pattern recognition, Statistics, Surges},
	file = {Akzeptierte Version:/home/stephan/snap/zotero-snap/common/Zotero/storage/5NUFGW2D/Hassibi et al. - 1993 - Optimal Brain Surgeon and general network pruning.pdf:application/pdf;IEEE Xplore Abstract Record:/home/stephan/snap/zotero-snap/common/Zotero/storage/P7944RPW/298572.html:text/html},
}

@inproceedings{dao_kaleidoscope_2020,
	title = {Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps},
	url = {https://iclr.cc/virtual_2020/poster_BkgrBgSYDS.html},
	shorttitle = {Kaleidoscope},
	abstract = {Modern neural network architectures use structured linear transformations, such as low-rank matrices, sparse matrices, permutations, and the Fourier transform, to improve inference speed and reduce memory usage compared to general linear maps. However, choosing which of the myriad structured transformations to use (and its associated parameterization) is a laborious task that requires trading off speed, space, and accuracy. We consider a different approach: we introduce a family of matrices called kaleidoscope matrices (K-matrices) that provably capture any structured matrix with near-optimal space (parameter) and time (arithmetic operation) complexity. We empirically validate that K-matrices can be automatically learned within end-to-end pipelines to replace hand-crafted procedures, in order to improve model quality. For example, replacing channel shuffles in {ShuffleNet} improves classification accuracy on {ImageNet} by up to 5\%. K-matrices can also simplify hand-engineered pipelines---we replace filter bank feature computation in speech data preprocessing with a learnable kaleidoscope layer, resulting in only 0.4\% loss in accuracy on the {TIMIT} speech recognition task. In addition, K-matrices can capture latent structure in models: for a challenging permuted image classification task, adding a K-matrix to a standard convolutional architecture can enable learning the latent permutation and improve accuracy by over 8 points. We provide a practically efficient implementation of our approach, and use K-matrices in a Transformer network to attain 36\% faster end-to-end inference speed on a language translation task.},
	eventtitle = {Eighth International Conference on Learning Representations},
	author = {Dao, Tri and Sohoni, Nimit and Gu, Albert and Eichhorn, Matthew and Blonder, Amit and Leszczynski, Megan and Rudra, Atri and Ré, Christopher},
	urldate = {2022-02-18},
	date = {2020-04},
	langid = {english},
	file = {Full Text PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/NNEJ8H7Z/Dao et al. - 2020 - Kaleidoscope An Efficient, Learnable Representati.pdf:application/pdf;Snapshot:/home/stephan/snap/zotero-snap/common/Zotero/storage/CP5X4RFB/poster_BkgrBgSYDS.html:text/html},
}

@article{li_butterfly-net_2020,
	title = {Butterfly-Net: Optimal Function Representation Based on Convolutional Neural Networks},
	volume = {28},
	issn = {1815-2406, 1991-7120},
	url = {http://global-sci.org/intro/article_detail/cicp/18398.html},
	doi = {10.4208/cicp.OA-2020-0214},
	shorttitle = {Butterfly-Net},
	pages = {1838--1885},
	number = {5},
	journaltitle = {Communications in Computational Physics},
	shortjournal = {{CiCP}},
	author = {Li, Yingzhou and Cheng, Xiuyuan and Lu, Jianfeng},
	urldate = {2022-02-18},
	date = {2020-06},
	file = {Eingereichte Version:/home/stephan/snap/zotero-snap/common/Zotero/storage/VSC87ETK/Li - 2020 - Butterfly-Net Optimal Function Representation Bas.pdf:application/pdf;1805.07451.pdf:/home/stephan/snap/zotero-snap/common/Zotero/storage/PK5NHZ4F/1805.07451.pdf:application/pdf},
}

@article{dettmers_sparse_2019,
	title = {Sparse Networks from Scratch: Faster Training without Losing Performance},
	url = {http://arxiv.org/abs/1907.04840},
	shorttitle = {Sparse Networks from Scratch},
	abstract = {We demonstrate the possibility of what we call sparse learning: accelerated training of deep neural networks that maintain sparse weights throughout training while achieving dense performance levels. We accomplish this by developing sparse momentum, an algorithm which uses exponentially smoothed gradients (momentum) to identify layers and weights which reduce the error efficiently. Sparse momentum redistributes pruned weights across layers according to the mean momentum magnitude of each layer. Within a layer, sparse momentum grows weights according to the momentum magnitude of zero-valued weights. We demonstrate state-of-the-art sparse performance on {MNIST}, {CIFAR}-10, and {ImageNet}, decreasing the mean error by a relative 8\%, 15\%, and 6\% compared to other sparse algorithms. Furthermore, we show that sparse momentum reliably reproduces dense performance levels while providing up to 5.61x faster training. In our analysis, ablations show that the benefits of momentum redistribution and growth increase with the depth and size of the network. Additionally, we find that sparse momentum is insensitive to the choice of its hyperparameters suggesting that sparse momentum is robust and easy to use.},
	journaltitle = {{arXiv}:1907.04840 [cs, stat]},
	author = {Dettmers, Tim and Zettlemoyer, Luke},
	urldate = {2022-02-18},
	date = {2019-08-23},
	eprinttype = {arxiv},
	eprint = {1907.04840},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/N243G9EP/Dettmers und Zettlemoyer - 2019 - Sparse Networks from Scratch Faster Training with.pdf:application/pdf;arXiv.org Snapshot:/home/stephan/snap/zotero-snap/common/Zotero/storage/Q9SNTJLT/1907.html:text/html},
}

@article{li_butterfly_2015,
	title = {Butterfly Factorization},
	volume = {13},
	issn = {1540-3459},
	url = {https://epubs.siam.org/doi/abs/10.1137/15M1007173},
	doi = {10.1137/15M1007173},
	abstract = {The paper introduces the butterfly factorization as a data-sparse approximation for the matrices that satisfy a complementary low-rank property. The factorization can be constructed efficiently if either fast algorithms for applying the matrix and its adjoint are available or the entries of the matrix can be sampled individually. For an \$N{\textbackslash}times N\$ matrix, the resulting factorization is a product of \$O({\textbackslash}log N)\$ sparse matrices, each with \$O(N)\$ nonzero entries. Hence, it can be applied rapidly in \$O(N{\textbackslash}log N)\$ operations. Numerical results are provided to demonstrate the effectiveness of the butterfly factorization and its construction algorithms.},
	pages = {714--732},
	number = {2},
	journaltitle = {Multiscale Modeling \& Simulation},
	shortjournal = {Multiscale Model. Simul.},
	author = {Li, Yingzhou and Yang, Haizhao and Martin, Eileen R. and Ho, Kenneth L. and Ying, Lexing},
	urldate = {2022-02-18},
	date = {2015-01},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {44A55, 65R10, 65T50, butterfly algorithm, data-sparse matrix, Fourier integral operators, matrix factorization, operator compression, randomized algorithm, special functions},
	file = {Eingereichte Version:/home/stephan/snap/zotero-snap/common/Zotero/storage/N6HLAF8X/Li et al. - 2015 - Butterfly Factorization.pdf:application/pdf},
}

@report{parker_random_1995,
	title = {Random Butterfly Transformations with Applications in Computational Linear Algebra},
	url = {http://web.cs.ucla.edu/~stott/},
	abstract = {Theory and practice of computational linear algebra differ over the issue of degeneracy. Block matrix decompositions are used heavily in theory, but less in practice, since even when a matrix is nondegenerate (has full rank) its block submatrices can be degenerate. The potential degeneracy of block submatrices can completely prevent practical use of block matrix algorithms. Gaussian elimination is an important example of an algorithm affected by the possibility of degeneracy. While the basic elimination procedure is simple to state and implement, it becomes more complicated with the addition of a pivoting procedure, which handles degenerate matrices having zeros on the diagonal. Pivoting can significantly complicate the algorithm, increase data movement, and reduce speed, particularly on high-performance computers. We propose a randomization scheme that preconditions an input matrix by multiplying it with random matrices, where this multiplication can be performed efficiently. At the e...},
	author = {Parker, D. Stott},
	date = {1995},
	file = {CSD-950023.pdf:/home/stephan/snap/zotero-snap/common/Zotero/storage/AWMRRXRE/CSD-950023.pdf:application/pdf},
}

@inproceedings{louizos_learning_2018,
	title = {Learning Sparse Neural Networks through L\_0 Regularization},
	url = {https://openreview.net/forum?id=H1Y8hhg0b},
	abstract = {We show how to optimize the expected L\_0 norm of parametric models with gradient descent and introduce a new distribution that facilitates hard gating.},
	eventtitle = {International Conference on Learning Representations},
	author = {Louizos, Christos and Welling, Max and Kingma, Diederik P.},
	urldate = {2022-02-21},
	date = {2018-02-15},
	langid = {english},
	file = {Full Text PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/EA9QZ2PR/Louizos et al. - 2018 - Learning Sparse Neural Networks through L_0 Regula.pdf:application/pdf;Snapshot:/home/stephan/snap/zotero-snap/common/Zotero/storage/5XND2X2A/forum.html:text/html},
}

@inproceedings{wen_learning_2016,
	title = {Learning Structured Sparsity in Deep Neural Networks},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/hash/41bfd20a38bb1b0bec75acf0845530a7-Abstract.html},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
	urldate = {2022-02-21},
	date = {2016},
	file = {Full Text PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/P8C4LBEC/Wen et al. - 2016 - Learning Structured Sparsity in Deep Neural Networ.pdf:application/pdf},
}

@inproceedings{ailon_sparse_2021,
	title = {Sparse linear networks with a fixed butterfly structure: theory and practice},
	url = {https://proceedings.mlr.press/v161/ailon21a.html},
	shorttitle = {Sparse linear networks with a fixed butterfly structure},
	abstract = {A butterfly network consists of logarithmically many layers, each with a linear number of non-zero weights (pre-specified). The fast Johnson-Lindenstrauss transform ({FJLT}) can be represented as a butterfly network followed by a projection onto a random subset of the coordinates. Moreover, a random matrix based on {FJLT} with high probability approximates the action of any matrix on a vector. Motivated by these facts, we propose to replace a dense linear layer in any neural network by an architecture based on the butterfly network. The proposed architecture significantly improves upon the quadratic number of weights required in a standard dense layer to nearly linear with little compromise in expressibility of the resulting operator. In a collection of wide variety of experiments, including supervised prediction on both the {NLP} and vision data, we show that this not only produces results that match and at times outperform existing well-known architectures, but it also offers faster training and prediction in deployment. To understand the optimization problems posed by neural networks with a butterfly network, we also study the optimization landscape of the encoder-decoder network, where the encoder is replaced by a butterfly network followed by a dense linear layer in smaller dimension. Theoretical result presented in the paper explains why the training speed and outcome are not compromised by our proposed approach.},
	eventtitle = {Uncertainty in Artificial Intelligence},
	pages = {1174--1184},
	booktitle = {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},
	publisher = {{PMLR}},
	author = {Ailon, Nir and Leibovitch, Omer and Nair, Vineet},
	urldate = {2022-02-21},
	date = {2021-12-01},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/HMPMZ8E4/Ailon et al. - 2021 - Sparse linear networks with a fixed butterfly stru.pdf:application/pdf;Supplementary PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/3K9F8MBG/Ailon et al. - 2021 - Sparse linear networks with a fixed butterfly stru.pdf:application/pdf},
}

@inproceedings{jaderberg_speeding_2014,
	title = {Speeding up Convolutional Neural Networks with Low Rank Expansions},
	doi = {http://dx.doi.org/10.5244/C.28.88},
	booktitle = {Proceedings of the British Machine Vision Conference},
	publisher = {{BMVA} Press},
	author = {Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
	date = {2014},
	file = {Jaderberg et al. - 2014 - Speeding up Convolutional Neural Networks with Low.pdf:/home/stephan/snap/zotero-snap/common/Zotero/storage/82EBBFY4/Jaderberg et al. - 2014 - Speeding up Convolutional Neural Networks with Low.pdf:application/pdf;paper073.pdf:/home/stephan/snap/zotero-snap/common/Zotero/storage/7SAHQC2J/paper073.pdf:application/pdf},
}

@inproceedings{ioannou_training_2016,
	title = {Training {CNNs} with Low-Rank Filters for Efficient Image Classification},
	doi = {10.13140/RG.2.1.3727.8163},
	abstract = {Published as a conference paper at {ICLR} 2016

Trained Models at http://dx.doi.org/10.5281/zenodo.53189},
	author = {Ioannou, Yani and Robertson, Duncan and Shotton, Jamie and Cipolla, Roberto and Criminisi, Antonio},
	date = {2016-05-02},
	file = {Full Text PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/HJPXHIUB/Ioannou et al. - 2016 - Training CNNs with Low-Rank Filters for Efficient .pdf:application/pdf},
}

@article{blalock_what_2020,
	title = {What is the State of Neural Network Pruning?},
	volume = {2},
	url = {https://proceedings.mlsys.org/paper/2020/hash/d2ddea18f00665ce8623e36bd4e3c7c5-Abstract.html},
	pages = {129--146},
	journaltitle = {Proceedings of Machine Learning and Systems},
	author = {Blalock, Davis and Gonzalez Ortiz, Jose Javier and Frankle, Jonathan and Guttag, John},
	urldate = {2022-02-21},
	date = {2020-03-15},
	langid = {english},
	file = {Full Text PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/HEQQYRFR/Blalock et al. - 2020 - What is the State of Neural Network Pruning.pdf:application/pdf;Snapshot:/home/stephan/snap/zotero-snap/common/Zotero/storage/IJPBTXPB/d2ddea18f00665ce8623e36bd4e3c7c5-Abstract.html:text/html},
}

@inproceedings{ithapu_decoding_2017,
	title = {Decoding the Deep: Exploring Class Hierarchies of Deep Representations Using Multiresolution Matrix Factorization},
	url = {https://openaccess.thecvf.com/content_cvpr_2017_workshops/w26/html/Ithapu_Decoding_the_Deep_CVPR_2017_paper.html},
	shorttitle = {Decoding the Deep},
	eventtitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition Workshops},
	pages = {45--54},
	author = {Ithapu, Vamsi K.},
	urldate = {2022-03-07},
	date = {2017},
	file = {Full Text PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/7432CCFJ/Ithapu - 2017 - Decoding the Deep Exploring Class Hierarchies of .pdf:application/pdf;Snapshot:/home/stephan/snap/zotero-snap/common/Zotero/storage/ZGMIYJI6/Ithapu_Decoding_the_Deep_CVPR_2017_paper.html:text/html},
}

@inproceedings{rigamonti_learning_2013,
	title = {Learning Separable Filters},
	url = {https://openaccess.thecvf.com/content_cvpr_2013/html/Rigamonti_Learning_Separable_Filters_2013_CVPR_paper.html},
	eventtitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {2754--2761},
	author = {Rigamonti, Roberto and Sironi, Amos and Lepetit, Vincent and Fua, Pascal},
	urldate = {2022-03-10},
	date = {2013},
	file = {Full Text PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/RZVWQNRI/Rigamonti et al. - 2013 - Learning Separable Filters.pdf:application/pdf;Snapshot:/home/stephan/snap/zotero-snap/common/Zotero/storage/I6VNFB85/Rigamonti_Learning_Separable_Filters_2013_CVPR_paper.html:text/html},
}

@inproceedings{zhou_greedy_2013,
	title = {Greedy Bilateral Sketch, Completion \& Smoothing},
	url = {https://proceedings.mlr.press/v31/zhou13b.html},
	abstract = {Recovering a large low-rank matrix from highly corrupted, incomplete or sparse outlier overwhelmed observations is the crux of various intriguing statistical problems. We explore the power of "greedy bilateral ({GreB})" paradigm in reducing both time and sample complexities for solving these problems. {GreB} models a low-rank variable as a bilateral factorization, and updates the left and right factors in a mutually adaptive and greedy incremental manner. We detail how to model and solve low-rank approximation, matrix completion and robust {PCA} in {GreB}’s paradigm. On their {MATLAB} implementations, approximating a noisy 10000x10000 matrix of rank 500 with {SVD} accuracy takes 6s; {MovieLens}10M matrix of size 69878x10677 can be completed in 10s from 30\% of 10{\textasciicircum}7 ratings with {RMSE} 0.86 on the rest 70\%; the low-rank background and sparse moving outliers in a 120x160 video of 500 frames are accurately separated in 1s. This brings 30 to 100 times acceleration in solving these popular statistical problems.},
	eventtitle = {Artificial Intelligence and Statistics},
	pages = {650--658},
	booktitle = {Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics},
	publisher = {{PMLR}},
	author = {Zhou, Tianyi and Tao, Dacheng},
	urldate = {2022-03-10},
	date = {2013-04-29},
	langid = {english},
	note = {{ISSN}: 1938-7228},
	file = {Full Text PDF:/home/stephan/snap/zotero-snap/common/Zotero/storage/FHS7RRHQ/Zhou und Tao - 2013 - Greedy Bilateral Sketch, Completion & Smoothing.pdf:application/pdf},
}




