% Possible types of documents/theses
%   doctype=bachelorsthesis
%   doctype=mastersthesis
%   doctype=idp
%   doctype=phdthesis
%   doctype=studienarbeit
%   doctype=diplomarbeit
%
% Document language
%   without 'lang' attribute: English
%   lang=ngerman:             German (new orthography)
%
% Binding correction
%   BCOR=<Längenangabe>
%   Additional margin, which is invisible due to binding the book
%   The usual binding by the Fachschaft has a thickness of 1,5 cm 
%
% biblatex (citations)
%   This requires 'biber' to be used instead of 'bibtex', please
%   adapt your editor's settings accordingly!
\documentclass[doctype=mastersthesis,BCOR=15mm,biblatex]{ldvbook}%lang=ngerman

% !TeX spellcheck = en_US

\usepackage{pgfplots}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{matrix}
\usetikzlibrary{ decorations.markings}
\usetikzlibrary {decorations.shapes}

\input{diagrams/systems.tex}

\usepackage[boxed]{algorithm}
\usepackage{algpseudocode}

\usepackage{todonotes}
\usepackage{subcaption}
\usepackage{mathdots}

% Look for citation sources in the database "diplomarbeit.bib"
\addbibresource{thesis.bib}


%operator declarations
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\triu}{triu}
\DeclareMathOperator{\tril}{tril}
\DeclareMathOperator{\diag}{diag}



%define symbols
\newcommand{\R}{\mathcal{R}} %Reachabilityy matrix
\newcommand{\Ob}{\mathcal{O}} %Observability operator 
\newcommand{\eye}{I} %identity matrix
\newcommand{\bigO}{O}


\begin{document}

% Bibliographic information about the thesis, please change accordingly!
\title{Combining Matrix Representations for Structured Approximation of Neural Network Weight Matrices}
\author{Stephan Nüßlein}
\license{CC-BY}
\supervisor{Matthias Kissel}


\maketitle[frontcover=Design1]


\chapter*{Abstract}

\begin{itemize}
	\item neural nets
	\item reduce cost of neural nets using structured matrix
	\item how to use Sequentially semisperable matrices in this context
	\item choose number of stages and tuning segmentation
	\item some sentence on results
\end{itemize}


\tableofcontents

60-80 pages total

% Please compile this example document including the bibliography
% database. Check the resulting document and the references for 
% correct appearance (especially the German Umlaute).
% Thus you ensure that LaTeX is detecting the character encoding
% correctly and your build chain is working.
% If it does not, please tell your supervisor.






\chapter{Introduction} 3p
Motivation Why

What others are dooing

What has been done here

increasing cost

Order of matrix vector product, makes larger systems expensive

\todo{why ML interesting}

As the matrices for fully connected layers get larger the evaluation of the neural nets get computationally more expensive. This is often problematic as the computational resources are a limiting factor, especially on embedded or mobile systems.


The linear layers in neural nets can be represented with matrices.
Usually full matrices are used.
When evaluation the neural nets one has to compute the matrix vector product.
For a full unstructured matrix this is of order $O(n^2)$. 
Whereas the cost for calculating the nonlinear part is usually linear in the number of neurons the cost for the calculation of the liner layers is quadratic in the number of neurons.
This means that a larger number of neurons the cost of the linear layer inccears 
\todo{reword}


There are also matrix structures where the cost of the matrix vector product is of lower order.
These have been developed to help solving PDEs like $\mathcal{H}$-Matrices, arise in different circumstances like semiseperabel matrices, %Vander Bib

or are description of time varying systems like sequentially semiseperable matrices.
All of them can not only be represented as a matrix but also have a different representation.
It is possible to use these representations to construct algorithms that can compute the matrix vector product.
One example of this are circulant matrices. 
These describe time invariant systems and therefore convolutions \todo{check and reword}
Using the DFT matrices, circulant matrices can be transformed into diagonal matrices.
This allows us to obtain a faster algorithm for the product $y=Tu$ by suing the Fast Fourier Trandform(FFT):
We can first compute the FFT of the input, then multiply the vector element-wise with a weight vector. Finally we use the inverse FFT to obtian the result $y$.
Instead of of the order $\bigO(n^2)$ we now have $\bigO(n\log(n))$.
This result is used in widely used in Convolutional Neural Networks (CNN).
Instead of using a matrix vector products the underlying structure is utilized and convolutions are used.

One might analogously try to use other structures to represent weight matrices:
A possible representation are sequentially semiseperable matrices. 
These matrices represent the input output behaviour of
Time varing systems.
These systems consist of  subsystems, so called stages that can change over time.
%Every stage can be described using four small matrices.\todo{reword}
Therefore we can also use this state space representation of the system to describe the matrix.
It is also possible to to calculate the matrix-vector product using algorithms based on this representation.
%The number of inputs and outputs of a system also create a segmentation of the system.
The definition of the Sequentially semiseperable matrix also results in the fact that these matrices have a underlying segmentation.
This segmentation is often determined by a known system ot an underlying physical system and therefore known. 
The matrices in neural networks do not necessarily have the same underlying structure.
Even if the matrix is close to a sequentially semiseperable matrix the structural parameters are usually not know.
To represent the matrices, one need algorithms to derive these structural parameters.
%This can be split in different sub problems.
%Firs the number of stages is not known.
There are existing algorithms to calculate the state space representation for a known segmentation.
After guessing the structural parameters we use these algorithms to create a system.
After this we refine the parameters by an algorithm that can change the structural parameters of a system.
As the weight matrices usually are not sequentuilly semiseperable we also need to approximate the system, if we want to reduce the computational cost.
This will also be incorporated in the algorithm.

In section different approaches to use structured matrices in neural nets are collected.
In section related matrix structures will be presented.
After this the Sequentually semiseperable matrices that will be used in this thesis will be explained in more detail in chapter
In chapter <methods> the methods to create a state space representation for a matrix with unknown structural parameters will be presented.
%the sequentially semeiseperabel structure will be used to approximate matrices. 
%For these matrices it the structural parameters are not known a prior.
In section <approx> an algorithm for the approximation of a system with a different system is explored.
%In section we give an algorithm, for the approximation of systems.
Based on this choose the structural parameters are chosen.
In section <> an approach to determine the number of stages for a system will be presented
We still have to determine the input and output dimensions.
%An Algorithm to adapt the input and output dimentiosn of a system will be explained.
This is done by starting with an initial guess and then refine the structure by adapt the input and output dimensions.
In section <> an algorithm to change these dimensions is given.
Additionally this section presents strategies how to use these algorithms in cases where the final structure has to be determined.
In chapter the algorithms will be tested.
Section contains the results for test cases and in section the algorithm are used to approxiamte weight matrices.

´

\chapter{Literature Review}
\todo[inline]{Some test}

\section{Neural Nets}
%Special matrix structures are been explored for the use in neural nets.
%Here different matrices structures are used to improve the performance. 
In this section different approaches that use structured matrices in neural nets are shortly introduced.
\todo[inline]{moved to introduction delete/replace this}
\textcolor{blue}{The linear layers in matrices can be represented with matrices.
Usually full matrices are used. These do not have a particular structure.
It is also possible to use structured matrices.
These often have representations for which a matrix vector products can be computed with a reduced cost.
Instead of using the matrix vector product one might use these structured representations that are computationally cheaper.
%Instead of using matrix vector products the underlying structure is used to replace the multiplications with convolutions.
%One might analogously try to use the structure of a matrix to represent it with a different representation that has a reduced computational cost.
One widely known example are Convolutional Neural Networks (CNN).
The convolutions are equivalent with an multiplication with a circulant matrix.
Instead of using a matrix vector products the underlying structure is utilized and convolutions are used.
One might analogously try to use the structure of a matrix to represent it with a different representation that has a reduced computational cost.}
There are two main approaches to obtain structured matrices:
In one case the structure is predetermined and the parameters of the structure are trained by a regular training process \cite{fan_multiscale_2019,dao_kaleidoscope_2020,li_butterfly_2015,ailon_sparse_2021,ioannou_training_2016}.
Or the neural net is trained without a predetermined matrix structures and the matrices are later approximated with structured matrices \cite{wu_hybrid_2020,hassibi_optimal_1993,jaderberg_speeding_2014,rigamonti_learning_2013}. In \cite{yu_compressing_2017} the model was retrained after the approximation. 
There are also approaches where the approximation is done during the training procedure like \cite{dettmers_sparse_2019} or an structure is created using regularization \cite{louizos_learning_2018,wen_learning_2016}.


One widely known approach to reduce the complexity of neural nets is pruning. 
These are processes that set elements in neural net matrices to 0 to reduce the computational cost \cite{blalock_what_2020}.
An approach to remove parameters from a trained net based on the Hessian of the parameters\todo{how to phrase} was proposed by Hassibi \cite{hassibi_optimal_1993}.
It is also possible to obtain sparse matrices while training.
Dettmers and Zettlemoyer use an algorithm that includes pruning steps during the training after every epoch \cite{dettmers_sparse_2019}.
A different approach to obtain sparse matrices is including a regularization term.
As an example Louizos et al. use $L_0$ regularization  \cite{louizos_learning_2018} and Wen et al. use LASSO regularization \cite{wen_learning_2016}.

Low Rank approximations for filters in convolutional layers were proposed by Rigamonti et al. \cite{rigamonti_learning_2013}. These use combinations of separable filters. Jaderberg et al. approximate existing filters using low rank filters \cite{jaderberg_speeding_2014}. 
Ioannou et al. directly learn low rank filters and combine them later \cite{ioannou_training_2016}.
In low rank and sparse decomposition a matrix is approximated with a low rank and a sparse matrix. 
Yu et al. represented matrices in deep models as a sum of low rank and sparse matrices \cite{yu_compressing_2017}.
The computation of the decomposition is based on an algorithm to calculate a low rank and sparse decomposition by Zhou and Tao \cite{zhou_greedy_2013}.
To avoid an accumulation of the error due to the compression of multiple layers the decomposition are not done independently.
This is archived by defining the objective function as the difference between the outputs of the approximation for a collection of approximated input vectors and the corresponding output vectors.

The structure of Hirarchical matrices will be explained in subsection\,\ref{subsec:H-mat}. These matrices have also been used in machine learning. 
Fan et al. used the structure of $\mathcal{H}$-Matrices in neural nets to solve PDEs \cite{fan_multiscale_2019}. There the structure of the neural net makes use of the different scales in the system.
Hierarchical tensor decomposition were used by Wu et al. to represent weight matrices and convolutional kernels \cite{wu_hybrid_2020}.
Ithapu used hierarchical factorization of covariance matrices to explore relationships between classes \cite{ithapu_decoding_2017}.

A different matrix structure explored in neural nets are butterfly matrices.
These are products of sparse matrices that overall have a similar structure to the fast Fourier transform \cite{li_butterfly_2015,parker_random_1995}. Ailon et al. used this matrix structure to represent dense layers \cite{ailon_sparse_2021}.
The butterfly structure was combined with CNNs by Li et al. \cite{li_butterfly-net_2020}. This structure can be used to efficiently represent Fourier kernels.
An extension of butterfly matrices are Kaleidoscope matrices (K-matrices). These were introduced by Dao et al. in \cite{dao_kaleidoscope_2020}.
K-Matrices are product of factors of the shape $B_aB_b^\top$, where $B_a$ and $B_b$ are butterfly matrices. These can be used to represent a wide class of structured matrices.
The patterns in the factors are fixed. Therefore the parameters can be learned using gradient descent.
K-matrices can also be used to represent linear hand crafted structures like reprocessing steps. %example fft 

\todo[inline]{
Properties of dense matrices
Approxiamtions of dense Layer matricies}



\section{Matrix structures}
\todo[inline]{eventually move this section to Background}

\subsection{Matrix structures}
In the following certain matrix structures are presented.
These have in common that the rank of sub-matrices are important.
Here semiseperable, Hierarchical and Sequentially semiseparable matrices are presented.

\subsection{Semiseperable matricies}
Semiseperable matrices are not consistently defined in the literature. 
In this thesis the definitions described by Vandebril \cite{vandebril_bibliography_2005,vandebril_matrix_2007} is used.
An important differentiation are generator representable semiseparable matrices and semiseparable matries.
\paragraph{Generator Representable Semiseparable Matrix}
A matrix $S$ is a generator representable semiseparable matrix if the lower and upper triangular parts of $S$ are taken from rank 1 matrices.
This can be expressed as 
\begin{align}
	\triu(S) &= \triu(pq^\top)\\
	\tril(S) &= \tril(uv^\top)
\end{align}
Where $\triu$ is the upper triangular matrix and $\tril$ is the lower triangular matrix. The vectors $p,q,u$ and $v$ are called the generators.
It is important to note here that the diagonal of $S$ is both included in the lower and upper triangular matrix.

\paragraph{Semiseparable Matrix}
In a semiseperable matrix every subblock selected from the lower triangular part of $S$ have rank 1. The analogous statement has to be fulfilled for the upper triangular part.
This can be formalized as 
\begin{align}
	\rank(S_{i:n,1:i}) &\leq 1 & \forall_i &\in\{i,\dots,n\}\\
	\rank(S_{1:i,i:n}) &\leq 1 & \forall_i &\in\{i,\dots,n\}
\end{align}
A extension of this matrix class are the semiseparable plus diagonal matrices.

\paragraph{Quasiseparable Matrix}
The quasiseparable matrices are similar to the semiseperable matries. In a quasiseparable matrix every subblock selected from the strictly lower strictly upper triangular part of $S$ have rank 1. 
This can be formalized as 
\begin{align}
\rank(S_{i+1:n,1:i}) &\leq 1 & \forall_i &\in\{i,\dots,n\}\\
\rank(S_{1:i,i+1:n}) &\leq 1 & \forall_i &\in\{i,\dots,n\}
\end{align}

A quasiseperable matrix is illustrated in Figure\,\ref{fig:quasiseperable}. All the marked submatrices have the property that their rank is 1.
\begin{figure}
	\centering
	\input{diagrams/semiseperable}
	\caption{Illustration of a quasiseperable matrix}
	\label{fig:quasiseperable}
\end{figure}
As the quasiseperable structure does not impose conditions on the diagonal it is more general as the semiseperable matrices 

There is an relation between invertible semiseperable matrices and invertible tridiagonal matrices.
The inverses of a generator representable semiseparable matrix is a irreducible tridiagonal matrix and vice versa. 
The inverse of a semiseparable matrix is a tridiagonal matrix and vice versa.
If a invertible quasiseparable matrix is inverted, the inverse is again a quasiseparable matrix.

These matrix classes can also be extended for higher ranks.
A matrix $S$ is a generator representable semiseparable matrix of semiseparability rank r if there exist the matrices $R_1$ and $R_2$ with $\rank(R_1)=r$ and $\rank(R_2)=r$ such that
\begin{align}
\triu(S) &= \triu(R_1)\\
\tril(S) &= \tril(R_2)
\end{align}

A similar definition for semiseparable matrices of semiseparability rank $r$ is given in \cite{vandebril_bibliography_2005}.
%For this class of matrices and some slight alterations there are algorithms for the efficient calculation of different operations.

\subsection{Hirarchical Matrices}\label{subsec:H-mat}
The Hirarchical matrices ($\mathcal{H}$-Matrices) are a matrix structure to approximate large matrices. These were mainly introduced by Hackbusch \cite{hackbusch_hierarchische_2009} and Grasedyck \cite{grasedyck_theorie_2001}.
The $\mathcal{H}$-Matrices where developed for the solution of PDEs.
If an PDEs is solved numerically, it has to be discreticed in order to obtain an approximated solution.
As the disscretization already introduces errors, it is advantageous to drop the requirement that the matrix representation is exact, if this results in a reduction of the computational cost.
This can be done by partitioning the matrix in segments. These blocks are represented by low rank matrices. It the rank is far smaller than the size of the matrix this representation is cheaper in terms of storage and in terms of computational cost.
The partitioning is done by hierarchically dividing blocks that cannot be represented using a low rank representation.
To decide if a block has to be divided an admissabillity condition is introduced.
This is a way to predict the representability of a matrix block.
For discretizations of PDEs this admissibility condition is usually based on the geometrical distance.
For other applications different admissibility conditions have to be derived.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\input{diagrams/H-matrix.pgf}
		\caption{Matrix Segmentation}
		\label{fig:strukturh-matrix_a}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
	\centering
	\resizebox{0.7\textwidth}{!}{
	\input{diagrams/H-tree.pgf}
	}
	\caption{Block Bluster Tree}
	\label{fig:strukturh-matrix_b}
	\end{subfigure}
	\caption{Illustration of the structure of a $H$-Matrix}
	\label{fig:strukturh-matrix}
\end{figure}


An $\mathcal{H}$-Matrix is shown in Figure\,\ref{fig:strukturh-matrix}a. 
In this case the matrix is divided in four blocks.
If a block is admissible, it is stored as a low rank representations. These are illustrated as the white rectangles.
If a block is not admissible, the block is divided in smaller subblocks.
If matrices are already small and still not admissible the matrices are stored directly. In Figure\,\ref{fig:strukturh-matrix}a these blocks are colored red.
As the partition is done in a hierarchical fashion the matrices can be represented in a block-tree. 
The bock-tree for the matrix in Figure\,\ref{fig:strukturh-matrix_a} is illustrated in Figure\,\ref{fig:strukturh-matrix_b}.
The Hierarchical matrices make it possible to compute different matrix operations efficiently. \todo[inline]{some detailed statement on complexity}
The need for efficient operations also constraints the possible sizes of the blocks as these should be compatible.

\subsection{Sequentially semiseperable}
\todo[inline]{Quellen, also which notation, note that book is on the transposed...}
Sequentially semiseperable matrices were introduced in \cite{dewilde_time-varying_1998}.
As the Sequentially semiseperable maticies will be used in this thesis a introduction will be given in chapter\,\ref{chap:background}. 
There it will be explained form the standpoint of time varying systems.
Here the commonalities and differences to the semiseperable and hirarchical matrices are explored.

The sequentially semiseperable matrices have the properties that submatrices have a low rank.
This is similar to semiseperable matrices. 
Unlike the semiseperable matrices this is not true for all matrices taken form the lower and upper triangular part.
The sequentially semiseperable matrices are divided into blocks.
Matrices taken form the strict lower and strict upper triangular blockmatrix have the condition that their rank is low.
This is illustrated in Figure\,\ref{fig:sequentiallysep}.

\begin{figure}[htb]
	\centering
	\input{diagrams/sequentially_sep}
	\caption{Illustration of a sequentially semiseperable matrix. The thick dotted lines mark the segemntation of the matrix. The thin lines represent submatrices with low rank.}
	\label{fig:sequentiallysep}
\end{figure}

This segmentation in blocks makes similar to the hirarchical Matrices that also have a segmentation.
Unlike semiseperable matrices the sequentially semiseperable matrices do not have to be quadratic. 

\chapter{Background}\label{chap:background}


Sequentially semiseperable Matrices can be considered as descriptions of time varying systems.
\footnote{
The naming and structure of the matrices is not consistent in the literature.
Here we will use the notation used in \cite{tong_blind_2003}. 
In \cite{dewilde_time-varying_1998} a similar notation is used, but the transfer operator is transposed.
In other works like \cite{rice_efficient_2010,chandrasekaran_fast_2002} the causal and anticausal parts are considered jointly
}
These systems can be described using the formulas
\begin{subequations}
	\begin{align}
	x_{k+1} &= A_k x_k + B_k u_k \\
	y_k &= C_k x_k + D_k u_k 
	\end{align}.
	\label{eq:def_causal}
\end{subequations}

The $u_k$ are the inputs, the $y_k$ are the outputs and the $x_k$ are the states.
The matrices $A_k$, $B_k$, $C_k$ and $D_k$ can vary with the time index $k$.
This is different from time-invariant systems, where the matrices $A$, $B$, $C$ and $D$ are constant.
This also means that the dimensions of the input, the output and the states can vary with time.
The structure of a system is represented in Figure\ref{fig:struktur-system_a}.
\begin{figure}[htb]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		%\resizebox{0.7\textwidth}{!}{
		\input{diagrams/Example_system.pgf}
		%}
		\caption{Causal system}
		\label{fig:struktur-system_a}
	\end{subfigure}
	\hspace{0.8cm}
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		%\resizebox{0.7\textwidth}{!}{
		\input{diagrams/Example_system_anti.pgf}
		%}
		\caption{anticausal system}
		\label{fig:struktur-system_b}
	\end{subfigure}
	\caption{Illustration of the structure of time varying systems}
	\label{fig:struktur-system}
\end{figure}
Here we can see that the system can be structured in stages.
The stage $k$ has the matrices $A_k$, $B_k$, $C_k$ and $D_k$.
The matrix $A_k$ maps the old state $x_k$ to the next state $x_{k+1}$, the matrix $B_k$ maps the current input $u_k$ to the next state $x_{k+1}$.
The matrix $C_k$ maps the old state $x_k$ to the output $y_k$.
Finally the matrix $D_k$ directly maps the input $u_k$ to the output $y_k$.
The system defined in Equation\,\ref{eq:def_causal} is a causal system.
This means that the output $y_k$ only depends on the current input $u_k$ and the previous inputs $u_l$ with $l < k$.
It is also possible to define an anticausal system where the output only depends on the current and the later inputs.
An anticausal system is described using the formula
\begin{subequations}
	\begin{align}
	x_{k-1} &= A_k x_k + B_k u_k \\
	y_k &= C_k x_k + D_k u_k 
	\end{align}.
	\label{eq:def_anticausal}
\end{subequations}

In Figure\ref{fig:struktur-system_b} an anticausal system is illustrated.
We can see that the structure is analogous except for the direction of the arrows for the states $x_k$. \todo{rephrase}

When we stack the input vectors to a one vector $u = [u_1^\top, \dots ,u_k^\top]^\top$ and stack the outputs vectors to a one output vector $y = [y_1^\top, \dots ,y_k^\top]^\top$ we can describe the input-output behaviour of the system using a single matrix vector product 
\begin{equation}\label{eq:System_mat_vec}
	y = Tu,
\end{equation}
where the matrix $T$ is called the transfer operator.

The matrix $T$ for a causal system with four stages is
\begin{equation}\label{eq:T_causal}
T_\text{causal}=
\begin{bmatrix}D_{1} & 0 & 0 & 0\\C_{2} B_{1} & D_{2} & 0 & 0\\
C_{3} A_{2} B_{1} & C_{3} B_{2} & D_{3} & 0\\
C_{4} A_{3} A_{2} B_{1} & C_{4} A_{3} B_{2} & C_{4} B_{3} & D_{4}\end{bmatrix}
.
\end{equation}
For an anticausal system the matrix description is 
\begin{equation}
T_{\text{anticausal}}=
\begin{bmatrix}D_{1} & C_{1} B_{2} & C_{1} A_{2} B_{3} & C_{1} A_{2} A_{3} B_{4}\\
0 & D_{2} & C_{2} B_{3} & C_{2} A_{3} B_{4}\\
0 & 0 & D_{3} & C_{3} B_{4}\\
0 & 0 & 0 & D_{4}\end{bmatrix}.
\end{equation}
The matrices for a higher number of stages are analogous.
One can see that the causal system results in a lower triangular blockmatrix whereas the anticausal system results in a upper triangular blockmatrix.
If one wants to represent the whole Matrix one can use mixed systems.
These can be defined as the sum of a causal and an anticausal system.
These usually have the same input and output dimensions, while the state dimensions can de different for the causal and anticausal part.
When adding the systems,  the $D$ matrices have to be added.
As the use of two seperate $D$ does usually not have any benefits, the $D$ matrices for the anticausal system are usually set to 0 and therefore can be ignored.
When representing matrices the first state $x_1$ and the last stage $x_{n+1}$ are usually set to zero dimensions.

\paragraph{Hankel Operator and Minimality}
When we are interested in properties of the system, we can study the relation between the inputs, the states and the outputs. 
The reachability matrix $\R$ is the mapping from the inputs to the state. For a causal system the reachability matrix for state $x_k$ can be written as
\begin{equation}
	\R_k = \begin{bmatrix}
	 \dots & A_{k-1}A_{k-2}B_{k-1} &A_{k-1}B_{k-2} &B_{k-1}
	\end{bmatrix}
\end{equation}
The mapping form the state to the output is called the observability operator $\Ob$.
For a causal system the observability matrix for state $x_k$ can be written as
\begin{equation}
	\Ob_k = 
	\begin{bmatrix}
		C_k\\
		C_{k+1}A_k\\
		C_{k+2}A_{k+1}A_k\\
		\vdots
	\end{bmatrix}.
\end{equation}
When we multiply these we obtain the mapping from the inputs to the outputs. This is called the Hankel operator $H_k$.
For a causal system this can be written as
\begin{equation}
	H_k = \Ob_k \R_k = 
		\begin{bmatrix}
\dots   & C_{k}A_{k-1}B_{k-2} & C_{k}B_{k-1} \\
\dots   & C_{k+1}A_{k}A_{k-1}B_{k-2} & C_{k+1}A_{k}B_{k-1} \\
\iddots &\vdots &\vdots
\end{bmatrix}
%%3x3 Version
%		\begin{bmatrix}
%		\dots & C_{k}A_{k-1}A_{k-2}B_{k-3}  & C_{k}A_{k-1}B_{k-2} & C_{k}B_{k-1} \\
%		\dots & C_{k+1}A_{k}A_{k-1}A_{k-2}B_{k-3}  & C_{k+1}A_{k}A_{k-1}B_{k-2} & C_{k+1}A_{k}B_{k-1} \\
%		\dots & C_{k+2}A_{k+1}A_{k}A_{k-1}A_{k-2}B_{k-3}  & C_{k+2}A_{k+1}A_{k}A_{k-1}B_{k-2} & C_{k+2}A_{k+1}A_{k}B_{k-1} \\
%		\iddots & \vdots &\vdots &\vdots
%		\end{bmatrix}
\end{equation}
This is the same matrix strucure as we have seen in Equation\,\ref{eq:T_causal}.
\todo{reword}

If the rows  of rows of $\R_k$ are linearly independent, the range of $\R_k$ is $\mathbb{R}^n$, where $n$ is the number of dimensions of $x_k$.
This means that we can reach every state with some input.
Therefore we call a system reachable if every $R_k$ has a full row rank.
If the columns of $O_k$ are linearly independent, we can reconstruct the state $x_k$ from the output.  
Therefore we call a system observable if every $R_k$ has a full column rank.

If a system is both observable and reachable, it is called minimal.
In a minimal system the state dimension is cannot be further reduced.
\todo{reword}
The rank of the Hankel operator is called the Hankelrank. 
If the system is minimal we have 
\begin{equation}
	\rank{H_k} = rank{\Ob_k} = \rank{R_k} = n_k
\end{equation}
for every $k$.
Usually the state dims are far smaller than the input and output dims.
This allows us to factor the $H_k$ in a tall and a flat matrix. 
This results in the factorization $H_k = \Ob_k\R_k$

\paragraph{State Transforms}
As this factorization is not unique, the state in also not uniquely defined. 
The state can be transformed with an non-singular state transform matrix $S$ according to $\tilde{x} =Sx$.
This gives a transformed 
reachabilty matrix $\tilde{\R_k}=S \R_k$ and the transformed
observability matrix $\tilde{\Ob_k}= \Ob_k S^{-1}$. 
The Hankel matrix stays the same as $\tilde{H_k} = \Ob_k S^{-1} S \R_k= \Ob_k \R_k = H_k$
There are are three particular types of factorization that are called canonical forms.
An input-normal realization has the property that the columns of each $\Ob_k$ are orthormormal. Whereas in an output-normal realization, all rows of each $\R_k$ are orthonormal.
The balanced realization results form the reduced Singular value decomposition $H_k = U_k \Sigma_k V_k^\top$. 
In a balanced realization $\Ob_k = U_k \Sigma^{1/2}$ and $\R_k = \Sigma^{1/2} V_k^\top$.
These canonical form usually also require that the system is minimal. In this thesis the minimality is usually omitted as the algorithms described also work on non-minimal systems and minimality is hard to define numerically for systems where the $\sigma$s decay slowly. \todo[]{finite equivalent of compact operators, search for word} If minimality is required, it will be explicitly stated.

\paragraph{Matrix Factorization}
The sequentially semisperabel matrices can also be considered as a matrix factorization.
For a causal system,the matrix $T$ can be expressed as a product $T = T_n T_{n-1} \dots T_2 T_1$
Where every $T_k$ represents a stage. The matrices $T_k$ is constructed according to 

\begin{equation}
\newcommand{\sddots}{\hspace{-6pt}\ddots}
\newcommand{\n}{\hspace{-7pt}}
	T_k =
	\begin{bmatrix}
	\begin{array}{c|cccccccc}
	A_k&    &   & & B_k & & &\\
	\hline
	   &\eye&   &&     & & &\\[-8pt]
	   & &\sddots&&    & & &\\[-6pt]
	   & & &\n\eye&      & & &\\
	C_k& & &    & D_k  & & &\\
	   & & &    &      &\eye& &\\[-8pt]
	   & & &    &      & &\sddots&\\[-6pt]
	   & & &    &      & & &\n\eye 
	\end{array}
	\end{bmatrix} 
\end{equation}

For a system with 3 stages
\begin{equation*}
	T=
	\left[\begin{matrix}A_{3} & 0 & 0 & B_{3}\\0 & 1 & 0 & 0\\0 & 0 & 1 & 0\\C_{3} & 0 & 0 & D_{3}\end{matrix}\right]
\dots
%	\left[\begin{matrix}A_{2} & 0 & B_{2} & 0\\0 & 1 & 0 & 0\\C_{2} & 0 & D_{2} & 0\\0 & 0 & 0 & 1\end{matrix}\right]
	\left[\begin{matrix}A_{1} & B_{1} & 0 & 0\\C_{1} & D_{1} & 0 & 0\\0 & 0 & 1 & 0\\0 & 0 & 0 & 1\end{matrix}\right]
	=
	\left[\begin{matrix}A_{3} A_{2} A_{1} & A_{3} A_{2} B_{1} & A_{3} B_{2} & B_{3}\\C_{1} & D_{1} & 0 & 0\\C_{2} A_{1} & C_{2} B_{1} & D_{2} & 0\\C_{3} A_{2} A_{1} & C_{3} A_{2} B_{1} & C_{3} B_{2} & D_{3}\end{matrix}\right]
\end{equation*}

As $A_1$ has zero columns and $A_3$ has zero rows, the first block-column and the top block-row disappear and we get the familiars structure form Equation\,\ref{eq:T_causal}.

An analogous factorization is possible for anticausal systems.
Here the ordering is reversed an we have $T = T_1 T_2 \dots T_{n-1} T_{n}$

We can also represent mixed systems as factorization, if we use insert the factorizations for the causal part $T_{\text{causal}}$ and the anticausal parts $T_{\text{anticausal}}$ into
\begin{equation}
	T = 
	\begin{bmatrix}
	\eye &
	\eye
	\end{bmatrix}
	\begin{bmatrix}
	T_{\text{causal}}&\\
	&T_{\text{anticausal}}
	\end{bmatrix}	
	\begin{bmatrix}
	\eye\\
	\eye
	\end{bmatrix}
\end{equation}
Sequentially semiseperable matrices can be added, multiplied and inverted using algorithm that work on the ABCD description.
It is also possible to calculate QR factorization \cite{chandrasekaran_fast_2002,tong_blind_2003} or URV factorization \cite{chandrasekaran_fast_2005}. 
%Note on URV more papers in Dewilde,Diepold and van der Veen p315 
There are also a couple of other algorithms that can be used for different purposes like sign iterations \cite{rice_efficient_2010}.

\paragraph{Approxiamtions}
It is also possible, to reduce the number of states by approximation a system $T$ with a system $\tilde{T}$.
If the system $T$ is minimal, this is not trivial, as the a reduction of the number of states also result in a reduction of the Hankelrank.
On approach is the Hankel-norm approximation.
The Hankel Norm is defined as
\begin{equation}
	\|T\|_H = \sup_{i}\|H_i\|.
\end{equation}
This norm is the supremeum over the spectral norm (the matrix 2-norm) of each individual Hankel matrix.
In \cite{dewilde_time-varying_1998} an algorithm is given for the hankel rank approxiamtion that computes a $\tilde{T}$.
The approximated system satisfies the condition
\begin{equation}
	\| \Gamma^{-1}(T-\tilde{T})\|_H \leq 1.
\end{equation}
Where $\Gamma$ is a diagonal and hermition operator. 
If we set $\Gamma = \eye\gamma$ we obtain the simplified condition
\begin{equation}
	\|T-\tilde{T}\|_H \leq \gamma.
\end{equation}

A second approach is balanced truncation is described in . 
Here the state dimensions $x_k$ are reduced to a new dimension $\tilde{n}_k$.
The idea is to remove stated that result in a small output $y$ or need a relatively big input $u$.
We can calculate the norm of the output for a state $x$ using
\begin{equation}
	\|y\|_2^2 = u^\top u = x^\top \Ob^\top \Ob x
	.
\end{equation}
The matrix $\Ob^\top \Ob$ is called the Observability gramian.
analogously we can calculate the norm of the input required ti reach a certain state using
\begin{equation}
	\|u\|_2^2 = x^\top (\R\R^\top)^{-1} x
	.
\end{equation}
Where the Reachability gramian $\R\R^\top$ is invertible if the system is reachable.
The involved states can be changed using state transforms.
The idea is now to obtain a state transform, for which both problems are equivalent. 
This is the case for a balanced realization
\begin{equation}
	\Ob^\top \Ob = \Sigma^{1/2}U^\top U\Sigma^{1/2} = \Sigma = \Sigma^{1/2} V^\top V \Sigma^{1/2} = \R \R^\top 
\end{equation}
This results in a basis for the state with 
\begin{equation}
	\|y\|_2^2\big|_{x = e_l} = \|\Ob x\|_2^2\big|_{x = e_l} = \sigma_l
\end{equation}
and 
\begin{equation}
\|u\|_2^2\big|_{x = e_l} = \sigma_l^{-1}
\end{equation}
Where $e_l$ is the $l$-th standard basis vector.
This allows us to only keep the states with $\|y\| > \epsilon^{1/2} \|x\|$ and $\|x\| > \epsilon^{1/2} \|u\| $ for some $\epsilon>0$ by cutting all dimensions $l$ for all $l$ with $\sigma_l < \epsilon$.
\todo{reword}
%that are
% result in a large output and only need a small input to be reached. 

To obtain the reduced system $\tilde{T}$ the matrices of a balanced realization for every state are partitioned according to
\begin{align}
	A_k &=\begin{bmatrix}
	A_{k[11]} & A_{k[12]} \\
	A_{k[21]} & A_{k[22]} \\
	\end{bmatrix}
	&
	B_k &= \begin{bmatrix}
	B_{k[1]} \\ B_{k[2]}
	\end{bmatrix} 
	\\
	C &= \begin{bmatrix}
	C_{k[1]} & C_{k[2]}
	\end{bmatrix}& 
	D_k&=D_k.
\end{align}
The dimensions are determined by the new state dimensions.
The approximated system $\tilde{T}$ is then set to
\begin{align}
	\tilde{A}_k &= A_{k[11]}  & \tilde{B}_k &= B_{k[1]}\\
	\tilde{C}_k &=C_{k[1]}      & \tilde{D}_k &= D_k.
\end{align}
%This is equivalent to removing the tailing states.
%Because the realization is balanced every state in the original system corresponds to a $\sigma$ of the according Hankel matrix.
This approach is equivalent to removing all $\sigma<\epsilon$s of the SVD of the Hankel operator.
Therefore we can also construct an approximation when identifying a system using the SVD by truncating the SVD as described in \cite{shokoohi_identification_1987}.
\todo[inline]{Check proof in paper}
In an error bound given in \cite{sandberg_balanced_2004}.
\todo[inline]{find an easily understandable derivation or similar}

More details on general approximation.
\cite{antoulas_approximation_2005} detailed discussion of approxiamtions.

\chapter{Methods} 15p

\section{Matrix Approximations}
\subsection{Algorithm}
\begin{itemize}


\item Discuss ordering of the sigmas, and cutting states if the sigmas are ordered.
\item Define ordered realization as extention of the ballanced realization

\item Describe how to get the sigmas for a system by transforming it.General idea: We do not have to compute the SVD of the total Hankel matrix, if we take a input-nomal system and convert it step by step to a output-normal.

\item Describe the overall approximation algorithm.

\paragraph{Extention balanced truncation to nonbalanced systems/Define ordered system}
In Chapter\,\ref{chap:background} the balanced truncation is defined for balanced systems.
These have the property that $\Ob_k = U_k \Sigma_\Ob$ and $\R_k = \Sigma_\R V_k^\top$ where $\Sigma_\Ob = \Sigma_\R=\Sigma^{1/2}$.
The balanced truncation is also possible for a wider class of realizations.

In this thesis these Realizations will be called ordered.
\begin{definition}[Ordered Realization]
	A system is ordered if 
	\begin{subequations}
	\begin{align}
		\Ob_k^\top \Ob_k &= \Sigma_ {\Ob k}^2 \label{eq:orderd_obs}\\ %\diag(\sigma_{\Ob k}^2)\\
		\R_k \R_k^\top &= \Sigma_ {\R k}^2 \label{eq:orderd_reach}%\diag(\sigma_{\R k}^2)
	\end{align}
	\end{subequations}
	where $\Sigma_{\Ob k}$ and $\Sigma_{\R k}$ are diagonal matrices
	and the diagonal entries of $\Sigma_k =\Sigma_{\Ob k}\Sigma_{\R k}$ are non increasing (i.e. $\sigma_1 \leq \sigma_2 \leq \dots \leq \sigma_n$).
	%Positiov sowieso, weil gram matrix
	are fulfilled for every $k$.
\end{definition}

%As the SVD of the Hankel matrix does not change he have $\Sigma_\Ob\Sigma_\R=\Sigma$ as $H = \Ob\R = U_k \Sigma_\Ob \Sigma_\R V_k^\top = U_k \Sigma V_k^\top$.

An ordered system can be transformed into a balanced system using the diagonal state transformations $S_k = \Sigma_k^{1/2}\Sigma_{\R k}^{-1}$ and the inverse $S_k^{-1} = \Sigma_{\Ob k}^{-1}\Sigma_k^{1/2}$.
This gives the balanced system $\tilde{T}$
\begin{align}
	\tilde{A}_k &= S_{k+1} A_k S_k^{-1}   & \tilde{B} =& S_{k+1} B \\
	\tilde{C}_k &= C_k S_k^{-1}   & \tilde{D} =& D 
\end{align}
\begin{proof}
	By applying the state transform we can see that we get a balanced system
	$\Ob_k S_k^{-1} = U_k \Sigma_\Ob \Sigma_{\Ob k}^{-1}\Sigma_k^{1/2} = U_k \Sigma_k^{1/2}$\\
	$S_k \R_k = \Sigma_k^{1/2}\Sigma_{\R k}^{-1} \Sigma_{\R k} V_k^\top = \Sigma_k^{1/2} V_k^\top$
\end{proof}

A ordered system can be reduced using the same segmentation as used for the balanced truncation.
This allows us to use algorithms that require input- or output normal forms, as systems can be both balanced and input normal if $\Sigma_{\R k}=\eye$ or balanced and output normal if $\Sigma_{\Ob k}=\eye$.

\paragraph{Recusrsve factorization of $\Ob_k$ and  $\R_k$}
The goal of this algorithm is to calculate the singular values of the Hankel matrices without applying the SVD on $H$ or calculating $H$, $\Ob_k$ or $\R_k$.
The idea is that we can recursively factor the observability matrix according to
\begin{equation}\label{eq:decomp_O}
	\Ob_k = 
	\begin{bmatrix}
	C_k\\
	\Ob_{k+1}A_k
	\end{bmatrix}
	=
	\begin{bmatrix}
	\eye& 0 \\
	0& \Ob_{k+1}
	\end{bmatrix}
	\begin{bmatrix}
	C_k\\
	A_k
	\end{bmatrix}
\end{equation}
And analogously the reachability matrix
\begin{equation}\label{eq:decomp_R}
	\R_k = 
	\begin{bmatrix}
	A_{k-1} \R_{k-1} & B_{k-1}
	\end{bmatrix}
	=
	\begin{bmatrix}
	A_{k-1} & B_{k-1}
	\end{bmatrix}
	\begin{bmatrix}
	\R_{k-1} &0\\
	0& \eye
	\end{bmatrix}
\end{equation}
This allows us to create algorithms, that can convert realizations into input normal and output normal form.
With an combination of both it is also possible to obtain the singular values.

\paragraph{Make input normal and reachable}
First we give an algorithm to transform the system into an reachable and input normal system.
This means that we want to make all $R_k$ subunitary.
As we do not want to calculate the reachability matrices directly we will employ the decomposition given in Equation\,\ref{eq:decomp_R}.
If $\R_{k-1}^\top \R_{k-1}=\eye$ we can make $\R_{k}$ sub orthogonal by calculating the SVD of $[A_{k-1} B_{k-1}]$
\begin{equation}
\R_k = 
\begin{bmatrix}
A_{k-1} \R_{k-1} & B_{k-1}
\end{bmatrix}
=
\begin{bmatrix}
A_{k-1} & B_{k-1}
\end{bmatrix}
\begin{bmatrix}
\R_{k-1} &0\\
0& \eye
\end{bmatrix}
=U_k\Sigma _k
\underbrace{V_k
\begin{bmatrix}
\R_{k-1} &0\\
0& \eye
\end{bmatrix}}_{\tilde{\R}_k}
\end{equation}
This gives the state transform $S_k = U_k\Sigma_k$. 
The usage of the SVD also allows us to remove states that are not reachable by counting all nonzero elements in $\Sigma$ and reducing the SVD accordingly.
The product with the inverse $S_k^{-1}$ is implicitly calculated by computing the SVD.
Therefore the matrices $\tilde{A}_k$ and $\tilde{B}_k$ are set to $[\tilde{A}_k,\tilde{B}_k]=V_k^\top$.
The matrices $A_{k}$ and $B_{k}$ have to be updated with the state transform $S_k$.

The algorithm is formalized in Algorithm\,\ref{alg:inp_normal}

\begin{algorithm}
	\begin{algorithmic}
	\For{$k\gets 1$ \textbf{to} $K-1$}
		\State $U,\sigma,V^\top \gets$ SVD$([A_k,B_k])$
		\State $r \gets $ count$(\sigma>\text{tol}_i)$
		\State $U \gets U[:,:r]$
		\State $\sigma \gets \sigma[:r]$
		\State $V^\top \gets V^\top[:r,:]$
		\State $[A_k,B_k] \gets V^\top$
		\State $A_{k+1}=A_{k+1}U\diag(\sigma)$
		\State $C_{k+1}=C_{k+1}U\diag(\sigma)$
	\EndFor
	\end{algorithmic}
\caption{Algorithm to convert to input normal system}\label{alg:inp_normal}
\end{algorithm}

This results in a input normal and reachable system.
\begin{proof}
	For input normality $\tilde{R}_k\tilde{R}_k^\top = \eye$ has to be true for all $k \in [1,\dots,K-1]$
	From $\tilde{\R}_{k-1}\tilde{\R}_{k-1}^\top = \eye$ follows that $\tilde{\R}_k\tilde{\R}_k^\top = \eye$ as
	\begin{equation}\label{eq:induction_reach}
	\tilde{R}_k\tilde{\R}_k^\top
	=
	V_k^\top\begin{bmatrix}\tilde{\R}_{k-1} &0\\
		0& \eye \end{bmatrix}
	\begin{bmatrix}\tilde{\R}_{k-1}^\top &0\\
		0& \eye \end{bmatrix} V_k
	= 
	V_k^\top\begin{bmatrix}\eye &0\\
	0& \eye \end{bmatrix} V_k
	=
	\eye
	\end{equation}
	As $x_1$ zero dimensional $R_1$ vanishes and Equation\,\ref{eq:induction_reach} is also true for $k=1$.
	Reachability directly follows form $\R_k\R_k^\top = \eye$.
\end{proof}




\paragraph{Make output normal}
The algorithm to transform the system into an observable and output normal system is analogous.
Here all $\Ob_k$ have to be subunitary.
Here the decomposition given in Equation\,\ref{eq:decomp_O} is used to avoid a computation of $\Ob_k$.
If $\Ob_{k+1} \Ob_{k+1}^\top=\eye$ we can make $\Ob_{k}$ sub orthogonal by calculating the SVD of $[C_k^\top A_k^\top]^\top$.

\begin{equation}
	\Ob_{k}
	=
	\begin{bmatrix}
	\eye& 0 \\
	0& \Ob_{k+1}
	\end{bmatrix}
	\begin{bmatrix}
	C_k\\
	A_k
	\end{bmatrix}
	=
	\underbrace{
	\begin{bmatrix}
	\eye& 0 \\
	0& \Ob_{k+1}
	\end{bmatrix}
	U_k}_{\tilde{\Ob}_k}
	\Sigma_k V_k^\top
\end{equation}
This results in the state transform $S_k = \Sigma_k V_k^\top$.
Here the SVD is also in a reduced form, that results in a removal of all non observable states.
The state transform with $S_k^{-1}$ is done implicitly by the SVD.
The matrices $A_{k-1}$ and $C_{k-1}$ are updated with the state transform $S_k$

This gives the Algorithm\,\ref{alg:out_normal}
\begin{algorithm}
	\begin{algorithmic}
	\For{$k\gets K$ \textbf{downto} $2$}
		\State $U,\sigma,V^\top \gets$ SVD$\left(\begin{bmatrix} C_k\\A_k \end{bmatrix}\right)$
		\State $r \gets $ count$(\sigma>\text{tol}_o)$
		\State $U \gets U[:,:r]$
		\State $\sigma \gets \sigma[:r]$
		\State $V^\top \gets V^\top[:r,:]$
		\State $\begin{bmatrix} C_k\\A_k \end{bmatrix} \gets U$
		\State $A_{k-1}=\diag(\sigma)V^\top A_{k+1}$
		\State $B_{k-1}=\diag(\sigma)V^\top C_{k+1}$
	\EndFor
	\end{algorithmic}
	\caption{Algorithm to convert to output normal system}\label{alg:out_normal}
\end{algorithm}
This results in a output normal and observable system.
\begin{proof}
	For output normality $\tilde{\Ob}_k^\top\tilde{\Ob}_k = \eye$ has to be true for all $k \in [1,\dots,K-1]$
	From $\tilde{\Ob}_{k+1}^\top\tilde{\Ob}_{k+1} = \eye$ follows that $\tilde{\Ob}_k^\top\tilde{\Ob}_k = \eye$ as
	\begin{equation}\label{eq:induction_obs}
		\tilde{\Ob}_k^\top\tilde{\Ob}_k
		=
		U_k^\top
		\begin{bmatrix} \eye& 0 \\
			0& \Ob_{k+1}^\top \end{bmatrix}
		\begin{bmatrix} \eye& 0 \\
		0& \Ob_{k+1} \end{bmatrix}
		U_k
		=
		U_k^\top
		\begin{bmatrix} \eye& 0 \\
		0& \eye \end{bmatrix}
		U_k
		=
		\eye
	\end{equation}
	As $x_{K+1}$ zero dimensional $\Ob_{k+1}$ vanishes and Equation\,\ref{eq:induction_obs} is also true for $K$.
	Reachability directly follows form $\Ob_k\Ob_k^\top = \eye$.
\end{proof}


\paragraph{Obtain sigmas}
By converting an input normal system to an output normal system it is possible to calculate the singlaur values of $H$
This uses the factorization and the recursion relation in Equation\,\ref{eq:decomp_O}
to get
 \begin{equation}
 H = \Ob_k \R_k = 
 \begin{bmatrix}
 \eye& 0 \\
 0& \Ob_{k+1}
 \end{bmatrix}
 \begin{bmatrix}
 C_k\\
 A_k
 \end{bmatrix}
 \R_k
 \end{equation}.
As the algorithm started with an input normal system $\R_k\R_k^\top = \eye$ is still fulfilled.
And due to the fact the algorithm has transformed the states $x_i$ for $i>k$ to output normal form $\Ob_{k+1}^\top \Ob_{k+1} = \eye$ is already fulfilled.
When the SVD is applied to the matrix $[C_k^\top A_k^\top]^\top$ this results in an SVD decomposition of $H$
 \begin{equation}
 H = 
 \underbrace{
 	\begin{bmatrix}
 	\eye& 0 \\
 	0& \Ob_{k+1}
 	\end{bmatrix}
 	U_k}_{U_k} 
 \Sigma_k 
 \underbrace{\vphantom{\begin{matrix} 0 \\0\end{matrix}}
 	V_k^\top
 	\R_k}_{\breve{V}_k^\top}
 =\breve{U}_k\Sigma_k \breve{V}_k^\top
 \end{equation}

 \begin{proof}
 	To prove that $H = \breve{U}_k\Sigma_k\breve{V}^\top$ is a singluar value decomposition we have to prove that $\breve{U}_k^\top \breve{U}_k= \eye$ and $\breve{V}^\top \breve{V} = \eye$.
 	If $\Ob_{k+1}$, $R_k$, $U_k$ and $V$ are orthogonal the proof directly follows. For the sub unitary case we have 
 	$
 	\breve{V}_k^\top \breve{V}_k
 	=
 	V_k^\top
 	\R_k
 	\R_k^\top
 	V_k
 	=
 	V_k^\top
 	\eye
 	V_k
 	=
 	\eye
 	$
 	and
 	$
 	U_k^\top U_k
 	=
 	\eye 
 	$
 	has been proven in Equation\,\ref{eq:induction_obs}
 \end{proof}

The resulting system is ordered.
\begin{proof}
	The condition in Equation\,\ref{eq:orderd_obs} directly follows form the output normality $\tilde{\Ob}_k^\top \tilde{\Ob}_k = \eye$.
	The condition in Equation\,\ref{eq:orderd_reach} follows form the input normality of the intermediate system by
	\begin{equation}
		\tilde{\R}_k \tilde{\R}_k^\top 
		=
		\Sigma_k V_k \R_k \R_k^\top V_k^\top \Sigma_k 
		= \Sigma_k^2
	\end{equation}
	As $\Sigma_k$ is result form an SVD, the singular values are in decreasing order. 
\end{proof}
Therefore the system can be approximated using balanced truncation.

This algorithm is also a fast way to reduce the system to a minimal system.
The tolerance for the first transformation$\text{tol}_i$ is usually set to a value close to the machine precision to avoid errors due to the removed states.
The tolerance for the second transformation $\text{tol}_o$ is then set to the wanted $\epsilon$.
 
It is also possible to first convert the system to an output normal system and calculate the SVD when converting it to an input normal system.
The proof for this and the algorithms for the anticausal case are omitted for brevity.

 
 
 
\todo[inline]{is it clear if the ordering makes a difference, slo difference between immediately cutting or cutting later?}

\end{itemize}
\subsection{Cost of computation}
\begin{itemize}
\item Short section on the computational cost and number of parameters.

\item Define it with and without additions.

\item Might also go in the Background, but I think it would be a bit to much and I did not find a detailed discussion in the literature. 
\end{itemize}
\subsection{Number of stages}
\begin{itemize}
\item Describe continuous surrogate problem for the cost depending on the number of parameters. 
\item Discuss educated guesses for the number of stages.
\end{itemize}

\section{Segmentation Adaptation}
\begin{itemize}
\item Describe algorithm of moving boundaries.
\item First for causal system, then for mixed system.

\item Describe splitting and combining states.

\item Pseudocode for the causal systems

\item Move all the proves of minimality to the appendix for brevity.
\item Describe calculation of $\sigma$s
\end{itemize}
\section{Permutations}
if time allows add some notes on permutations here



\chapter{Experiments}



\section{Matrix Approximation}
\begin{itemize}
\item Test different approaches to represent matrices.
\item Have a look at the representation error in different norms for different algorithms
\end{itemize}
\section{Weight Matrix Approximation}
%\section{Setups}
eventually split this up in subsections

Setup: 
\begin{itemize}
	\item Some general notes on the tests.
	\item How define loss, images to features etc. 
	\item How measure accuracy
\end{itemize}

Tests:
\begin{itemize}
\item Test the different approaces in neural nets.
\end{itemize}
\chapter{Discussion}
4-5 4pages

\chapter{Conclusion}
1-2 pages



\the\textwidth



% Puts out the list of references
\printbibliography{}

\end{document}
