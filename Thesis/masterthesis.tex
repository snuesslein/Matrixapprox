% Possible types of documents/theses
%   doctype=bachelorsthesis
%   doctype=mastersthesis
%   doctype=idp
%   doctype=phdthesis
%   doctype=studienarbeit
%   doctype=diplomarbeit
%
% Document language
%   without 'lang' attribute: English
%   lang=ngerman:             German (new orthography)
%
% Binding correction
%   BCOR=<Längenangabe>
%   Additional margin, which is invisible due to binding the book
%   The usual binding by the Fachschaft has a thickness of 1,5 cm 
%
% biblatex (citations)
%   This requires 'biber' to be used instead of 'bibtex', please
%   adapt your editor's settings accordingly!
\documentclass[doctype=mastersthesis,BCOR=15mm,biblatex]{ldvbook}%lang=ngerman

% !TeX spellcheck = en_US

\usepackage{pgfplots}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{matrix}
\usetikzlibrary{ decorations.markings}
\usetikzlibrary {decorations.shapes}

\input{diagrams/systems.tex}

\usepackage[boxed]{algorithm}
\usepackage{algpseudocode}

\usepackage{todonotes}
\usepackage{subcaption}
\usepackage{mathdots}

% Look for citation sources in the database "diplomarbeit.bib"
\addbibresource{thesis.bib}


%operator declarations
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\triu}{triu}
\DeclareMathOperator{\tril}{tril}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\range}{range}



%define symbols
\newcommand{\R}{\mathcal{R}} %Reachabilityy matrix
\newcommand{\Ob}{\mathcal{O}} %Observability operator 
\newcommand{\eye}{I} %identity matrix
\newcommand{\bigO}{O}


%some shortcuts for things that might change
\newcommand{\m}{\triangledown} %indexing for moved inputs/outouta
\newcommand{\da}{d^*} %state dims for anticausal state


\begin{document}

% Bibliographic information about the thesis, please change accordingly!
\title{Combining Matrix Representations for Structured Approximation of Neural Network Weight Matrices}
\author{Stephan Nüßlein}
\license{CC-BY}
\supervisor{Matthias Kissel}


\maketitle[frontcover=Design1]


\chapter*{Abstract}

\begin{itemize}
	\item neural nets
	\item reduce cost of neural nets using structured matrix
	\item how to use Sequentially semisperable matrices in this context
	\item choose number of stages and tuning segmentation
	\item some sentence on results
\end{itemize}


\tableofcontents

60-80 pages total

% Please compile this example document including the bibliography
% database. Check the resulting document and the references for 
% correct appearance (especially the German Umlaute).
% Thus you ensure that LaTeX is detecting the character encoding
% correctly and your build chain is working.
% If it does not, please tell your supervisor.



\todo[inline]{Make th usage of the state transforms consistent}


\chapter{Introduction} 3p
Motivation Why

What others are dooing

What has been done here

increasing cost

Order of matrix vector product, makes larger systems expensive

\todo{why ML interesting}

As the matrices for fully connected layers get larger the evaluation of the neural nets get computationally more expensive. This is often problematic as the computational resources are a limiting factor, especially on embedded or mobile systems.


The linear layers in neural nets can be represented with matrices.
Usually full matrices are used.
When evaluation the neural nets one has to compute the matrix vector product.
For a full unstructured matrix this is of order $O(n^2)$. 
Whereas the cost for calculating the nonlinear part is usually linear in the number of neurons the cost for the calculation of the liner layers is quadratic in the number of neurons.
This means that a larger number of neurons the cost of the linear layer inccears 
\todo{reword}


There are also matrix structures where the cost of the matrix vector product is of lower order.
These have been developed to help solving PDEs like $\mathcal{H}$-Matrices, arise in different circumstances like semiseperabel matrices, %Vander Bib

or are description of time varying systems like sequentially semiseperable matrices.
All of them can not only be represented as a matrix but also have a different representation.
It is possible to use these representations to construct algorithms that can compute the matrix vector product.
One example of this are circulant matrices. 
These describe time invariant systems and therefore convolutions \todo{check and reword}
Using the DFT matrices, circulant matrices can be transformed into diagonal matrices.
This allows us to obtain a faster algorithm for the product $y=Tu$ by suing the Fast Fourier Trandform(FFT):
We can first compute the FFT of the input, then multiply the vector element-wise with a weight vector. Finally we use the inverse FFT to obtian the result $y$.
Instead of of the order $\bigO(n^2)$ we now have $\bigO(n\log(n))$.
This result is used in widely used in Convolutional Neural Networks (CNN).
Instead of using a matrix vector products the underlying structure is utilized and convolutions are used.

One might analogously try to use other structures to represent weight matrices:
A possible representation are sequentially semiseperable matrices. 
These matrices represent the input output behaviour of
Time varing systems.
These systems consist of  subsystems, so called stages that can change over time.
%Every stage can be described using four small matrices.\todo{reword}
Therefore we can also use this state space representation of the system to describe the matrix.
It is also possible to to calculate the matrix-vector product using algorithms based on this representation.
%The number of inputs and outputs of a system also create a segmentation of the system.
The definition of the Sequentially semiseperable matrix also results in the fact that these matrices have a underlying segmentation.
This segmentation is often determined by a known system ot an underlying physical system and therefore known. 
The matrices in neural networks do not necessarily have the same underlying structure.
Even if the matrix is close to a sequentially semiseperable matrix the structural parameters are usually not know.
To represent the matrices, one need algorithms to derive these structural parameters.
%This can be split in different sub problems.
%Firs the number of stages is not known.
There are existing algorithms to calculate the state space representation for a known segmentation.
After guessing the structural parameters we use these algorithms to create a system.
After this we refine the parameters by an algorithm that can change the structural parameters of a system.
As the weight matrices usually are not sequentuilly semiseperable we also need to approximate the system, if we want to reduce the computational cost.
This will also be incorporated in the algorithm.

In section different approaches to use structured matrices in neural nets are collected.
In section related matrix structures will be presented.
After this the Sequentually semiseperable matrices that will be used in this thesis will be explained in more detail in chapter
In chapter <methods> the methods to create a state space representation for a matrix with unknown structural parameters will be presented.
%the sequentially semeiseperabel structure will be used to approximate matrices. 
%For these matrices it the structural parameters are not known a prior.
In section <approx> an algorithm for the approximation of a system with a different system is explored.
%In section we give an algorithm, for the approximation of systems.
Based on this choose the structural parameters are chosen.
In section <> an approach to determine the number of stages for a system will be presented
We still have to determine the input and output dimensions.
%An Algorithm to adapt the input and output dimentiosn of a system will be explained.
This is done by starting with an initial guess and then refine the structure by adapt the input and output dimensions.
In section <> an algorithm to change these dimensions is given.
Additionally this section presents strategies how to use these algorithms in cases where the final structure has to be determined.
In chapter the algorithms will be tested.
Section contains the results for test cases and in section the algorithm are used to approxiamte weight matrices.

´

\chapter{Literature Review}
\todo[inline]{Some test}

\section{Neural Nets}
%Special matrix structures are been explored for the use in neural nets.
%Here different matrices structures are used to improve the performance. 
In this section different approaches that use structured matrices in neural nets are shortly introduced.
\todo[inline]{moved to introduction delete/replace this}
\textcolor{blue}{The linear layers in matrices can be represented with matrices.
Usually full matrices are used. These do not have a particular structure.
It is also possible to use structured matrices.
These often have representations for which a matrix vector products can be computed with a reduced cost.
Instead of using the matrix vector product one might use these structured representations that are computationally cheaper.
%Instead of using matrix vector products the underlying structure is used to replace the multiplications with convolutions.
%One might analogously try to use the structure of a matrix to represent it with a different representation that has a reduced computational cost.
One widely known example are Convolutional Neural Networks (CNN).
The convolutions are equivalent with an multiplication with a circulant matrix.
Instead of using a matrix vector products the underlying structure is utilized and convolutions are used.
One might analogously try to use the structure of a matrix to represent it with a different representation that has a reduced computational cost.}
There are two main approaches to obtain structured matrices:
In one case the structure is predetermined and the parameters of the structure are trained by a regular training process \cite{fan_multiscale_2019,dao_kaleidoscope_2020,li_butterfly_2015,ailon_sparse_2021,ioannou_training_2016}.
Or the neural net is trained without a predetermined matrix structures and the matrices are later approximated with structured matrices \cite{wu_hybrid_2020,hassibi_optimal_1993,jaderberg_speeding_2014,rigamonti_learning_2013}. In \cite{yu_compressing_2017} the model was retrained after the approximation. 
There are also approaches where the approximation is done during the training procedure like \cite{dettmers_sparse_2019} or an structure is created using regularization \cite{louizos_learning_2018,wen_learning_2016}.


One widely known approach to reduce the complexity of neural nets is pruning. 
These are processes that set elements in neural net matrices to 0 to reduce the computational cost \cite{blalock_what_2020}.
An approach to remove parameters from a trained net based on the Hessian of the parameters\todo{how to phrase} was proposed by Hassibi \cite{hassibi_optimal_1993}.
It is also possible to obtain sparse matrices while training.
Dettmers and Zettlemoyer use an algorithm that includes pruning steps during the training after every epoch \cite{dettmers_sparse_2019}.
A different approach to obtain sparse matrices is including a regularization term.
As an example Louizos et al. use $L_0$ regularization  \cite{louizos_learning_2018} and Wen et al. use LASSO regularization \cite{wen_learning_2016}.

Low Rank approximations for filters in convolutional layers were proposed by Rigamonti et al. \cite{rigamonti_learning_2013}. These use combinations of separable filters. Jaderberg et al. approximate existing filters using low rank filters \cite{jaderberg_speeding_2014}. 
Ioannou et al. directly learn low rank filters and combine them later \cite{ioannou_training_2016}.
In low rank and sparse decomposition a matrix is approximated with a low rank and a sparse matrix. 
Yu et al. represented matrices in deep models as a sum of low rank and sparse matrices \cite{yu_compressing_2017}.
The computation of the decomposition is based on an algorithm to calculate a low rank and sparse decomposition by Zhou and Tao \cite{zhou_greedy_2013}.
To avoid an accumulation of the error due to the compression of multiple layers the decomposition are not done independently.
This is archived by defining the objective function as the difference between the outputs of the approximation for a collection of approximated input vectors and the corresponding output vectors.

The structure of Hirarchical matrices will be explained in subsection\,\ref{subsec:H-mat}. These matrices have also been used in machine learning. 
Fan et al. used the structure of $\mathcal{H}$-Matrices in neural nets to solve PDEs \cite{fan_multiscale_2019}. There the structure of the neural net makes use of the different scales in the system.
Hierarchical tensor decomposition were used by Wu et al. to represent weight matrices and convolutional kernels \cite{wu_hybrid_2020}.
Ithapu used hierarchical factorization of covariance matrices to explore relationships between classes \cite{ithapu_decoding_2017}.

A different matrix structure explored in neural nets are butterfly matrices.
These are products of sparse matrices that overall have a similar structure to the fast Fourier transform \cite{li_butterfly_2015,parker_random_1995}. Ailon et al. used this matrix structure to represent dense layers \cite{ailon_sparse_2021}.
The butterfly structure was combined with CNNs by Li et al. \cite{li_butterfly-net_2020}. This structure can be used to efficiently represent Fourier kernels.
An extension of butterfly matrices are Kaleidoscope matrices (K-matrices). These were introduced by Dao et al. in \cite{dao_kaleidoscope_2020}.
K-Matrices are product of factors of the shape $B_aB_b^\top$, where $B_a$ and $B_b$ are butterfly matrices. These can be used to represent a wide class of structured matrices.
The patterns in the factors are fixed. Therefore the parameters can be learned using gradient descent.
K-matrices can also be used to represent linear hand crafted structures like reprocessing steps. %example fft 

\todo[inline]{
Properties of dense matrices
Approxiamtions of dense Layer matricies}



\section{Matrix structures}
\todo[inline]{eventually move this section to Background}

\subsection{Matrix structures}
In the following certain matrix structures are presented.
These have in common that the rank of sub-matrices are important.
Here semiseperable, Hierarchical and Sequentially semiseparable matrices are presented.

\subsection{Semiseperable matricies}
Semiseperable matrices are not consistently defined in the literature. 
In this thesis the definitions described by Vandebril \cite{vandebril_bibliography_2005,vandebril_matrix_2007} is used.
An important differentiation are generator representable semiseparable matrices and semiseparable matries.
\paragraph{Generator Representable Semiseparable Matrix}
A matrix $S$ is a generator representable semiseparable matrix if the lower and upper triangular parts of $S$ are taken from rank 1 matrices.
This can be expressed as 
\begin{align}
	\triu(S) &= \triu(pq^\top)\\
	\tril(S) &= \tril(uv^\top)
\end{align}
Where $\triu$ is the upper triangular matrix and $\tril$ is the lower triangular matrix. The vectors $p,q,u$ and $v$ are called the generators.
It is important to note here that the diagonal of $S$ is both included in the lower and upper triangular matrix.

\paragraph{Semiseparable Matrix}
In a semiseperable matrix every subblock selected from the lower triangular part of $S$ have rank 1. The analogous statement has to be fulfilled for the upper triangular part.
This can be formalized as 
\begin{align}
	\rank(S_{i:n,1:i}) &\leq 1 & \forall_i &\in\{i,\dots,n\}\\
	\rank(S_{1:i,i:n}) &\leq 1 & \forall_i &\in\{i,\dots,n\}
\end{align}
A extension of this matrix class are the semiseparable plus diagonal matrices.

\paragraph{Quasiseparable Matrix}
The quasiseparable matrices are similar to the semiseperable matries. In a quasiseparable matrix every subblock selected from the strictly lower strictly upper triangular part of $S$ have rank 1. 
This can be formalized as 
\begin{align}
\rank(S_{i+1:n,1:i}) &\leq 1 & \forall_i &\in\{i,\dots,n\}\\
\rank(S_{1:i,i+1:n}) &\leq 1 & \forall_i &\in\{i,\dots,n\}
\end{align}

A quasiseperable matrix is illustrated in Figure\,\ref{fig:quasiseperable}. All the marked submatrices have the property that their rank is 1.
\begin{figure}
	\centering
	\input{diagrams/semiseperable}
	\caption{Illustration of a quasiseperable matrix}
	\label{fig:quasiseperable}
\end{figure}
As the quasiseperable structure does not impose conditions on the diagonal it is more general as the semiseperable matrices 

There is an relation between invertible semiseperable matrices and invertible tridiagonal matrices.
The inverses of a generator representable semiseparable matrix is a irreducible tridiagonal matrix and vice versa. 
The inverse of a semiseparable matrix is a tridiagonal matrix and vice versa.
If a invertible quasiseparable matrix is inverted, the inverse is again a quasiseparable matrix.

These matrix classes can also be extended for higher ranks.
A matrix $S$ is a generator representable semiseparable matrix of semiseparability rank r if there exist the matrices $R_1$ and $R_2$ with $\rank(R_1)=r$ and $\rank(R_2)=r$ such that
\begin{align}
\triu(S) &= \triu(R_1)\\
\tril(S) &= \tril(R_2)
\end{align}

A similar definition for semiseparable matrices of semiseparability rank $r$ is given in \cite{vandebril_bibliography_2005}.
%For this class of matrices and some slight alterations there are algorithms for the efficient calculation of different operations.

\subsection{Hirarchical Matrices}\label{subsec:H-mat}
The Hirarchical matrices ($\mathcal{H}$-Matrices) are a matrix structure to approximate large matrices. These were mainly introduced by Hackbusch \cite{hackbusch_hierarchische_2009} and Grasedyck \cite{grasedyck_theorie_2001}.
The $\mathcal{H}$-Matrices where developed for the solution of PDEs.
If an PDEs is solved numerically, it has to be discreticed in order to obtain an approximated solution.
As the disscretization already introduces errors, it is advantageous to drop the requirement that the matrix representation is exact, if this results in a reduction of the computational cost.
This can be done by partitioning the matrix in segments. These blocks are represented by low rank matrices. It the rank is far smaller than the size of the matrix this representation is cheaper in terms of storage and in terms of computational cost.
The partitioning is done by hierarchically dividing blocks that cannot be represented using a low rank representation.
To decide if a block has to be divided an admissabillity condition is introduced.
This is a way to predict the representability of a matrix block.
For discretizations of PDEs this admissibility condition is usually based on the geometrical distance.
For other applications different admissibility conditions have to be derived.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\input{diagrams/H-matrix.pgf}
		\caption{Matrix Segmentation}
		\label{fig:strukturh-matrix_a}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
	\centering
	\resizebox{0.7\textwidth}{!}{
	\input{diagrams/H-tree.pgf}
	}
	\caption{Block Bluster Tree}
	\label{fig:strukturh-matrix_b}
	\end{subfigure}
	\caption{Illustration of the structure of a $H$-Matrix}
	\label{fig:strukturh-matrix}
\end{figure}


An $\mathcal{H}$-Matrix is shown in Figure\,\ref{fig:strukturh-matrix}a. 
In this case the matrix is divided in four blocks.
If a block is admissible, it is stored as a low rank representations. These are illustrated as the white rectangles.
If a block is not admissible, the block is divided in smaller subblocks.
If matrices are already small and still not admissible the matrices are stored directly. In Figure\,\ref{fig:strukturh-matrix}a these blocks are colored red.
As the partition is done in a hierarchical fashion the matrices can be represented in a block-tree. 
The bock-tree for the matrix in Figure\,\ref{fig:strukturh-matrix_a} is illustrated in Figure\,\ref{fig:strukturh-matrix_b}.
The Hierarchical matrices make it possible to compute different matrix operations efficiently. \todo[inline]{some detailed statement on complexity}
The need for efficient operations also constraints the possible sizes of the blocks as these should be compatible.

\subsection{Sequentially semiseperable}
\todo[inline]{Quellen, also which notation, note that book is on the transposed...}
Sequentially semiseperable matrices were introduced in \cite{dewilde_time-varying_1998}.
As the Sequentially semiseperable maticies will be used in this thesis a introduction will be given in chapter\,\ref{chap:background}. 
There it will be explained form the standpoint of time varying systems.
Here the commonalities and differences to the semiseperable and hirarchical matrices are explored.

The sequentially semiseperable matrices have the properties that submatrices have a low rank.
This is similar to semiseperable matrices. 
Unlike the semiseperable matrices this is not true for all matrices taken form the lower and upper triangular part.
The sequentially semiseperable matrices are divided into blocks.
Matrices taken form the strict lower and strict upper triangular blockmatrix have the condition that their rank is low.
This is illustrated in Figure\,\ref{fig:sequentiallysep}.

\begin{figure}[htb]
	\centering
	\input{diagrams/sequentially_sep}
	\caption{Illustration of a sequentially semiseperable matrix. The thick dotted lines mark the segemntation of the matrix. The thin lines represent submatrices with low rank.}
	\label{fig:sequentiallysep}
\end{figure}

This segmentation in blocks makes similar to the hirarchical Matrices that also have a segmentation.
Unlike semiseperable matrices the sequentially semiseperable matrices do not have to be quadratic. 

\chapter{Background}\label{chap:background}


Sequentially semiseperable Matrices can be considered as descriptions of time varying systems.
\footnote{
The naming and structure of the matrices is not consistent in the literature.
Here we will use the notation used in \cite{tong_blind_2003}. 
In \cite{dewilde_time-varying_1998} a similar notation is used, but the transfer operator is transposed.
In other works like \cite{rice_efficient_2010,chandrasekaran_fast_2002} the causal and anticausal parts are considered jointly
}
These systems can be described using the formulas
\begin{subequations}
	\begin{align}
	x_{k+1} &= A_k x_k + B_k u_k \\
	y_k &= C_k x_k + D_k u_k 
	\end{align}.
	\label{eq:def_causal}
\end{subequations}

The $u_k$ are the inputs, the $y_k$ are the outputs and the $x_k$ are the states.
The matrices $A_k$, $B_k$, $C_k$ and $D_k$ can vary with the time index $k$.
This is different from time-invariant systems, where the matrices $A$, $B$, $C$ and $D$ are constant.
This also means that the dimensions of the input, the output and the states can vary with time.
The structure of a system is represented in Figure\ref{fig:struktur-system_a}.
\begin{figure}[htb]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		%\resizebox{0.7\textwidth}{!}{
		\input{diagrams/Example_system.pgf}
		%}
		\caption{Causal system}
		\label{fig:struktur-system_a}
	\end{subfigure}
	\hspace{0.8cm}
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		%\resizebox{0.7\textwidth}{!}{
		\input{diagrams/Example_system_anti.pgf}
		%}
		\caption{anticausal system}
		\label{fig:struktur-system_b}
	\end{subfigure}
	\caption{Illustration of the structure of time varying systems}
	\label{fig:struktur-system}
\end{figure}
Here we can see that the system can be structured in stages.
The stage $k$ has the matrices $A_k$, $B_k$, $C_k$ and $D_k$.
The matrix $A_k$ maps the old state $x_k$ to the next state $x_{k+1}$, the matrix $B_k$ maps the current input $u_k$ to the next state $x_{k+1}$.
The matrix $C_k$ maps the old state $x_k$ to the output $y_k$.
Finally the matrix $D_k$ directly maps the input $u_k$ to the output $y_k$.
The system defined in Equation\,\ref{eq:def_causal} is a causal system.
This means that the output $y_k$ only depends on the current input $u_k$ and the previous inputs $u_l$ with $l < k$.
It is also possible to define an anticausal system where the output only depends on the current and the later inputs.
An anticausal system is described using the formula
\begin{subequations}
	\begin{align}
	x_{k-1} &= A_k x_k + B_k u_k \\
	y_k &= C_k x_k + D_k u_k 
	\end{align}.
	\label{eq:def_anticausal}
\end{subequations}

In Figure\ref{fig:struktur-system_b} an anticausal system is illustrated.
We can see that the structure is analogous except for the direction of the arrows for the states $x_k$. \todo{rephrase}

When we stack the input vectors to a one vector $u = [u_1^\top, \dots ,u_k^\top]^\top$ and stack the outputs vectors to a one output vector $y = [y_1^\top, \dots ,y_k^\top]^\top$ we can describe the input-output behaviour of the system using a single matrix vector product 
\begin{equation}\label{eq:System_mat_vec}
	y = Tu,
\end{equation}
where the matrix $T$ is called the transfer operator.

The matrix $T$ for a causal system with four stages is
\begin{equation}\label{eq:T_causal}
T_\text{causal}=
\begin{bmatrix}D_{1} & 0 & 0 & 0\\C_{2} B_{1} & D_{2} & 0 & 0\\
C_{3} A_{2} B_{1} & C_{3} B_{2} & D_{3} & 0\\
C_{4} A_{3} A_{2} B_{1} & C_{4} A_{3} B_{2} & C_{4} B_{3} & D_{4}\end{bmatrix}
.
\end{equation}
For an anticausal system the matrix description is 
\begin{equation}
T_{\text{anticausal}}=
\begin{bmatrix}D_{1} & C_{1} B_{2} & C_{1} A_{2} B_{3} & C_{1} A_{2} A_{3} B_{4}\\
0 & D_{2} & C_{2} B_{3} & C_{2} A_{3} B_{4}\\
0 & 0 & D_{3} & C_{3} B_{4}\\
0 & 0 & 0 & D_{4}\end{bmatrix}.
\end{equation}
The matrices for a higher number of stages are analogous.
One can see that the causal system results in a lower triangular blockmatrix whereas the anticausal system results in a upper triangular blockmatrix.
If one wants to represent the whole Matrix one can use mixed systems.
These can be defined as the sum of a causal and an anticausal system.
These usually have the same input and output dimensions, while the state dimensions can de different for the causal and anticausal part.
When adding the systems,  the $D$ matrices have to be added.
As the use of two seperate $D$ does usually not have any benefits, the $D$ matrices for the anticausal system are usually set to 0 and therefore can be ignored.
When representing matrices the first state $x_1$ and the last stage $x_{n+1}$ are usually set to zero dimensions.

\paragraph{Hankel Operator and Minimality}
When we are interested in properties of the system, we can study the relation between the inputs, the states and the outputs. 
The reachability matrix $\R$ is the mapping from the inputs to the state. For a causal system the reachability matrix for state $x_k$ can be written as
\begin{equation}
	\R_k = \begin{bmatrix}
	 \dots & A_{k-1}A_{k-2}B_{k-1} &A_{k-1}B_{k-2} &B_{k-1}
	\end{bmatrix}
\end{equation}
The mapping form the state to the output is called the observability operator $\Ob$.
For a causal system the observability matrix for state $x_k$ can be written as
\begin{equation}
	\Ob_k = 
	\begin{bmatrix}
		C_k\\
		C_{k+1}A_k\\
		C_{k+2}A_{k+1}A_k\\
		\vdots
	\end{bmatrix}.
\end{equation}
When we multiply these we obtain the mapping from the inputs to the outputs. This is called the Hankel operator $H_k$.
For a causal system this can be written as
\begin{equation}\label{eq:H_def}
	H_k = \Ob_k \R_k = 
		\begin{bmatrix}
\dots   & C_{k}A_{k-1}B_{k-2} & C_{k}B_{k-1} \\
\dots   & C_{k+1}A_{k}A_{k-1}B_{k-2} & C_{k+1}A_{k}B_{k-1} \\
\iddots &\vdots &\vdots
\end{bmatrix}
%%3x3 Version
%		\begin{bmatrix}
%		\dots & C_{k}A_{k-1}A_{k-2}B_{k-3}  & C_{k}A_{k-1}B_{k-2} & C_{k}B_{k-1} \\
%		\dots & C_{k+1}A_{k}A_{k-1}A_{k-2}B_{k-3}  & C_{k+1}A_{k}A_{k-1}B_{k-2} & C_{k+1}A_{k}B_{k-1} \\
%		\dots & C_{k+2}A_{k+1}A_{k}A_{k-1}A_{k-2}B_{k-3}  & C_{k+2}A_{k+1}A_{k}A_{k-1}B_{k-2} & C_{k+2}A_{k+1}A_{k}B_{k-1} \\
%		\iddots & \vdots &\vdots &\vdots
%		\end{bmatrix}
\end{equation}
This is the same matrix strucure as we have seen in Equation\,\ref{eq:T_causal}.
\todo{reword}

If the rows  of rows of $\R_k$ are linearly independent, the range of $\R_k$ is $\mathbb{R}^n$, where $n$ is the number of dimensions of $x_k$.
This means that we can reach every state with some input.
Therefore we call a system reachable if every $R_k$ has a full row rank.
If the columns of $O_k$ are linearly independent, we can reconstruct the state $x_k$ from the output.  
Therefore we call a system observable if every $R_k$ has a full column rank.

If a system is both observable and reachable, it is called minimal.
In a minimal system the state dimension is cannot be further reduced.
\todo{reword}
The rank of the Hankel operator is called the Hankelrank. 
If the system is minimal we have 
\begin{equation}
	\rank{H_k} = rank{\Ob_k} = \rank{R_k} = n_k
\end{equation}
for every $k$.
Usually the state dims are far smaller than the input and output dims.
This allows us to factor the $H_k$ in a tall and a flat matrix. 
This results in the factorization $H_k = \Ob_k\R_k$

\paragraph{State Transforms}
As this factorization is not unique, the state in also not uniquely defined. 
The state can be transformed with an non-singular state transform matrix $S$ according to $\tilde{x} =Sx$.
This gives a transformed 
reachabilty matrix $\tilde{\R_k}=S \R_k$ and the transformed
observability matrix $\tilde{\Ob_k}= \Ob_k S^{-1}$. 
The Hankel matrix stays the same as $\tilde{H_k} = \Ob_k S^{-1} S \R_k= \Ob_k \R_k = H_k$
There are are three particular types of factorization that are called canonical forms.
An input-normal realization has the property that the columns of each $\Ob_k$ are orthormormal. Whereas in an output-normal realization, all rows of each $\R_k$ are orthonormal.
The balanced realization results form the reduced Singular value decomposition $H_k = U_k \Sigma_k V_k^\top$. 
In a balanced realization $\Ob_k = U_k \Sigma^{1/2}$ and $\R_k = \Sigma^{1/2} V_k^\top$.
These canonical form usually also require that the system is minimal. In this thesis the minimality is usually omitted as the algorithms described also work on non-minimal systems and minimality is hard to define numerically for systems where the $\sigma$s decay slowly. \todo[]{finite equivalent of compact operators, search for word} If minimality is required, it will be explicitly stated.

\paragraph{Matrix Factorization}
The sequentially semisperabel matrices can also be considered as a matrix factorization.
For a causal system,the matrix $T$ can be expressed as a product $T = T_n T_{n-1} \dots T_2 T_1$
Where every $T_k$ represents a stage. The matrices $T_k$ is constructed according to 

\begin{equation}
\newcommand{\sddots}{\hspace{-6pt}\ddots}
\newcommand{\n}{\hspace{-7pt}}
	T_k =
	\begin{bmatrix}
	\begin{array}{c|cccccccc}
	A_k&    &   & & B_k & & &\\
	\hline
	   &\eye&   &&     & & &\\[-8pt]
	   & &\sddots&&    & & &\\[-6pt]
	   & & &\n\eye&      & & &\\
	C_k& & &    & D_k  & & &\\
	   & & &    &      &\eye& &\\[-8pt]
	   & & &    &      & &\sddots&\\[-6pt]
	   & & &    &      & & &\n\eye 
	\end{array}
	\end{bmatrix} 
\end{equation}

For a system with 3 stages
\begin{equation*}
	T=
	\left[\begin{matrix}A_{3} & 0 & 0 & B_{3}\\0 & 1 & 0 & 0\\0 & 0 & 1 & 0\\C_{3} & 0 & 0 & D_{3}\end{matrix}\right]
\dots
%	\left[\begin{matrix}A_{2} & 0 & B_{2} & 0\\0 & 1 & 0 & 0\\C_{2} & 0 & D_{2} & 0\\0 & 0 & 0 & 1\end{matrix}\right]
	\left[\begin{matrix}A_{1} & B_{1} & 0 & 0\\C_{1} & D_{1} & 0 & 0\\0 & 0 & 1 & 0\\0 & 0 & 0 & 1\end{matrix}\right]
	=
	\left[\begin{matrix}A_{3} A_{2} A_{1} & A_{3} A_{2} B_{1} & A_{3} B_{2} & B_{3}\\C_{1} & D_{1} & 0 & 0\\C_{2} A_{1} & C_{2} B_{1} & D_{2} & 0\\C_{3} A_{2} A_{1} & C_{3} A_{2} B_{1} & C_{3} B_{2} & D_{3}\end{matrix}\right]
\end{equation*}

As $A_1$ has zero columns and $A_3$ has zero rows, the first block-column and the top block-row disappear and we get the familiars structure form Equation\,\ref{eq:T_causal}.

An analogous factorization is possible for anticausal systems.
Here the ordering is reversed an we have $T = T_1 T_2 \dots T_{n-1} T_{n}$

We can also represent mixed systems as factorization, if we use insert the factorizations for the causal part $T_{\text{causal}}$ and the anticausal parts $T_{\text{anticausal}}$ into
\begin{equation}
	T = 
	\begin{bmatrix}
	\eye &
	\eye
	\end{bmatrix}
	\begin{bmatrix}
	T_{\text{causal}}&\\
	&T_{\text{anticausal}}
	\end{bmatrix}	
	\begin{bmatrix}
	\eye\\
	\eye
	\end{bmatrix}
\end{equation}
Sequentially semiseperable matrices can be added, multiplied and inverted using algorithm that work on the ABCD description.
It is also possible to calculate QR factorization \cite{chandrasekaran_fast_2002,tong_blind_2003} or URV factorization \cite{chandrasekaran_fast_2005}. 
%Note on URV more papers in Dewilde,Diepold and van der Veen p315 
There are also a couple of other algorithms that can be used for different purposes like sign iterations \cite{rice_efficient_2010}.

\paragraph{Approxiamtions}
It is also possible, to reduce the number of states by approximation a system $T$ with a system $\tilde{T}$.
If the system $T$ is minimal, this is not trivial, as the a reduction of the number of states also result in a reduction of the Hankelrank.
On approach is the Hankel-norm approximation.
The Hankel Norm is defined as
\begin{equation}
	\|T\|_H = \sup_{i}\|H_i\|.
\end{equation}
This norm is the supremeum over the spectral norm (the matrix 2-norm) of each individual Hankel matrix.
In \cite{dewilde_time-varying_1998} an algorithm is given for the hankel rank approxiamtion that computes a $\tilde{T}$.
The approximated system satisfies the condition
\begin{equation}
	\| \Gamma^{-1}(T-\tilde{T})\|_H \leq 1.
\end{equation}
Where $\Gamma$ is a diagonal and hermition operator. 
If we set $\Gamma = \eye\gamma$ we obtain the simplified condition
\begin{equation}
	\|T-\tilde{T}\|_H \leq \gamma.
\end{equation}

A second approach is balanced truncation is described in . 
Here the state dimensions $x_k$ are reduced to a new dimension $\tilde{n}_k$.
The idea is to remove stated that result in a small output $y$ or need a relatively big input $u$.
We can calculate the norm of the output for a state $x$ using
\begin{equation}
	\|y\|_2^2 = u^\top u = x^\top \Ob^\top \Ob x
	.
\end{equation}
The matrix $\Ob^\top \Ob$ is called the Observability gramian.
analogously we can calculate the norm of the input required ti reach a certain state using
\begin{equation}
	\|u\|_2^2 = x^\top (\R\R^\top)^{-1} x
	.
\end{equation}
Where the Reachability gramian $\R\R^\top$ is invertible if the system is reachable.
The involved states can be changed using state transforms.
The idea is now to obtain a state transform, for which both problems are equivalent. 
This is the case for a balanced realization
\begin{equation}
	\Ob^\top \Ob = \Sigma^{1/2}U^\top U\Sigma^{1/2} = \Sigma = \Sigma^{1/2} V^\top V \Sigma^{1/2} = \R \R^\top 
\end{equation}
This results in a basis for the state with 
\begin{equation}
	\|y\|_2^2\big|_{x = e_l} = \|\Ob x\|_2^2\big|_{x = e_l} = \sigma_l
\end{equation}
and 
\begin{equation}
\|u\|_2^2\big|_{x = e_l} = \sigma_l^{-1}
\end{equation}
Where $e_l$ is the $l$-th standard basis vector.
This allows us to only keep the states with $\|y\| > \epsilon^{1/2} \|x\|$ and $\|x\| > \epsilon^{1/2} \|u\| $ for some $\epsilon>0$ by cutting all dimensions $l$ for all $l$ with $\sigma_l < \epsilon$.
\todo{reword}
%that are
% result in a large output and only need a small input to be reached. 

To obtain the reduced system $\tilde{T}$ the matrices of a balanced realization for every state are partitioned according to
\begin{align}
	A_k &=\begin{bmatrix}
	A_{k[11]} & A_{k[12]} \\
	A_{k[21]} & A_{k[22]} \\
	\end{bmatrix}
	&
	B_k &= \begin{bmatrix}
	B_{k[1]} \\ B_{k[2]}
	\end{bmatrix} 
	\\
	C &= \begin{bmatrix}
	C_{k[1]} & C_{k[2]}
	\end{bmatrix}& 
	D_k&=D_k.
\end{align}
The dimensions are determined by the new state dimensions.
The approximated system $\tilde{T}$ is then set to
\begin{align}
	\tilde{A}_k &= A_{k[11]}  & \tilde{B}_k &= B_{k[1]}\\
	\tilde{C}_k &=C_{k[1]}      & \tilde{D}_k &= D_k.
\end{align}
%This is equivalent to removing the tailing states.
%Because the realization is balanced every state in the original system corresponds to a $\sigma$ of the according Hankel matrix.
This approach is equivalent to removing all $\sigma<\epsilon$s of the SVD of the Hankel operator.
Therefore we can also construct an approximation when identifying a system using the SVD by truncating the SVD as described in \cite{shokoohi_identification_1987}.
\todo[inline]{Check proof in paper}
In an error bound given in \cite{sandberg_balanced_2004}.
\todo[inline]{find an easily understandable derivation or similar}

More details on general approximation.
\cite{antoulas_approximation_2005} detailed discussion of approxiamtions.

\chapter{Methods} 15p

\section{Matrix Approximations}
\subsection{Algorithm}
\begin{itemize}


\item Discuss ordering of the sigmas, and cutting states if the sigmas are ordered.
\item Define ordered realization as extention of the ballanced realization

\item Describe how to get the sigmas for a system by transforming it.General idea: We do not have to compute the SVD of the total Hankel matrix, if we take a input-nomal system and convert it step by step to a output-normal.

\item Describe the overall approximation algorithm.

\paragraph{Extention balanced truncation to nonbalanced systems/Define ordered system}
In Chapter\,\ref{chap:background} the balanced truncation is defined for balanced systems.
These have the property that $\Ob_k = U_k \Sigma_\Ob$ and $\R_k = \Sigma_\R V_k^\top$ where $\Sigma_\Ob = \Sigma_\R=\Sigma^{1/2}$.
The balanced truncation is also possible for a wider class of realizations.

In this thesis these Realizations will be called ordered.
\begin{definition}[Ordered Realization]
	A system is ordered if 
	\begin{subequations}
	\begin{align}
		\Ob_k^\top \Ob_k &= \Sigma_ {\Ob k}^2 \label{eq:orderd_obs}\\ %\diag(\sigma_{\Ob k}^2)\\
		\R_k \R_k^\top &= \Sigma_ {\R k}^2 \label{eq:orderd_reach}%\diag(\sigma_{\R k}^2)
	\end{align}
	\end{subequations}
	where $\Sigma_{\Ob k}$ and $\Sigma_{\R k}$ are diagonal matrices
	and the diagonal entries of $\Sigma_k =\Sigma_{\Ob k}\Sigma_{\R k}$ are non increasing (i.e. $\sigma_1 \leq \sigma_2 \leq \dots \leq \sigma_n$).
	%Positiov sowieso, weil gram matrix
    for every $k$.
\end{definition}

%As the SVD of the Hankel matrix does not change he have $\Sigma_\Ob\Sigma_\R=\Sigma$ as $H = \Ob\R = U_k \Sigma_\Ob \Sigma_\R V_k^\top = U_k \Sigma V_k^\top$.

An ordered system can be transformed into a balanced system using the diagonal state transformations $S_k = \Sigma_k^{1/2}\Sigma_{\R k}^{-1}$ and the inverse $S_k^{-1} = \Sigma_{\Ob k}^{-1}\Sigma_k^{1/2}$.
This gives the balanced system $\tilde{T}$
\begin{align}
	\tilde{A}_k &= S_{k+1} A_k S_k^{-1}   & \tilde{B} =& S_{k+1} B \\
	\tilde{C}_k &= C_k S_k^{-1}   & \tilde{D} =& D 
\end{align}
\begin{proof}
	By applying the state transform we can see that we get a balanced system
	$\Ob_k S_k^{-1} = U_k \Sigma_\Ob \Sigma_{\Ob k}^{-1}\Sigma_k^{1/2} = U_k \Sigma_k^{1/2}$\\
	$S_k \R_k = \Sigma_k^{1/2}\Sigma_{\R k}^{-1} \Sigma_{\R k} V_k^\top = \Sigma_k^{1/2} V_k^\top$
\end{proof}

A ordered system can be reduced using the same segmentation as used for the balanced truncation.
This allows us to use algorithms that require input- or output normal forms, as systems can be both balanced and input normal if $\Sigma_{\R k}=\eye$ or balanced and output normal if $\Sigma_{\Ob k}=\eye$.

\paragraph{Recusrsve factorization of $\Ob_k$ and  $\R_k$}
The goal of this section is to construct an algorithm to calculate the singular values of the Hankel matrices without applying the SVD on $H$ or calculating $H$, $\Ob_k$ or $\R_k$.
The idea is that we can recursively factor the observability matrix according to
\begin{equation}\label{eq:decomp_O}
	\Ob_k = 
	\begin{bmatrix}
	C_k\\
	\Ob_{k+1}A_k
	\end{bmatrix}
	=
	\begin{bmatrix}
	\eye& 0 \\
	0& \Ob_{k+1}
	\end{bmatrix}
	\begin{bmatrix}
	C_k\\
	A_k
	\end{bmatrix}
\end{equation}
And analogously the reachability matrix
\begin{equation}\label{eq:decomp_R}
	\R_k = 
	\begin{bmatrix}
	A_{k-1} \R_{k-1} & B_{k-1}
	\end{bmatrix}
	=
	\begin{bmatrix}
	A_{k-1} & B_{k-1}
	\end{bmatrix}
	\begin{bmatrix}
	\R_{k-1} &0\\
	0& \eye
	\end{bmatrix}
\end{equation}
This allows us to create algorithms, that can convert realizations into input normal and output normal form.
With an combination of both it is also possible to obtain the singular values.
This is similar to the idea used in square root algorithms for the computation of normal forms.
The algorithm derived here was introduced in \cite{chandrasekaran_fast_2005}.
Here a different explanation will be given will be given as well as a derivation of the SVD of the Hankel matrix. 

\todo[inline]{More details on the fact that we do not require mininality and unlike QR based algorithms we can make sure that we do not create spurious observable states....
May bee some proof based on the original rank of $\Ob_{k}$ and $\R_k$.
Also note that a Rank revealing QR-algorithm would also work}

\paragraph{Make input normal and reachable}
First we give an algorithm to transform the system into an reachable and input normal system.
This means that we want to make all $R_k$ subunitary.
As we do not want to calculate the reachability matrices directly we will employ the decomposition given in Equation\,\ref{eq:decomp_R}.
If $\R_{k-1}^\top \R_{k-1}=\eye$ we can make $\R_{k}$ sub orthogonal by calculating the SVD of $[A_{k-1} B_{k-1}]$
\begin{equation}
\R_k = 
%\begin{bmatrix}
%A_{k-1} \R_{k-1} & B_{k-1}
%\end{bmatrix}
%=
\begin{bmatrix}
A_{k-1} & B_{k-1}
\end{bmatrix}
\begin{bmatrix}
\R_{k-1} &0\\
0& \eye
\end{bmatrix}
=U_k\Sigma _k
\underbrace{V_k
\begin{bmatrix}
\R_{k-1} &0\\
0& \eye
\end{bmatrix}}_{\tilde{\R}_k}
\end{equation}
This gives the state transform $S_k = U_k\Sigma_k$. 
The usage of the SVD also allows us to remove states that are not reachable by counting all nonzero elements in $\Sigma$ and reducing the SVD accordingly.
The product with the inverse $S_k^{-1}$ is implicitly calculated by computing the SVD.
Therefore the matrices $\tilde{A}_k$ and $\tilde{B}_k$ are set to $[\tilde{A}_k,\tilde{B}_k]=V_k^\top$.
The matrices $A_{k}$ and $B_{k}$ have to be updated with the state transform $S_k$.

The algorithm is formalized in Algorithm\,\ref{alg:inp_normal}

\begin{algorithm}[htb]
	\begin{algorithmic}
	\For{$k\gets 1$ \textbf{to} $K-1$}
		\State $U,\sigma,V^\top \gets$ SVD$([A_k,B_k])$
		\State $r \gets $ count$(\sigma>\text{tol}_i)$
		\State $U \gets U[:,:r]$
		\State $\sigma \gets \sigma[:r]$
		\State $V^\top \gets V^\top[:r,:]$
		\State $[A_k,B_k] \gets V^\top$
		\State $A_{k+1}=A_{k+1}U\diag(\sigma)$
		\State $C_{k+1}=C_{k+1}U\diag(\sigma)$
	\EndFor
	\end{algorithmic}
\caption{Algorithm to convert to input normal system}\label{alg:inp_normal}
\end{algorithm}

This results in a input normal and reachable system.
\begin{proof}
	From $\tilde{\R}_{k-1}\tilde{\R}_{k-1}^\top = \eye$ follows that $\tilde{\R}_k\tilde{\R}_k^\top = \eye$ as
	\begin{equation}\label{eq:induction_reach}
	\tilde{R}_k\tilde{\R}_k^\top
	=
	V_k^\top\begin{bmatrix}\tilde{\R}_{k-1} &0\\
		0& \eye \end{bmatrix}
	\begin{bmatrix}\tilde{\R}_{k-1}^\top &0\\
		0& \eye \end{bmatrix} V_k
	= 
	V_k^\top\begin{bmatrix}\eye &0\\
	0& \eye \end{bmatrix} V_k
	=
	\eye
	\end{equation}
	As $x_1$ is zero dimensional $R_1$ vanishes and Equation\,\ref{eq:induction_reach} is true for $k=1$.
	By induction this proofs $\tilde{R}_k\tilde{R}_k^\top = \eye$ for all $k \in [1,\dots,K-1]$.
	Reachability directly follows form $\R_k\R_k^\top = \eye$.
\end{proof}




\paragraph{Make output normal}
The algorithm to transform the system into an observable and output normal system is analogous.
Here all $\Ob_k$ have to be subunitary.
Here the decomposition given in Equation\,\ref{eq:decomp_O} is used to avoid a computation of $\Ob_k$.
If $\Ob_{k+1} \Ob_{k+1}^\top=\eye$ we can make $\Ob_{k}$ sub orthogonal by calculating the SVD of $[C_k^\top A_k^\top]^\top$.

\begin{equation}
	\Ob_{k}
	=
	\begin{bmatrix}
	\eye& 0 \\
	0& \Ob_{k+1}
	\end{bmatrix}
	\begin{bmatrix}
	C_k\\
	A_k
	\end{bmatrix}
	=
	\underbrace{
	\begin{bmatrix}
	\eye& 0 \\
	0& \Ob_{k+1}
	\end{bmatrix}
	U_k}_{\tilde{\Ob}_k}
	\Sigma_k V_k^\top
\end{equation}
This results in the state transform $S_k = \Sigma_k V_k^\top$.
Here the SVD is also in a reduced form, that results in a removal of all non observable states.
The state transform with $S_k^{-1}$ is done implicitly by the SVD.
The matrices $A_{k-1}$ and $C_{k-1}$ are updated with the state transform $S_k$

This gives the Algorithm\,\ref{alg:out_normal}
\begin{algorithm}[htb]
	\begin{algorithmic}
	\For{$k\gets K$ \textbf{downto} $2$}
		\State $U,\sigma,V^\top \gets$ SVD$\left(\begin{bmatrix} C_k\\A_k \end{bmatrix}\right)$
		\State $r \gets $ count$(\sigma>\text{tol}_o)$
		\State $U \gets U[:,:r]$
		\State $\sigma \gets \sigma[:r]$
		\State $V^\top \gets V^\top[:r,:]$
		\State $\begin{bmatrix} C_k\\A_k \end{bmatrix} \gets U$
		\State $A_{k-1}=\diag(\sigma)V^\top A_{k+1}$
		\State $B_{k-1}=\diag(\sigma)V^\top C_{k+1}$
	\EndFor
	\end{algorithmic}
	\caption{Algorithm to convert to output normal system}\label{alg:out_normal}
\end{algorithm}
This results in a output normal and observable system.
\begin{proof}
	From $\tilde{\Ob}_{k+1}^\top\tilde{\Ob}_{k+1} = \eye$ follows that $\tilde{\Ob}_k^\top\tilde{\Ob}_k = \eye$ as
	\begin{equation}\label{eq:induction_obs}
		\tilde{\Ob}_k^\top\tilde{\Ob}_k
		=
		U_k^\top
		\begin{bmatrix} \eye& 0 \\
			0& \Ob_{k+1}^\top \end{bmatrix}
		\begin{bmatrix} \eye& 0 \\
		0& \Ob_{k+1} \end{bmatrix}
		U_k
		=
		U_k^\top
		\begin{bmatrix} \eye& 0 \\
		0& \eye \end{bmatrix}
		U_k
		=
		\eye
	\end{equation}
	As $x_{K+1}$ is zero dimensional $\Ob_{k+1}$ vanishes and Equation\,\ref{eq:induction_obs} is true for $K$.
	By induction this proofs $\tilde{\Ob}_k^\top\tilde{\Ob}_k = \eye$ for all $k \in [1,\dots,K-1]$
	Reachability directly follows form $\Ob_k\Ob_k^\top = \eye$.
\end{proof}


\paragraph{Obtain sigmas}
By converting an input normal system to an output normal system it is possible to calculate the singlaur values of $H$.
This uses the factorization in Equation\,\ref{eq:H_def} and the recursion relation in Equation\,\ref{eq:decomp_O}
to get
 \begin{equation}
 H = \Ob_k \R_k = 
 \begin{bmatrix}
 \eye& 0 \\
 0& \Ob_{k+1}
 \end{bmatrix}
 \begin{bmatrix}
 C_k\\
 A_k
 \end{bmatrix}
 \R_k
 \end{equation}.
As the algorithm started with an input normal system $\R_k\R_k^\top = \eye$ is still fulfilled.
And due to the fact the algorithm has transformed the states $x_i$ for $i>k$ to output normal form $\Ob_{k+1}^\top \Ob_{k+1} = \eye$ is already fulfilled.
When the SVD is applied to the matrix $[C_k^\top A_k^\top]^\top$ this results in an SVD decomposition of $H$
 \begin{equation}
 H = 
 \underbrace{
 	\begin{bmatrix}
 	\eye& 0 \\
 	0& \Ob_{k+1}
 	\end{bmatrix}
 	U_k}_{U_k} 
 \Sigma_k 
 \underbrace{\vphantom{\begin{matrix} 0 \\0\end{matrix}}
 	V_k^\top
 	\R_k}_{\breve{V}_k^\top}
 =\breve{U}_k\Sigma_k \breve{V}_k^\top
 \end{equation}

 \begin{proof}
 	To prove that $H = \breve{U}_k\Sigma_k\breve{V}^\top$ is a singluar value decomposition we have to prove that $\breve{U}_k^\top \breve{U}_k= \eye$ and $\breve{V}^\top \breve{V} = \eye$.
 	If $\Ob_{k+1}$, $R_k$, $U_k$ and $V$ are orthogonal the proof directly follows. For the sub unitary case we have 
 	$
 	\breve{V}_k^\top \breve{V}_k
 	=
 	V_k^\top
 	\R_k
 	\R_k^\top
 	V_k
 	=
 	V_k^\top
 	\eye
 	V_k
 	=
 	\eye
 	$
 	and
 	$
 	U_k^\top U_k
 	=
 	\eye 
 	$
 	has been proven in Equation\,\ref{eq:induction_obs}
 \end{proof}

The resulting system is ordered.
\begin{proof}
	The condition in Equation\,\ref{eq:orderd_obs} directly follows form the output normality $\tilde{\Ob}_k^\top \tilde{\Ob}_k = \eye$.
	The condition in Equation\,\ref{eq:orderd_reach} follows form the input normality of the intermediate system by
	\begin{equation}
		\tilde{\R}_k \tilde{\R}_k^\top 
		=
		\Sigma_k V_k \R_k \R_k^\top V_k^\top \Sigma_k 
		= \Sigma_k^2
	\end{equation}
	As $\Sigma_k$ is result form an SVD, the singular values are in decreasing order. 
\end{proof}
Therefore the system can be approximated using balanced truncation.

This algorithm is also a fast way to reduce the system to a minimal system.
The tolerance for the first transformation$\text{tol}_i$ is usually set to a value close to the machine precision to avoid errors due to the removed states.
The tolerance for the second transformation $\text{tol}_o$ is then set to the wanted $\epsilon$.
 
It is also possible to first convert the system to an output normal system and calculate the SVD when converting it to an input normal system.
The proof for this and the algorithms for the anticausal case are omitted for brevity.

\todo[inline]{Some notes on cumputational cost}
 
 
\todo[inline]{is it clear if the ordering makes a difference, slo difference between immediately cutting or cutting later?}

\end{itemize}
\subsection{Cost of computation}
\begin{itemize}
\item Short section on the computational cost and number of parameters.

\item Define it with and without additions.

\item Might also go in the Background, but I think it would be a bit to much and I did not find a detailed discussion in the literature. 
\end{itemize}

When using sss to approximate weight matrices, we are mainly interested in the cost of computing the product $y = Tu$ using time varying systems and the number of parameters.

First we consider a causal system.
The cost $C_k$ for the stage $k$ is 
Without additions this gives the cost for one stage
\begin{equation}
	C_k = d_{k+1}d_k + d_{k+1}m_k+p_kd_k+p_km_k 
\end{equation}
 we can also include the additions and get the cost
 \begin{equation}
 	C_k' = d_{k+1}(2d_k-1) + d_{k+1}(2m_k-1)+p_k(2d_k-1)+p_k(2m_k-1).
 \end{equation}


For anticausal system, the formulas are the same except a change in the indexing 
Without additions this gives the cost for one stage
\begin{equation}
C_k = \da_{k-1}\da_k + \da_{k-1}m_k+p_k \da_k+p_km_k 
\end{equation}
we can also include the additions and get the cost
\begin{equation}
C_k' = \da_{k-1}(2\da_k-1) + \da_{k-1}(2m_k-1)+p_k(2d_k-1)+p_k(2m_k-1).
\end{equation}

If we consider mixed systems we can add the cost for the causal and the anticausal system.
As we do not need the $D$-matrices for the anticausal part we can ignore the according flops.

This gives the cost for a mixed stage

\begin{equation}
C_k = d_{k+1}d_k + \da_{k-1}\da_k + (d_{k+1}+\da_{k-1})m_k +(d_k+\da_k)p_k +p_km_k 
\end{equation}
we can also include the additions and get the cost
\begin{multline}
C_k' = d_{k-1}(2d_k-1)+ \da_{k-1}(2\da_k-1) 
+ (d_{k-1}+\da_{k+1})(2m_k-1)\\
+p_k(2d_k-1)+p_k(2\da_k+1)+p_k(2m_k-1)
.
\end{multline}
In the following we will only use the number of multiplications, as it is easier to simplify.

We get the total cost by adding the cost for all stages.
This gives the total cost 
\begin{equation}
C = \sum_{k=1}^N C_k
\end{equation}
In cases where $d$ $\da$, $p$ and $m$ are constant this can be simplified to a multiplication
And we get
\begin{equation}
C = K(d^2 + {\da}^2 + (d+\da)m +(d+\da)p +pm) 
\end{equation}.
The input and output sizes are related to the number of stages by $m = M/K$ and $p=P/K$.
This results in
\begin{align}
C =& K(d^2 + {\da}^2 + (d+\da)\frac{M}{K} +(d+\da)\frac{P}{K} +\frac{PM}{K^2}) \\
=&
K(d^2 + {\da}^2) + (d+\da)M +(d+\da)P +\frac{1}{K}PM) 
\end{align}.
This relation describes how the general computational cost depends on the number of stages.
This approximation is not helpful if one wants to approximate matrices, as in these cases the state dimension are usually not constant.
Usually the state dimensions are small for the first and last states and larger for the states in the middle.
Here different approximations of the state dimensions have to be used.

I used an approximation based on the properties of the singular values and their relation to the Frobenius norm.
The singular values are related to the Frobenius norm by $\|H\|_F = \sqrt{\sum \sigma_i^2}$.
If we consider the elements of $H$ as constant on average, then $\|H\|_F \propto \text{size(H)}$.
This means that the Frobenius norm increases with the size of the matrix, this should also result in increasing singular values.
This is not directly related to the number of states as it depends on the distribution of the singular values and the wanted accuracy.
But the approximation $d_k \propto \text{size}(H_k)$ fits the behavior of the considered matrices fairly well as illustrated in Figure\todo{make Figure}.
It is also possible to approximate the state dimension based on the the maximum possible rank of the Hankel matrix.
The Rank has to be lower than $\min(\text{width}(H_k),\text{height}(H_k))$.
We approximate the degree using
\begin{equation}\label{eq:approx_d}
	d_k = \gamma (K-k+1)(k-1)\frac{PM}{K^2\min(P,M)}.
\end{equation}
The variable $\gamma$ is a proportionality factor.
The factor $1/\min(P,M)$ is used to normalize the state in such a way that for $\gamma = 1$ the approximation is equivalent to the maximal rank for the first and last states. 
If we suppose that the statedimensions for the anticausal case is equivalent to the statedimensions for the causal case one can calculate the cost using
\begin{equation}
\sum_{k=1}^{K} \left(2 d_{k} d_{k+1} + \frac{2 M d_{k+1}}{K} + \frac{2 P d_{k}}{K} + \frac{M P}{K^{2}}\right)
\end{equation}
As the the matrix can be transposed, $\min(P,M)=M$ can be assumed without loss of generality.
By inserting the approximation form Equation\ref{eq:approx_d} and transforming the Sums using Faulhaber's formula \cite{knuth_johann_1993} the cost is transformed to
\begin{equation}
 \frac{K P^{2} \gamma^{2}}{15} + \frac{M P \gamma}{3} + \frac{P^{2} \gamma}{3} + \frac{M P - \frac{P^{2} \gamma^{2}}{3}}{K} + \frac{- \frac{M P \gamma}{3} - \frac{P^{2} \gamma}{3}}{K^{2}} + \frac{4 P^{2} \gamma^{2}}{15 K^{3}}
\end{equation}
Thsi makes it possible, to calculate the cost for different $\gamma$s.
In Figure\todo{figure}%\,\ref{fig:cost_parameters} 
this is done for a example with $M=N = 2^{10}$.

One can see that for one stage the cost is equivalent to the cost of a simple matrix product, as the whole matrix is stored as one $D$-matrix.
For $\gamma = 1$ the cost only increases for higher number of stages.
For $\gamma < 0$ the cost first increases before increasing later. 
We can see that the optimal number of stages increases as $\gamma$ decreases.
%This means that the number of stages can be higher if the stete diemsniuon 
%This is not surprising, as for a high number of stages and a high state dimension, the 


\subsection{Number of stages}
\todo[inline]{
	Write this up in a notebook, \\
	Question:what number of stage is best?\\ 
	Start with simple example and constant state dims\\
	note that this approximation is not good, other approxiamtions -> see notebook}
\begin{itemize}
\item Describe continuous surrogate problem for the cost depending on the number of parameters. 
\item Discuss educated guesses for the number of stages.
\end{itemize}

\section{Segmentation Adaptation}
\begin{itemize}
\item Describe algorithm of moving boundaries.
\item First for causal system, then for mixed system.

\item Describe splitting and combining states.

\item Pseudocode for the causal systems

\item Move all the proves of minimality to the appendix for brevity.
\item Describe calculation of $\sigma$s
\end{itemize}
\subsection{Moving Boundaries}
In this section an algorithm to move the boundaries of a causal system is introduced.

The algorithms can be constructed in a way such that they preserve minimality.
In Subsection\,\ref{subsec:move_sig} the algorithm are going to be extended in a way that makes it possible to calculate the singular values.

When the boundaries are moved, some connections that are no longer possible in a causal system will be dropped or new connections will be established.
This changes the parts of the matrix that can be represented with a causal system.
For an mixed system these connections can be moved to the anticausal system and vice versa.
\paragraph{move left}
First the bounds are moved left.
This means that the last element of the input $u_k$ is removed and inserted as a new first element in $u_{k+1}$. 
The moved input is designated as $u_\m$
This is illustrated in Figure\,\ref{fig:move_left}.
\begin{figure}[htb]
	\centering
	\input{diagrams/Moves/Move_left.pgf}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{Original system}
		\label{fig:move_left_a}
	\end{subfigure}
	\hspace{0.8cm}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{Moved system}
		\label{fig:move_left_b}
	\end{subfigure}
	\caption{Illustration of the movement of boundary}
	\label{fig:move_left}
\end{figure}
The vector $b$ is the last column of $B_k$ and describes the mapping from $u_\m$ to $x_{k+1}$. The vector $d$ is the last column of $D_k$ and describes the mapping from $u_\m$ to $y_{k}$.
These are removed form the matrices and we get $\breve{B}_k$ and $\tilde{D}$
We can see that the connection $d$ in the original system cannot by realized with an causal system as this would mean a mapping from $\tilde{u}_{k+1}$ to $y_k$ in the transformed system.
The mapping form the input $u_\m$ to the output $y_{k+1}$ can be directly incorporated by adding a the vector $A_{k+1}b$ to $D_{k+1}$ as a new column.
Analogously the mapping to the state $x_{k+1}$ can be incorporated by attaching the vector $Ab$ to $B_{k+1}$.
As the last column of $\R_{k+1}$ is removed, this might result in a non reachable system.
This is the case if the matrix $[A_b \breve{B}_k]$ no longer has full rank.
This can be checked using the SVD. 
If the last $\sigma$ is equal to $0$ the state has to be removed. 
This can be done using a state transform based on the reduced SVD. 
The matrices $[\tilde{A}_b \tilde{B}_k]$ are set to $\diag{sigma} V^\top$ and 
the following stages are transformed according to $\tilde{A}_{k+1} = A_{k+1}U$ and $\tilde{C}_{k+1} = C_{k+1} U$.
 
\begin{algorithm}[htb]
	\begin{algorithmic}
		\State $b \gets B_k[:,-1]$
		\State $D_k \gets D_k[:,:-2]$
		\State $B_k \gets B_k[:,:-2]$
		\State $D_{k+1} \gets [C_{k+1}b,D_{k+1}]$
		\State $B_{k+1} \gets [A_{k+1}b,B_{k+1}]$
		\State $U,\sigma,V^\top \gets$ SVD$([A_k,B_k])$ 
		\If{$\sigma[-1] < \text{tol}$}
			\State $U \gets U[:,:-2]$
			\State $\sigma \gets \sigma[:-2]$
			\State $V \gets V[:-2,:]$
			\State$[A_k,B_k] \gets \diag(\sigma) V^\top$
			\State$A_{k+1} \gets A_{k+1}U$
			\State$C_{k+1} \gets C_{k+1}U$
		\EndIf
	\end{algorithmic}
	\caption{Algorithm to move a boundary between $u_k$ and $u_{k+1}$ to the left}\label{alg:move_left}
\end{algorithm}

\paragraph{Move down}
A similar strategy can be used to move a boundary down.
That means removing the first output $y_\m$ form $y_{k+1}$ and adding it to $y_k$.
This is illustrated in Figure\,\ref{fig:move_left}.
The vector $c^\top$ describes the mapping form $x_k$ to $y_\m$.
This is also the first row of $C_{k+1}$.
When moving the bound the first row of $C_{k+1}$ and from $D_{k+1}$ are removed.
To attach the output $y_\m$ to the output vector $y_k$ the vector $c^\top B_k$ is added as last row to $D_k$ and the vector $c^\top A_k$is added as last row to $B_k$.
As the algorithm removes a row from $\Ob_{k+1}$ it might happen that the state $x_{k+1}$ is no longer observable.
This is the case if the kernel of $\Ob_{k+1}$ is not zero subspace.
This can be checked with the SVD of $[A_{k+1}^\top,C_{k+1}^\top]^\top$.
If the last singular values is equal to zero the state has to be reduced.
This is done by using the reduced matrix $V$ as state transform.
Again the inverse does not need to be calculated as it is implicitly calculated by the SVD.
An algorithm to move the state is given in Algorithm\,\ref{alg:move_down}



\begin{figure}[htb]
	\centering
	\input{diagrams/Moves/Move_down.pgf}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{Original system}
		\label{fig:move_down_a}
	\end{subfigure}
	\hspace{0.8cm}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{Moved system}
		\label{fig:move_down_b}
	\end{subfigure}
	\caption{Illustration of the movement of boundary}
	\label{fig:move_down}
\end{figure}

\begin{algorithm}[htb]
	\begin{algorithmic}
		\State $c^\top \gets C_{k+1}[1,:]$
		\State $D_{k+1} \gets D_{k+1}[2:,]$
		\State $C_{k+1} \gets C_{k+1}[2:,:]$
		\State $D_k \gets \begin{bmatrix} D_k\\c^\top B_k	\end{bmatrix}$
		\State $C_k \gets \begin{bmatrix} C_k\\c^\top A_k	\end{bmatrix}$
		\State $U,\sigma,V^\top \gets$ SVD$\left(\begin{bmatrix} A_{k+1}\\C_{k+1}	\end{bmatrix}\right)$ 
		\If{$\sigma[-1] < \text{tol}$}
		\State $U \gets U[:,:-2]$
		\State $\sigma \gets \sigma[:-2]$
		\State $V \gets V[:-2,:]$
		\State$\begin{bmatrix} A_{k+1}\\C_{k+1}	\end{bmatrix} \gets U\diag(\sigma)$
		\State$A_{k} \gets V^\top A_{k}$
		\State$C_{k} \gets V^\top C_{k}$
		\EndIf
	\end{algorithmic}
	\caption{Algorithm to move a boundary between $y_k$ and $y_{k+1}$ down}\label{alg:move_down}
\end{algorithm}

\paragraph{Move right}
It is also possible to reverse these changes.
When moving the boundary right the input $x_\m$ is removed form the input $u_{k+1}$ and attached to the input vector $u_k$.
For this the first row of $D_{k+1}$ is removed and stored in the vector $d$. Analogously the first row of $B_{k+1}$ is removed and stored in the vector $b$.
The input $u_\m$ is attached to the end of the state vector $x_{k+1}$.
The vector $b$ is attached to the end of $A_{k+1}$ and $d$ is attached to $C_{k+1}$.
This is illustrated in Figure\,\ref{fig:move_left}.
This trivial algorithm results in a new Observability matrix $\tilde{\Ob}_{k+1}$ that has one additional column.
\begin{equation}
	\tilde{\Ob}_{k+1}
	=
	\begin{bmatrix}
	\eye & 0\\ 0 &\Ob_{k+2}
	\end{bmatrix}
	\begin{bmatrix}
	C_{k+1} & d\\
	A_{k+1} & b
	\end{bmatrix}
\end{equation}
If the last row is not linearly independent, the system is no longer observable as $\Ob_{k+1}$ does not have full column rank.
This can be checked using
\begin{equation}
	\begin{bmatrix}
	d\\b
	\end{bmatrix}
	\in
	\range\left(
	\begin{bmatrix}
	C_{k+1}\\A_{k+1}
	\end{bmatrix}\right).
\end{equation}
If the vector is in the range then there exits a state $m$ with the property that
\begin{equation}
	\begin{bmatrix}
	d\\b
	\end{bmatrix}
	=
	\begin{bmatrix}
	C_{k+1}\\A_{k+1}
	\end{bmatrix}
	m
\end{equation}
This also means that there is no new state dimension required.
In this case the vector $m$ is appended to the matrix $B_{k}$.
The algorithm uses the SVD of $[C_{k+1}^\top,A_{k+1}^\top]^\top$ to check the relations.
This uses the intermediate vector 
\begin{equation}
	a = U^\top 	
	\begin{bmatrix}
	d\\b
	\end{bmatrix}
\end{equation} 
If the vector $[d^\top,b^\top]^\top$ is in the range, the norm of 
\begin{equation}
	\|a_{[r+1:]} \| = 0
\end{equation}
Here $r$ is the number of nonzero singular values.
The vector $m$ is computed with 
\begin{equation}
	m = V[:,:r] \diag(\sigma[:r])^{-1} a[:r]
\end{equation}
This is equivalent to calculating the vector $m$ using the pseudoniverse.  
Additionally the vector $d_\text{new}$ is attached $D_k$.
\begin{figure}[htb]
	\centering
	\input{diagrams/Moves/Move_right.pgf}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{Original system}
		\label{fig:move_right_a}
	\end{subfigure}
	\hspace{0.8cm}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{Moved system}
		\label{fig:move_right_b}
	\end{subfigure}
	\caption{Illustration of the movement of boundary}
	\label{fig:move_right}
\end{figure}

\begin{algorithm}[htb]
	\begin{algorithmic}
%		def move_right(sys,v,d_new=None):
%		#function to move a boundary left 
%		#v is the index to the left of the boundary

		\State $b \gets B_{k+1}[:,1]$
		\State $d \gets D_{k+1}[:,1]$
		\State $B_{k+1} \gets B_{k+1}[:,2:]$
		\State $D_{k+1} \gets D_{k+1}[:,2:]$
		
%		#check if [d;b] in range(C_v+1;Av+1)
		\State $U,\sigma,V^\top \gets$ SVD$\left(\begin{bmatrix}
		C_{k+1}\\A_{k+1}
		\end{bmatrix}\right)$ Full matrices 

		\State $r \gets $ count$(\sigma>\text{tol}_i)$
		\State $a \gets U^\top \begin{bmatrix}
		d\\b
		\end{bmatrix}$
		%\If{any $|a[r+1:]|>\text{tol}$}
		\If{$\|a[r+1:]\|>\text{tol}$}
%		#not in range -> add a ned dimention to the state
		\State $B_k \gets \begin{bmatrix}
		B_k & 0\\
		0 & 1
		\end{bmatrix}$
		\State $A_k \gets \begin{bmatrix}
		A_k \\ 0
		\end{bmatrix}$
		\State $A_{k+1} \gets [A_{k+1},b]$
		\State $C_{k+1} \gets [C_{k+1},d]$		
		\Else
%		#in range -> no need for an additional dim
		\State $m \gets V[:,:r] \diag(\sigma[:r])^{-1} a[:r]$
		\State $B_k \gets \begin{bmatrix} B & m\end{bmatrix}$
%		
		\EndIf
%		#set the appropirate D_v
		\State $D_v \gets \begin{bmatrix}
		D_k&d_{\text{new}}
		\end{bmatrix}$
	\end{algorithmic}
	\caption{Algorithm to move a boundary between $u_k$ and $u_{k+1}$ to the right}\label{alg:move_right}
\end{algorithm}

\paragraph{Move up}
An similar algorithm, makes it possible to move up a boundary.
When moving the boundary up the output $y_\m$ is removed form the output $y_k$ and attached to the output vector $y_{k+1}$.
First the last column of $D_{k}$ is removed and stored in $d^\top$ and analogously the last column of $C_k$ is removed and stored in $c^\top$.
The output $y_\m$ is appended to the state $x_{k+1}$.
The vectors $c^\top$ is appended to the matrix $A_k$ and the vector $d^\top$ is attached to $B_k$.
This adds a new column to reachability matrix.
\begin{equation}
\tilde{\R}_{k+1}
=
\begin{bmatrix}
A_{k} & B_{k}\\
c^\top & d^\top
\end{bmatrix}
\begin{bmatrix}
\R_{k} &0\\
0& \eye
\end{bmatrix}
\end{equation}
If this new column is part of the co-range of $\R_{k+1}$, then the system is no longer reachable.
In this case no additional state is needed. 
Using a similar derivation as used for a movement to the right...


\begin{figure}[htb]
	\centering
	\input{diagrams/Moves/Move_up.pgf}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{Original system}
		\label{fig:move_up_a}
	\end{subfigure}
	\hspace{0.8cm}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{Moved system}
		\label{fig:move_up_b}
	\end{subfigure}
	\caption{Illustration of the movement of boundary}
	\label{fig:move_up}
\end{figure}

\begin{algorithm}[htb]
	\begin{algorithmic}
		%		def move_right(sys,v,d_new=None):
		%		#function to move a boundary left 
		%		#v is the index to the left of the boundary

		\State $c^\top \gets C_{k}[-1,:]$
		\State $d^\top \gets D_{k}[-1,:]$
		\State $C_{k} \gets C_{k}[:-2,:]$
		\State $D_{k} \gets D_{k}[:-2,:]$
		\State $U,\sigma,V^\top \gets$ SVD$\left(\begin{bmatrix}
		A_{k+1}^\top\\B_{k+1}^\top
		\end{bmatrix}\right)$ Full matrices 
		\State $r \gets $ count$(\sigma>\text{tol}_i)$
%#check if [d;c]^T in range(B_v;A_v)^T
%U,s,Vt= np.linalg.svd(np.vstack((sys.stages[v].B_matrix.T,sys.stages[v].A_matrix.T)),full_matrices=True)
%eps = 1e-13
%r = np.count_nonzero(s>=eps)
\State $a \gets U^\top \begin{bmatrix}
c\\d
\end{bmatrix}$
%a = U.T@np.vstack((d.T,c.T))
%if np.linalg.norm(a[r:])>eps:
\If{$\|a[r+1:]\|>\text{tol}$}
%		#not in range -> add a ned dimention to the state
\State $A_k \gets \begin{bmatrix}
A_k \\ c^\top
\end{bmatrix}$
\State $B_k \gets \begin{bmatrix}
B_k \\ d^\top
\end{bmatrix}$
\State $C_{k+1} \gets \begin{bmatrix}
0 & 1\\ C_{k+1} & 0
\end{bmatrix}$
\State $A_{k+1} \gets \begin{bmatrix}
A_{k+1} & 0
\end{bmatrix}$
%sys.stages[v+1].C_matrix = np.block([
%[np.zeros((1, sys.stages[v+1].C_matrix.shape[1])), 1     ],
%[sys.stages[v+1].C_matrix,np.zeros((sys.stages[v+1].C_matrix.shape[0],1))]
%])
%sys.stages[v+1].A_matrix = np.hstack((sys.stages[v+1].A_matrix,np.zeros((sys.stages[v+1].A_matrix.shape[0],1))))
%sys.stages[v].A_matrix = np.vstack((sys.stages[v].A_matrix,c))
%sys.stages[v].B_matrix = np.vstack((sys.stages[v].B_matrix,d))

%else:
\Else
%		#in range -> no need for an additional dim
\State $m \gets V[:,:r] \diag(\sigma[:r])^{-1} a[:r]$
\State $C_{k+1} \gets \begin{bmatrix} m\\C_{k+1} \end{bmatrix}$
%#in range -> no need for an additional dim
%m = Vt.T[:,:r]@(a[:r].flatten()/s[:r])
%sys.stages[v+1].C_matrix = np.vstack((m.reshape((1,-1)),sys.stages[v+1].C_matrix))
\EndIf
%#set the appropirate D_v
%sys.stages[v+1].D_matrix=np.vstack((d_new,sys.stages[v+1].D_matrix))
\State $D_v \gets \begin{bmatrix}
d_{\text{new}} \\D_k
\end{bmatrix}$
	\end{algorithmic}
	\caption{Algorithm to move a boundary between $y_k$ and $y_{k+1}$ up}\label{alg:move_up}
\end{algorithm}

\subsection{Combine and Split stages}
It is also possible to split and combine stages. In \cite{chandrasekaran_fast_2005} an similar algorithm is given.
\todo[inline]{The same algorithm is given in the other paper, but we could give a more detailed explaination on the combination.
Also I think the derivation of the splitting is clearer than the derivation given in the paper}

\subsection{Moving Boundaries and calc sigmas}\label{subsec:move_sig}

\section{Permutations}
if time allows add some notes on permutations here



\chapter{Experiments}



\section{Matrix Approximation}
\begin{itemize}
\item Test different approaches to represent matrices.
\item Have a look at the representation error in different norms for different algorithms
\end{itemize}
\section{Weight Matrix Approximation}
%\section{Setups}
eventually split this up in subsections

Setup: 
\begin{itemize}
	\item Some general notes on the tests.
	\item How define loss, images to features etc. 
	\item How measure accuracy
\end{itemize}

Tests:
\begin{itemize}
\item Test the different approaces in neural nets.
\end{itemize}
\chapter{Discussion}
4-5 4pages

\chapter{Conclusion}
1-2 pages



\the\textwidth



% Puts out the list of references
\printbibliography{}

\end{document}
