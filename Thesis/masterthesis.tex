% Possible types of documents/theses
%   doctype=bachelorsthesis
%   doctype=mastersthesis
%   doctype=idp
%   doctype=phdthesis
%   doctype=studienarbeit
%   doctype=diplomarbeit
%
% Document language
%   without 'lang' attribute: English
%   lang=ngerman:             German (new orthography)
%
% Binding correction
%   BCOR=<Längenangabe>
%   Additional margin, which is invisible due to binding the book
%   The usual binding by the Fachschaft has a thickness of 1,5 cm 
%
% biblatex (citations)
%   This requires 'biber' to be used instead of 'bibtex', please
%   adapt your editor's settings accordingly!
\documentclass[doctype=mastersthesis,BCOR=15mm,biblatex]{ldvbook}%lang=ngerman

% !TeX spellcheck = en_US

\usepackage{placeins}

\usepackage{pgfplots}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{matrix}
\usetikzlibrary{ decorations.markings}
\usetikzlibrary {decorations.shapes}

\input{diagrams/systems.tex}

\usepackage[boxed]{algorithm}
\usepackage{algpseudocode}
\newcommand{\Input}{\Require}
\newcommand{\Output}{\Ensure}
\renewcommand{\algorithmicrequire}{\textbf{Input:\phantom{\textbf{Output}}\hspace{-0.75cm}}}
\renewcommand{\algorithmicensure}{\textbf{Output:\phantom{\textbf{Input}}\hspace{-0.75cm}}}
\newcommand{\spaceIO}{\phantom{Output:Input}\hspace{-0.7cm}}

\usepackage{todonotes}
\usepackage{subcaption}
\usepackage{mathdots}

% Look for citation sources in the database "diplomarbeit.bib"
\addbibresource{thesis.bib}


%operator declarations
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\triu}{triu}
\DeclareMathOperator{\tril}{tril}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\range}{range}
\DeclareMathOperator{\argmin}{argmin}


%define symbols
\newcommand{\R}{\mathcal{R}} %Reachabilityy matrix
\newcommand{\Ob}{\mathcal{O}} %Observability operator 
\newcommand{\eye}{I} %identity matrix
\newcommand{\bigO}{O}

\newcommand{\sys}{\Sigma}

\newcommand{\lsearchud}{l^\text{ud}}
\newcommand{\lsearchrl}{l^\text{rl}}


%some shortcuts for things that might change
\newcommand{\m}{\triangledown} %indexing for moved inputs/outouta
\newcommand{\da}{d^*} %state dims for anticausal state


\begin{document}

% Bibliographic information about the thesis, please change accordingly!
\title{Combining Matrix Representations for Structured Approximation of Neural Network Weight Matrices}
\author{Stephan Nüßlein}
\license{CC-BY}
\supervisor{Matthias Kissel}


\maketitle[frontcover=Design1]


\chapter*{Abstract}

\begin{itemize}
	\item neural nets
	\item reduce cost of neural nets using structured matrix
	\item how to use Sequentially semisperable matrices in this context?
	\item choose and tuning segmentation
	\item some sentence on results
\end{itemize}


\tableofcontents

60-80 pages total

% Please compile this example document including the bibliography
% database. Check the resulting document and the references for 
% correct appearance (especially the German Umlaute).
% Thus you ensure that LaTeX is detecting the character encoding
% correctly and your build chain is working.
% If it does not, please tell your supervisor.



\todo[inline]{Make the usage of the state transforms consistent}
\todo[inline]{Definition of SVD, maybe state reduced svd directly in algorithm to avoid this counting and cutting}
\todo[inline]{How to denote matrices and $d$ for anticausal systems}


\chapter{Introduction} 3p
\todo{Only draft so far}
Motivation Why

What others are dooing

What has been done here

increasing cost

Order of matrix vector product, makes larger systems expensive

\todo{why ML interesting}

As the matrices for fully connected layers get larger the evaluation of the neural nets get computationally more expensive. This is often problematic as the computational resources are a limiting factor, especially on embedded or mobile systems.


The linear layers in neural nets can be represented with matrices.
Usually full matrices are used.
When evaluation the neural nets one has to compute the matrix vector product.
For a full unstructured matrix this is of order $O(n^2)$. 
Whereas the cost for calculating the nonlinear part is usually linear in the number of neurons the cost for the calculation of the liner layers is quadratic in the number of neurons.
This means that a larger number of neurons the cost of the linear layer inccears 
\todo{reword}


There are also matrix structures where the cost of the matrix vector product is of lower order.
These have been developed to help solving PDEs like $\mathcal{H}$-Matrices, arise in different circumstances like semiseperabel matrices, %Vander Bib

or are description of time varying systems like sequentially semiseperable matrices.
All of them can not only be represented as a matrix but also have a different representation.
It is possible to use these representations to construct algorithms that can compute the matrix vector product.
One example of this are circulant matrices. 
These describe time invariant systems and therefore convolutions \todo{check and reword}
Using the DFT matrices, circulant matrices can be transformed into diagonal matrices.
This allows us to obtain a faster algorithm for the product $y=Tu$ by suing the Fast Fourier Trandform(FFT):
We can first compute the FFT of the input, then multiply the vector element-wise with a weight vector. Finally we use the inverse FFT to obtian the result $y$.
Instead of of the order $\bigO(n^2)$ we now have $\bigO(n\log(n))$.
This result is used in widely used in Convolutional Neural Networks (CNN).
Instead of using a matrix vector products the underlying structure is utilized and convolutions are used.

One might analogously try to use other structures to represent weight matrices:
A possible representation are sequentially semiseperable matrices. 
These matrices represent the input output behaviour of
Time varing systems.
These systems consist of  subsystems, so called stages that can change over time.
%Every stage can be described using four small matrices.\todo{reword}
Therefore we can also use this state space representation of the system to describe the matrix.
It is also possible to to calculate the matrix-vector product using algorithms based on this representation.
%The number of inputs and outputs of a system also create a segmentation of the system.
The definition of the Sequentially semiseperable matrix also results in the fact that these matrices have a underlying segmentation.
This segmentation is often determined by a known system ot an underlying physical system and therefore known. 
The matrices in neural networks do not necessarily have the same underlying structure.
Even if the matrix is close to a sequentially semiseperable matrix the structural parameters are usually not know.
To represent the matrices, one need algorithms to derive these structural parameters.
%This can be split in different sub problems.
%Firs the number of stages is not known.
There are existing algorithms to calculate the state space representation for a known segmentation.
After guessing the structural parameters we use these algorithms to create a system.
After this we refine the parameters by an algorithm that can change the structural parameters of a system.
As the weight matrices usually are not sequentuilly semiseperable we also need to approximate the system, if we want to reduce the computational cost.
This will also be incorporated in the algorithm.

In section different approaches to use structured matrices in neural nets are collected.
In section related matrix structures will be presented.
After this the Sequentually semiseperable matrices that will be used in this thesis will be explained in more detail in chapter
In chapter <methods> the methods to create a state space representation for a matrix with unknown structural parameters will be presented.
%the sequentially semeiseperabel structure will be used to approximate matrices. 
%For these matrices it the structural parameters are not known a prior.
In section <approx> an algorithm for the approximation of a system with a different system is explored.
%In section we give an algorithm, for the approximation of systems.
Based on this choose the structural parameters are chosen.
In section <> an approach to determine the number of stages for a system will be presented
We still have to determine the input and output dimensions.
%An Algorithm to adapt the input and output dimentiosn of a system will be explained.
This is done by starting with an initial guess and then refine the structure by adapt the input and output dimensions.
In section <> an algorithm to change these dimensions is given.
Additionally this section presents strategies how to use these algorithms in cases where the final structure has to be determined.
In chapter the algorithms will be tested.
Section contains the results for test cases and in section the algorithm are used to approxiamte weight matrices.

´

\chapter{Literature Review}
\todo[inline]{Some text}

\section{Neural Nets}
%Special matrix structures are been explored for the use in neural nets.
%Here different matrices structures are used to improve the performance. 
In this section different approaches that use structured matrices in neural nets are shortly introduced.
There are two main approaches to obtain structured matrices:
In one case the structure is predetermined and the parameters of the structure are trained by a regular training process \cite{fan_multiscale_2019,dao_kaleidoscope_2020,li_butterfly_2015,ailon_sparse_2021,ioannou_training_2016}.
Or the neural net is trained without a predetermined matrix structures and the matrices are later approximated with structured matrices \cite{wu_hybrid_2020,hassibi_optimal_1993,jaderberg_speeding_2014,rigamonti_learning_2013}. In \cite{yu_compressing_2017} the model was retrained after the approximation. 
There are also approaches where the approximation is done during the training procedure like \cite{dettmers_sparse_2019} or an structure is created using regularization \cite{louizos_learning_2018,wen_learning_2016}.


One widely known approach to reduce the complexity of neural nets is pruning. 
These are processes that set elements in neural net matrices to zero to reduce the computational cost \cite{blalock_what_2020}.
This is equivalent to removing connections between neurons.
An approach to remove connections from a trained net based on the Hessian of the loss function was proposed by Hassibi \cite{hassibi_optimal_1993}.
It is also possible to obtain sparse matrices while training.
Dettmers and Zettlemoyer use an algorithm that includes pruning steps during the training after every epoch \cite{dettmers_sparse_2019}.
A different approach to obtain sparse matrices is including a regularization term.
As an example Louizos et al. use $L_0$ regularization  \cite{louizos_learning_2018} and Wen et al. use LASSO regularization \cite{wen_learning_2016}.

Low Rank approximations for filters in convolutional layers were proposed by Rigamonti et al. \cite{rigamonti_learning_2013}. These use combinations of separable filters. Jaderberg et al. approximate existing filters using low rank filters \cite{jaderberg_speeding_2014}. 
Ioannou et al. directly learn low rank filters and combine them later \cite{ioannou_training_2016}.
In low rank and sparse decomposition a matrix is approximated with a low rank and a sparse matrix. 
Yu et al. represented matrices in deep models as a sum of low rank and sparse matrices \cite{yu_compressing_2017}.
The computation of the decomposition is based on an algorithm to calculate a low rank and sparse decomposition by Zhou and Tao \cite{zhou_greedy_2013}.
To avoid an accumulation of the error due to the compression of multiple layers the decomposition are not done independently.
This is archived by defining the objective function as the difference between the outputs of the approximation for a collection of approximated input vectors and the corresponding output vectors.

The structure of Hirarchical matrices will be explained in subsection\,\ref{subsec:H-mat}. These matrices have also been used in machine learning. 
Fan et al. used the structure of $\mathcal{H}$-Matrices in neural nets to solve PDEs \cite{fan_multiscale_2019}. There the structure of the neural net makes use of the different scales in the system.
Hierarchical tensor decomposition were used by Wu et al. to represent weight matrices and convolutional kernels \cite{wu_hybrid_2020}.
Ithapu used hierarchical factorization of covariance matrices to explore relationships between classes \cite{ithapu_decoding_2017}.

A different matrix structure explored in neural nets are butterfly matrices.
These are products of sparse matrices that overall have a similar structure to the fast Fourier transform \cite{li_butterfly_2015,parker_random_1995}. Ailon et al. used this matrix structure to represent dense layers \cite{ailon_sparse_2021}.
The butterfly structure was combined with CNNs by Li et al. \cite{li_butterfly-net_2020}. This structure can be used to efficiently represent Fourier kernels.
An extension of butterfly matrices are Kaleidoscope matrices (K-matrices). These were introduced by Dao et al. in \cite{dao_kaleidoscope_2020}.
K-Matrices are product of factors of the shape $B_aB_b^\top$, where $B_a$ and $B_b$ are butterfly matrices. These can be used to represent a wide class of structured matrices.
The patterns in the factors are fixed. Therefore the parameters can be learned using gradient descent.
K-matrices can also be used to represent linear hand crafted structures like reprocessing steps. %example fft 

\todo[inline]{
Properties of dense matrices
Approxiamtions of dense Layer matricies}



\section{Matrix structures}
\todo[inline]{eventually move this section to Background}

\subsection{Matrix structures}
In the following certain matrix structures are presented.
These have in common that the rank of sub-matrices are important.
Here semiseperable, Hierarchical and Sequentially semiseparable matrices are presented.

\subsection{Semiseperable matricies}
Semiseperable matrices are not consistently defined in the literature. 
In this thesis the definitions described by Vandebril \cite{vandebril_bibliography_2005,vandebril_matrix_2007} is used.
An important differentiation are generator representable semiseparable matrices and semiseparable matries.
\paragraph{Generator Representable Semiseparable Matrix}
A matrix $S$ is a generator representable semiseparable matrix if the lower and upper triangular parts of $S$ are taken from rank 1 matrices.
This can be expressed as 
\begin{align}
	\triu(S) &= \triu(pq^\top)\\
	\tril(S) &= \tril(uv^\top)
\end{align}
Where $\triu$ is the upper triangular matrix and $\tril$ is the lower triangular matrix. The vectors $p,q,u$ and $v$ are called the generators.
It is important to note here that the diagonal of $S$ is both included in the lower and upper triangular matrix.

\paragraph{Semiseparable Matrix}
In a semiseperable matrix every subblock selected from the lower triangular part of $S$ have rank 1. The analogous statement has to be fulfilled for the upper triangular part.
This can be formalized as 
\begin{align}
	\rank(S_{i:n,1:i}) &\leq 1 & \forall_i &\in\{i,\dots,n\}\\
	\rank(S_{1:i,i:n}) &\leq 1 & \forall_i &\in\{i,\dots,n\}
\end{align}
A extension of this matrix class are the semiseparable plus diagonal matrices.

\paragraph{Quasiseparable Matrix}
The quasiseparable matrices are similar to the semiseperable matries. In a quasiseparable matrix every subblock selected from the strictly lower strictly upper triangular part of $S$ have rank 1. 
This can be formalized as 
\begin{align}
\rank(S_{i+1:n,1:i}) &\leq 1 & \forall_i &\in\{i,\dots,n\}\\
\rank(S_{1:i,i+1:n}) &\leq 1 & \forall_i &\in\{i,\dots,n\}
\end{align}

A quasiseperable matrix is illustrated in Figure\,\ref{fig:quasiseperable}. All the marked submatrices have the property that their rank is 1.
\begin{figure}
	\centering
	\input{diagrams/semiseperable}
	\caption{Illustration of a quasiseperable matrix}
	\label{fig:quasiseperable}
\end{figure}
As the quasiseperable structure does not impose conditions on the diagonal it is more general as the semiseperable matrices 

There is an relation between invertible semiseperable matrices and invertible tridiagonal matrices.
The inverses of a generator representable semiseparable matrix is a irreducible tridiagonal matrix and vice versa. 
The inverse of a semiseparable matrix is a tridiagonal matrix and vice versa.
If a invertible quasiseparable matrix is inverted, the inverse is again a quasiseparable matrix.

These matrix classes can also be extended for higher ranks.
A matrix $S$ is a generator representable semiseparable matrix of semiseparability rank r if there exist the matrices $R_1$ and $R_2$ with $\rank(R_1)=r$ and $\rank(R_2)=r$ such that
\begin{align}
\triu(S) &= \triu(R_1)\\
\tril(S) &= \tril(R_2)
\end{align}

A similar definition for semiseparable matrices of semiseparability rank $r$ is given in \cite{vandebril_bibliography_2005}.
%For this class of matrices and some slight alterations there are algorithms for the efficient calculation of different operations.

\subsection{Hirarchical Matrices}\label{subsec:H-mat}
The Hirarchical matrices ($\mathcal{H}$-Matrices) are a matrix structure to approximate large matrices. These were mainly introduced by Hackbusch \cite{hackbusch_hierarchische_2009} and Grasedyck \cite{grasedyck_theorie_2001}.
The $\mathcal{H}$-Matrices where developed for the solution of PDEs.
If an PDEs is solved numerically, it has to be discreticed in order to obtain an approximated solution.
As the disscretization already introduces errors, it is advantageous to drop the requirement that the matrix representation is exact, if this results in a reduction of the computational cost.
This can be done by partitioning the matrix in segments. These blocks are represented by low rank matrices. It the rank is far smaller than the size of the matrix this representation is cheaper in terms of storage and in terms of computational cost.
The partitioning is done by hierarchically dividing blocks that cannot be represented using a low rank representation.
To decide if a block has to be divided an admissabillity condition is introduced.
This is a way to predict the representability of a matrix block.
For discretizations of PDEs this admissibility condition is usually based on the geometrical distance.
For other applications different admissibility conditions have to be derived.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\input{diagrams/H-matrix.pgf}
		\caption{Matrix Segmentation}
		\label{fig:strukturh-matrix_a}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
	\centering
	\resizebox{0.7\textwidth}{!}{
	\input{diagrams/H-tree.pgf}
	}
	\caption{Block Bluster Tree}
	\label{fig:strukturh-matrix_b}
	\end{subfigure}
	\caption{Illustration of the structure of a $H$-Matrix}
	\label{fig:strukturh-matrix}
\end{figure}


An $\mathcal{H}$-Matrix is shown in Figure\,\ref{fig:strukturh-matrix}a. 
In this case the matrix is divided in four blocks.
If a block is admissible, it is stored as a low rank representations. These are illustrated as the white rectangles.
If a block is not admissible, the block is divided in smaller subblocks.
If matrices are already small and still not admissible the matrices are stored directly. In Figure\,\ref{fig:strukturh-matrix}a these blocks are colored red.
As the partition is done in a hierarchical fashion the matrices can be represented in a block-tree. 
The bock-tree for the matrix in Figure\,\ref{fig:strukturh-matrix_a} is illustrated in Figure\,\ref{fig:strukturh-matrix_b}.
The Hierarchical matrices make it possible to compute different matrix operations efficiently. %\todo[inline]{some detailed statement on complexity}
%The need for efficient operations also constraints the possible sizes of the blocks as these should be compatible.

\subsection{Sequentially semiseperable}
%\todo[inline]{Quellen, also which notation, note that book is on the transposed...}
Sequentially semiseperable matrices were introduced in \cite{dewilde_time-varying_1998}.
These matrices can represent time varying systems.
As the Sequentially semiseperable maticies will be used in this thesis a introduction will be given in chapter\,\ref{chap:background}. 
%Here the commonalities and differences to the semiseperable and hirarchical matrices are explored.

%The sequentially semiseperable matrices have the properties that submatrices have a low rank.
%This is similar to semiseperable matrices. 
%Unlike the semiseperable matrices this is not true for all matrices taken form the lower and upper triangular part.
The sequentially semiseperable matrices are divided into blocks.
Matrices taken form the strict lower and strict upper triangular blockmatrix have the condition that their rank is low.
This is illustrated in Figure\,\ref{fig:sequentiallysep}.

\begin{figure}[htb]
	\centering
	\input{diagrams/sequentially_sep}
	\caption{Illustration of a sequentially semiseperable matrix. The thick dotted lines mark the segemntation of the matrix. The colored lines represent submatrices with low rank.}
	\label{fig:sequentiallysep}
\end{figure}

This segmentation in blocks makes similar to the hirarchical Matrices that also have a segmentation.
Unlike semiseperable matrices the sequentially semiseperable matrices do not have to be quadratic. 

\chapter{Background}\label{chap:background}


Sequentially semiseperable Matrices can be considered as descriptions of time varying systems
\footnote{
The naming and structure of the matrices is not consistent in the literature.
Here we will use the notation used in \cite{tong_blind_2003}. 
In \cite{dewilde_time-varying_1998} a similar notation is used, but the transfer operator is transposed.
In other works like \cite{rice_efficient_2010,chandrasekaran_fast_2002} the causal and anticausal parts are considered jointly
}
.
These systems can be described using the formulas
\begin{subequations}
	\begin{align}
	x_{k+1} &= A_k x_k + B_k u_k \\
	y_k &= C_k x_k + D_k u_k 
	.
	\end{align}
	\label{eq:def_causal}
\end{subequations}
The $u_k$ are the inputs, the $y_k$ are the outputs and the $x_k$ are the states.
The matrices $A_k$, $B_k$, $C_k$ and $D_k$ can vary with the time index $k$.
This is different from time-invariant systems, where the matrices $A$, $B$, $C$ and $D$ are constant.
This also means that the dimensions of the input, the output and the states can vary with time.
The structure of a system is represented in Figure\,\ref{fig:struktur-system_a}.
\begin{figure}[htb]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		%\resizebox{0.7\textwidth}{!}{
		\input{diagrams/Example_system.pgf}
		%}
		\caption{Causal system}
		\label{fig:struktur-system_a}
	\end{subfigure}
	\hspace{0.8cm}
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		%\resizebox{0.7\textwidth}{!}{
		\input{diagrams/Example_system_anti.pgf}
		%}
		\caption{anticausal system}
		\label{fig:struktur-system_b}
	\end{subfigure}
	\caption{Illustration of the structure of time varying systems}
	\label{fig:struktur-system}
\end{figure}
We can see that the system is structured in stages.
The stage $k$ consists of the matrices $A_k$, $B_k$, $C_k$ and $D_k$.
The matrix $A_k$ maps the old state $x_k$ to the next state $x_{k+1}$, the matrix $B_k$ maps the current input $u_k$ to the next state $x_{k+1}$.
The matrix $C_k$ maps the old state $x_k$ to the output $y_k$.
Finally the matrix $D_k$ directly maps the input $u_k$ to the output $y_k$.
The system defined in Equation\,\ref{eq:def_causal} is a causal system.
This means that the output $y_k$ only depends on the current input $u_k$ and the previous inputs $u_l$ with $l < k$.
It is also possible to define an anticausal system where the output only depends on the current and the later inputs.
An anticausal system is described using the formula
\begin{subequations}
	\begin{align}
	x_{k-1} &= A_k x_k + B_k u_k \\
	y_k &= C_k x_k + D_k u_k 
	.
	\end{align}
	\label{eq:def_anticausal}
\end{subequations}
In Figure\,\ref{fig:struktur-system_b} an anticausal system is illustrated.
The structure is analogous except for the reversed direction of the arrows for the states.

When we stack the input vectors to a one vector $u = [u_1^\top, \dots ,u_k^\top]^\top$ and stack the outputs vectors to a one output vector $y = [y_1^\top, \dots ,y_k^\top]^\top$ we can describe the input-output behaviour of the system using a single matrix vector product 
\begin{equation}\label{eq:System_mat_vec}
	y = Tu,
\end{equation}
where the matrix $T$ is called the transfer operator.
The transfor operator $T$ for a causal system with four stages is
\begin{equation}\label{eq:T_causal}
T_\text{causal}=
\begin{bmatrix}D_{1} & 0 & 0 & 0\\C_{2} B_{1} & D_{2} & 0 & 0\\
C_{3} A_{2} B_{1} & C_{3} B_{2} & D_{3} & 0\\
C_{4} A_{3} A_{2} B_{1} & C_{4} A_{3} B_{2} & C_{4} B_{3} & D_{4}\end{bmatrix}
.
\end{equation}
For an anticausal system the matrix description is 
\begin{equation}
T_{\text{anticausal}}=
\begin{bmatrix}D_{1} & C_{1} B_{2} & C_{1} A_{2} B_{3} & C_{1} A_{2} A_{3} B_{4}\\
0 & D_{2} & C_{2} B_{3} & C_{2} A_{3} B_{4}\\
0 & 0 & D_{3} & C_{3} B_{4}\\
0 & 0 & 0 & D_{4}\end{bmatrix}.
\end{equation}
The matrices for a higher number of stages are analogous.
We can see that the causal system results in a lower triangular blockmatrix whereas the anticausal system results in a upper triangular blockmatrix.
If we want to represent the whole Matrix we can use mixed systems.
These can be defined as the sum of a causal and an anticausal system.
These usually have the same input and output dimensions, while the state dimensions can de different for the causal and anticausal part.
When adding the systems,  the $D$ matrices have to be added.
As the use of two seperate $D$ does usually not have any benefits, the $D$ matrices for the anticausal system are usually set to 0 and therefore can be ignored.
When representing matrices the first state $x_1$ and the last stage $x_{n+1}$ are usually set to zero dimensions.

\paragraph{Hankel Operator and Minimality}
When we are interested in properties of the system, we can study the relation between the inputs, the states and the outputs. 
The reachability matrix $\R$ is the mapping from the inputs to the state. For a causal system the reachability matrix for state $x_k$ can be written as
\begin{equation}
	\R_k = \begin{bmatrix}
	 \dots & A_{k-1}A_{k-2}B_{k-1} &A_{k-1}B_{k-2} &B_{k-1}
	\end{bmatrix}
\end{equation}
The mapping form the state to the output is called the observability operator $\Ob$.
For a causal system the observability matrix for state $x_k$ can be written as
\begin{equation}
	\Ob_k = 
	\begin{bmatrix}
		C_k\\
		C_{k+1}A_k\\
		C_{k+2}A_{k+1}A_k\\
		\vdots
	\end{bmatrix}.
\end{equation}
When we multiply these we obtain the mapping from the inputs to the outputs. This is called the Hankel operator $H_k$.
For a causal system this can be written as
\begin{equation}\label{eq:H_def}
	H_k = \Ob_k \R_k = 
		\begin{bmatrix}
\dots   & C_{k}A_{k-1}B_{k-2} & C_{k}B_{k-1} \\
\dots   & C_{k+1}A_{k}A_{k-1}B_{k-2} & C_{k+1}A_{k}B_{k-1} \\
\iddots &\vdots &\vdots
\end{bmatrix}
%%3x3 Version
%		\begin{bmatrix}
%		\dots & C_{k}A_{k-1}A_{k-2}B_{k-3}  & C_{k}A_{k-1}B_{k-2} & C_{k}B_{k-1} \\
%		\dots & C_{k+1}A_{k}A_{k-1}A_{k-2}B_{k-3}  & C_{k+1}A_{k}A_{k-1}B_{k-2} & C_{k+1}A_{k}B_{k-1} \\
%		\dots & C_{k+2}A_{k+1}A_{k}A_{k-1}A_{k-2}B_{k-3}  & C_{k+2}A_{k+1}A_{k}A_{k-1}B_{k-2} & C_{k+2}A_{k+1}A_{k}B_{k-1} \\
%		\iddots & \vdots &\vdots &\vdots
%		\end{bmatrix}
\end{equation}
This is the same matrix strucure as we have seen in Equation\,\ref{eq:T_causal}.
\todo{reword}

If the rows of $\R_k$ are linearly independent, the range of $\R_k$ is $\mathbb{R}^d_k$, where $d_k$ is the number of dimensions of $x_k$.
This means that we can reach every state with some input.
Therefore we call a system reachable if every $R_k$ has a full row rank.
If the columns of $O_k$ are linearly independent, we can reconstruct the state $x_k$ from the output.  
Therefore we call a system observable if every $R_k$ has a full column rank.

If a system is both observable and reachable, it is called minimal.
In a minimal system the state dimension is cannot be further reduced without a loss of information.
The rank of the Hankel operator is called the Hankelrank. 
If the system is minimal we have 
\begin{equation}
	\rank(H_k) = rank(\Ob_k) = \rank(\R_k) = d_k
\end{equation}
for every $k$.
Usually the state dimensions are far smaller than the input and output dimensions.
This allows us to factor the $H_k$ in a tall and a flat matrix. 
This results in the factorization $H_k = \Ob_k\R_k$

\paragraph{State Transforms}
As this factorization is not unique, the state in also not uniquely defined. 
The state can be transformed with an non-singular state transform matrix $S$ according to $\tilde{x}_k =S_kx_k$.
This gives a transformed 
reachabilty matrix $\tilde{\R}_k=S_k \R_k$ and the transformed
observability matrix $\tilde{\Ob}_k= \Ob_k S_k^{-1}$. 
The Hankel matrix stays the same as $\tilde{H_k} = \Ob_k S_k^{-1} S_k \R_k= \Ob_k \R_k = H_k$.
If a matrix factorization is used to construct a state transformation, then the state transformation can be considered as moving a matrix form the reachability matrix to the observability matrix
\begin{equation}
	\Ob_{k}R_{k} = \underbrace{\Ob_{k}A}_{\tilde{\Ob}_k}\underbrace{B\vphantom{\Ob_k}}_{\tilde{\R}_k} = \tilde{\Ob}_k\tilde{\R}_k
\end{equation}
or moving a matrix form the observability matrix to the reachability matrix.
\begin{equation}
\Ob_{k}R_{k} = \underbrace{A\vphantom{\R_k}}_{\tilde{\Ob}_k}\underbrace{B\R_{k}}_{\tilde{\R}_k} = \tilde{\Ob}_k\tilde{\R}_k
\end{equation}
If the used algorithm is able to work directly on the state space model this also makes it possible to reduce the state dimension of nonminimal systems as the resulting state transformation matrix $S_k$ does not need to be nonsingular.

There are three particular types of factorization that are called canonical forms.
An input-normal realization has the property that the columns of each $\Ob_k$ are orthormormal. Whereas in an output-normal realization, all rows of each $\R_k$ are orthonormal.
The balanced realization results form the reduced Singular value decomposition $H_k = U_k \Sigma_k V_k^\top$. 
In a balanced realization $\Ob_k = U_k \Sigma^{1/2}_k$ and $\R_k = \Sigma^{1/2}_k V_k^\top$.
These canonical form usually also require that the system is minimal. In this thesis the minimality is usually omitted as the algorithms described also work on non-minimal systems and minimality is nontrivial to define for systems where the singular values decay slowly. If minimality is required, it will be explicitly stated.

\paragraph{Matrix Factorization}
The sequentially semisperabel matrices can also be considered as a matrix factorization.
For a causal system, the matrix $T$ can be expressed as a product $T = T_n T_{n-1} \dots T_2 T_1$
Where every $T_k$ represents a stage. The matrices $T_k$ is constructed according to 

\begin{equation}
\newcommand{\sddots}{\hspace{-6pt}\ddots}
\newcommand{\n}{\hspace{-7pt}}
	T_k =
	\begin{bmatrix}
	\begin{array}{c|cccccccc}
	A_k&    &   & & B_k & & &\\
	\hline
	   &\eye&   &&     & & &\\[-8pt]
	   & &\sddots&&    & & &\\[-6pt]
	   & & &\n\eye&      & & &\\
	C_k& & &    & D_k  & & &\\
	   & & &    &      &\eye& &\\[-8pt]
	   & & &    &      & &\sddots&\\[-6pt]
	   & & &    &      & & &\n\eye 
	\end{array}
	\end{bmatrix} 
\end{equation}

For a system with 3 stages this gives the factorization
\begin{equation*}
	T=
	\left[\begin{matrix}A_{3} & 0 & 0 & B_{3}\\0 & 1 & 0 & 0\\0 & 0 & 1 & 0\\C_{3} & 0 & 0 & D_{3}\end{matrix}\right]
\dots
%	\left[\begin{matrix}A_{2} & 0 & B_{2} & 0\\0 & 1 & 0 & 0\\C_{2} & 0 & D_{2} & 0\\0 & 0 & 0 & 1\end{matrix}\right]
	\left[\begin{matrix}A_{1} & B_{1} & 0 & 0\\C_{1} & D_{1} & 0 & 0\\0 & 0 & 1 & 0\\0 & 0 & 0 & 1\end{matrix}\right]
	=
	\left[\begin{matrix}A_{3} A_{2} A_{1} & A_{3} A_{2} B_{1} & A_{3} B_{2} & B_{3}\\C_{1} & D_{1} & 0 & 0\\C_{2} A_{1} & C_{2} B_{1} & D_{2} & 0\\C_{3} A_{2} A_{1} & C_{3} A_{2} B_{1} & C_{3} B_{2} & D_{3}\end{matrix}\right]
\end{equation*}

As $A_1$ has zero columns and $A_3$ has zero rows, the first block-column and the top block-row disappear and we get the familiars structure form Equation\,\ref{eq:T_causal}.

An analogous factorization is possible for anticausal systems.
Here the ordering is reversed an we have $T = T_1 T_2 \dots T_{n-1} T_{n}$

We can also represent mixed systems as factorization, if we use insert the factorizations for the causal part $T_{\text{causal}}$ and the anticausal parts $T_{\text{anticausal}}$ in the formula
\begin{equation}
	T = 
	\begin{bmatrix}
	\eye &
	\eye
	\end{bmatrix}
	\begin{bmatrix}
	T_{\text{causal}}&\\
	&T_{\text{anticausal}}
	\end{bmatrix}	
	\begin{bmatrix}
	\eye\\
	\eye
	\end{bmatrix}
\end{equation}
Sequentially semiseperable matrices can be added, multiplied and inverted using algorithm that work on the state space description.
It is also possible to calculate QR factorization \cite{chandrasekaran_fast_2002,tong_blind_2003} or URV factorization \cite{chandrasekaran_fast_2005}. 
%Note on URV more papers in Dewilde,Diepold and van der Veen p315 
There are also a couple of other algorithms that can be used for controller design like sign iterations \cite{rice_efficient_2010}.

\paragraph{Approxiamtions}
It is also possible, to reduce the number of states by approximating a system $T$ with a system $\tilde{T}$.
If the system $T$ is minimal, this is not trivial, as a reduction of the number of states also result in a reduction of the Hankelrank.
On approach is the Hankel-Norm approximation.
The Hankel-Norm is defined as
\begin{equation}
	\|T\|_H = \sup_{i}\|H_i\|.
\end{equation}
This norm is the supremeum over the spectral norm (the matrix 2-norm) of each individual Hankel matrix.
In \cite{dewilde_time-varying_1998} an algorithm is given for the hankel norm approxiamtion that computes an approximated $\tilde{T}$.  
This is based on the results form Adamjan, Arov, and Kreĭn \cite{adamjan_analytic_1971}.
The approximated system satisfies the condition
\begin{equation}
	\| \Gamma^{-1}(T-\tilde{T})\|_H \leq 1.
\end{equation}
Where $\Gamma$ is a diagonal and hermition operator. 
If we set $\Gamma = \eye\gamma$ we obtain the simplified condition
\begin{equation}
	\|T-\tilde{T}\|_H \leq \gamma.
\end{equation}
This problem has no unique solution, as the smaller singular values can be changed as long as they remain smaller as the supremum.

A second approach is balanced truncation. 
This is described in \cite{sandberg_balanced_2004,hinrichsen_improved_1990}. 
Here the state dimensions $d_k$ are reduced to a new dimension $\tilde{d}_k$.
The idea is to remove stated that result in a small output $y$ or need a big input $u$.
We can calculate the norm of the output for a state $x$ using
\begin{equation}
	\|y\|_2^2 = y^\top y = x^\top \Ob^\top \Ob x
	.
\end{equation}
The matrix $\Ob^\top \Ob$ is called the Observability gramian.
analogously we can calculate the norm of the input required to reach a certain state using
\begin{equation}
	\|u\|_2^2 = x^\top (\R\R^\top)^{-1} x
	.
\end{equation}
Where the Reachability gramian $\R\R^\top$ is invertible if the system is reachable.
The involved states can be changed using state transforms.
The idea is now to obtain a state transform, for which both problems are equivalent. 
This is the case for a balanced realization
\begin{equation}
	\Ob^\top \Ob = \Sigma^{1/2}U^\top U\Sigma^{1/2} = \Sigma = \Sigma^{1/2} V^\top V \Sigma^{1/2} = \R \R^\top 
\end{equation}
This results in a basis for the state with 
\begin{equation}
	\|y\|_2^2\big|_{x = e_l} = \|\Ob x\|_2^2\big|_{x = e_l} = \sigma_l
\end{equation}
and 
\begin{equation}
\|u\|_2^2\big|_{x = e_l} = \sigma_l^{-1}
\end{equation}
Where $e_l$ is the $l$-th standard basis vector.
This allows us to only keep the states with $\|y\| > \epsilon^{1/2} \|x\|$ and $\|x\| > \epsilon^{1/2} \|u\| $ for some $\epsilon>0$ by cutting all dimensions $l$ with $\sigma_l < \epsilon$.
%that are
% result in a large output and only need a small input to be reached. 

To obtain the reduced system $\tilde{T}$ the matrices of a balanced realization for every state are partitioned according to
\begin{align}
	A_k &=\begin{bmatrix}
	A_{k[11]} & A_{k[12]} \\
	A_{k[21]} & A_{k[22]} \\
	\end{bmatrix}
	&
	B_k &= \begin{bmatrix}
	B_{k[1]} \\ B_{k[2]}
	\end{bmatrix} 
	\\
	C &= \begin{bmatrix}
	C_{k[1]} & C_{k[2]}
	\end{bmatrix}& 
	D_k&=D_k.
\end{align}
The dimensions are determined by the new state dimensions.
The approximated system $\tilde{T}$ is then set to
\begin{align}
	\tilde{A}_k &= A_{k[11]}  & \tilde{B}_k &= B_{k[1]}\\
	\tilde{C}_k &=C_{k[1]}      & \tilde{D}_k &= D_k.
\end{align}
%This is equivalent to removing the tailing states.
%Because the realization is balanced every state in the original system corresponds to a $\sigma$ of the according Hankel matrix.
This approach is equivalent to removing all $\sigma_l<\epsilon$ of the SVD of the Hankel operator.
Therefore we can also construct an approximation when identifying a system using the SVD by truncating the SVD as described in \cite{shokoohi_identification_1987}.
\todo[inline]{Check proof in paper}
The error can be bound to two times the cut singular values \cite{lall_error-bounds_2003}.
\todo[inline]{TODO: understand the proof and conditions in the paper and summarize it}

A detailed discussion of approxiamtions is given in
\cite{antoulas_approximation_2005}.

\chapter{Methods} 15p

\section{Matrix Approximations}\label{sec:approx}
\subsection{Algorithm}
%\begin{itemize}
%\item Discuss ordering of the sigmas, and cutting states if the sigmas are ordered.
%\item Define ordered realization as extention of the ballanced realization
%\item Describe how to get the sigmas for a system by transforming it.General idea: We do not have to compute the SVD of the total Hankel matrix, if we take a input-nomal system and convert it step by step to a output-normal.
%\item Describe the overall approximation algorithm.
%\end{itemize}

\paragraph{Balanced truncation of ordered systems}
In Chapter\,\ref{chap:background} the balanced truncation is defined for balanced systems.
These have the property that $\Ob_k = U_k \Sigma_\Ob$ and $\R_k = \Sigma_\R V_k^\top$ where $\Sigma_\Ob = \Sigma_\R=\Sigma^{1/2}$.
The balanced truncation is also possible for a wider class of realizations.

In this thesis these Realizations will be called ordered.
\begin{definition}[Ordered Realization]
	A system is ordered if 
	\begin{subequations}
	\begin{align}
		\Ob_k^\top \Ob_k &= \Sigma_ {\Ob k}^2 \label{eq:orderd_obs}\\ %\diag(\sigma_{\Ob k}^2)\\
		\R_k \R_k^\top &= \Sigma_ {\R k}^2 \label{eq:orderd_reach}%\diag(\sigma_{\R k}^2)
	\end{align}
	\end{subequations}
	where $\Sigma_{\Ob k}$ and $\Sigma_{\R k}$ are diagonal matrices
	and the diagonal entries of $\Sigma_k =\Sigma_{\Ob k}\Sigma_{\R k}$ are non increasing (i.e. $\sigma_1 \leq \sigma_2 \leq \dots \leq \sigma_n$)
	%Positiov sowieso, weil gram matrix
    for every $k$.
\end{definition}

%As the SVD of the Hankel matrix does not change he have $\Sigma_\Ob\Sigma_\R=\Sigma$ as $H = \Ob\R = U_k \Sigma_\Ob \Sigma_\R V_k^\top = U_k \Sigma V_k^\top$.

An ordered system can be transformed into a balanced system using the diagonal state transformations $S_k = \Sigma_k^{1/2}\Sigma_{\R k}^{-1}$ and the inverse $S_k^{-1} = \Sigma_{\Ob k}^{-1}\Sigma_k^{1/2}$.
This gives the balanced system $\tilde{T}$
\begin{align}
	\tilde{A}_k &= S_{k+1} A_k S_k^{-1}   & \tilde{B} =& S_{k+1} B \\
	\tilde{C}_k &= C_k S_k^{-1}   & \tilde{D} =& D 
\end{align}
\begin{proof}
	By applying the state transform we can see that we get a balanced system\\
	$\Ob_k S_k^{-1} = U_k \Sigma_{\Ob k} \Sigma_{\Ob k}^{-1}\Sigma_k^{1/2} = U_k \Sigma_k^{1/2}$\\
	$S_k \R_k = \Sigma_k^{1/2}\Sigma_{\R k}^{-1} \Sigma_{\R k} V_k^\top = \Sigma_k^{1/2} V_k^\top$
\end{proof}

A ordered system can be reduced using the same segmentation as used for the balanced truncation.
\begin{proof}
	As the state transforms are diagonal matrices the result of first reducing the system and then applying the reduced state transformation is equivalent to applying the state transformation and later reducing the system.
\end{proof}
This allows us to use algorithms that require input normal or output normal forms, as systems can be both ordered and input normal $\Sigma_{\R k}=\eye$ or both ordered and output normal if $\Sigma_{\Ob k}=\eye$.

\paragraph{Recusrsve factorization of $\Ob_k$ and  $\R_k$}
The goal of this section is to construct an algorithm to calculate the singular values of the Hankel matrices without applying the SVD on $H$ or calculating $H$, $\Ob_k$ or $\R_k$.
The idea is that we can recursively factor the observability matrix according to
\begin{equation}\label{eq:decomp_O}
	\Ob_k = 
	\begin{bmatrix}
	C_k\\
	\Ob_{k+1}A_k
	\end{bmatrix}
	=
	\begin{bmatrix}
	\eye& 0 \\
	0& \Ob_{k+1}
	\end{bmatrix}
	\begin{bmatrix}
	C_k\\
	A_k
	\end{bmatrix}
	.
\end{equation}
And analogously the reachability matrix
\begin{equation}\label{eq:decomp_R}
	\R_k = 
	\begin{bmatrix}
	A_{k-1} \R_{k-1} & B_{k-1}
	\end{bmatrix}
	=
	\begin{bmatrix}
	A_{k-1} & B_{k-1}
	\end{bmatrix}
	\begin{bmatrix}
	\R_{k-1} &0\\
	0& \eye
	\end{bmatrix}
	.
\end{equation}
This allows us to create algorithms, that can convert realizations into input normal and output normal form.
With an combination of both it is also possible to obtain the singular values.
This is similar to the idea used in square root algorithms for the computation of normal forms.
The algorithm derived here is also given in \cite{chandrasekaran_fast_2005}.
Here a different explanation will be presented as well as a derivation of the SVD of the Hankel matrix. 

%\todo[inline]{More details on the fact that we do not require mininality and unlike QR based algorithms we can make sure that we do not create spurious observable states....
%May bee some proof based on the original rank of $\Ob_{k}$ and $\R_k$.
%Also note that a Rank revealing QR-algorithm would also work}

\paragraph{Make input normal and reachable}
First an algorithm to transform the system into an reachable and input normal system is given.
This means that all $R_k$ are made subunitary.
As we do not want to calculate the reachability matrices directly, the algorithm employs the decomposition given in Equation\,\ref{eq:decomp_R}.
If $\R_{k-1}^\top \R_{k-1}=\eye$ the next reachability matrix $\R_{k}$ is made sub orthogonal by calculating the SVD of $[A_{k-1} B_{k-1}]$
\begin{equation}
\R_k = 
%\begin{bmatrix}
%A_{k-1} \R_{k-1} & B_{k-1}
%\end{bmatrix}
%=
\begin{bmatrix}
A_{k-1} & B_{k-1}
\end{bmatrix}
\begin{bmatrix}
\R_{k-1} &0\\
0& \eye
\end{bmatrix}
=U_k\Sigma _k
\underbrace{V_k
\begin{bmatrix}
\R_{k-1} &0\\
0& \eye
\end{bmatrix}}_{\tilde{\R}_k}
\end{equation}
This gives the state transform $S_k = U_k\Sigma_k$. 
The usage of the reduced SVD also removes states that are not reachable by not increasing the rank of $\R_k$.
The product with the inverse $S_k^{-1}$ is implicitly calculated by computing the SVD.
We can set the matrices $\tilde{A}_k$ and $\tilde{B}_k$ to $[\tilde{A}_k,\tilde{B}_k]=V_k^\top$.
The matrices $A_{k}$ and $B_{k}$ have to be updated with the state transform $S_k$.

The algorithm is formalized in Algorithm\,\ref{alg:inp_normal}

\begin{algorithm}[htb]
	\begin{algorithmic}
	\Input System $[A,B,C,D]$
	\Output Input normal system $[A,B,C,D]$
	\For{$k\gets 1$ \textbf{to} $K-1$}
		\State $U,\sigma,V^\top \gets$ reducedSVD$([A_k,B_k],\epsilon=\text{tol}_i)$
		%\State $r \gets $ count$(\sigma>\text{tol}_i)$
		%\State $U \gets U[:,:r]$
		%\State $\sigma \gets \sigma[:r]$
		%\State $V^\top \gets V^\top[:r,:]$
		\State $[A_k,B_k] \gets V^\top$
		\State $A_{k+1}=A_{k+1}U\diag(\sigma)$
		\State $C_{k+1}=C_{k+1}U\diag(\sigma)$
	\EndFor
	\end{algorithmic}
\caption{Algorithm to convert to input normal system}\label{alg:inp_normal}
\end{algorithm}

This results in a input normal and reachable system.
\begin{proof}
	From $\tilde{\R}_{k-1}\tilde{\R}_{k-1}^\top = \eye$ follows that $\tilde{\R}_k\tilde{\R}_k^\top = \eye$ as
	\begin{equation}\label{eq:induction_reach}
	\tilde{R}_k\tilde{\R}_k^\top
	=
	V_k^\top\begin{bmatrix}\tilde{\R}_{k-1} &0\\
		0& \eye \end{bmatrix}
	\begin{bmatrix}\tilde{\R}_{k-1}^\top &0\\
		0& \eye \end{bmatrix} V_k
	= 
	V_k^\top\begin{bmatrix}\eye &0\\
	0& \eye \end{bmatrix} V_k
	=
	\eye
	\end{equation}
	As $x_1$ is zero dimensional $R_1$ vanishes and Equation\,\ref{eq:induction_reach} is true for $k=1$.
	By induction this proofs $\tilde{R}_k\tilde{R}_k^\top = \eye$ for all $k \in [1,\dots,K-1]$.
	Reachability directly follows form $\R_k\R_k^\top = \eye$.
\end{proof}




\paragraph{Make output normal}
The algorithm to transform the system into an observable and output normal system is analogous.
Here all $\Ob_k$ have to be sub-orthogonal.
We use the decomposition given in Equation\,\ref{eq:decomp_O} to avoid a computation of $\Ob_k$.
If $\Ob_{k+1} \Ob_{k+1}^\top=\eye$ the algorithm makes $\Ob_{k}$ sub-orthogonal by calculating the SVD of $[C_k^\top A_k^\top]^\top$.

\begin{equation}
	\Ob_{k}
	=
	\begin{bmatrix}
	\eye& 0 \\
	0& \Ob_{k+1}
	\end{bmatrix}
	\begin{bmatrix}
	C_k\\
	A_k
	\end{bmatrix}
	=
	\underbrace{
	\begin{bmatrix}
	\eye& 0 \\
	0& \Ob_{k+1}
	\end{bmatrix}
	U_k}_{\tilde{\Ob}_k}
	\Sigma_k V_k^\top
\end{equation}
This results in the state transform $S_k = \Sigma_k V_k^\top$.
Here the SVD is also in a reduced form, which results in a removal of all non observable states.
The state transform with $S_k^{-1}$ is done implicitly by the SVD.
The matrices $A_{k-1}$ and $C_{k-1}$ are updated with the state transform $S_k$

This gives the Algorithm\,\ref{alg:out_normal}
\begin{algorithm}[htb]
	\begin{algorithmic}
	\Input{System $[A,B,C,D]$}
	\Output{Output normal system $[A,B,C,D]$}
	\For{$k\gets K$ \textbf{downto} $2$}
		\State $U,\sigma,V^\top \gets$ reducedSVD$\left(\begin{bmatrix} C_k\\A_k \end{bmatrix},\epsilon=\text{tol}_o\right)$
		%\State $r \gets $ count$(\sigma>\text{tol}_o)$
		%\State $U \gets U[:,:r]$
		%\State $\sigma \gets \sigma[:r]$
		%\State $V^\top \gets V^\top[:r,:]$
		\State $\begin{bmatrix} C_k\\A_k \end{bmatrix} \gets U$
		\State $A_{k-1}=\diag(\sigma)V^\top A_{k+1}$
		\State $B_{k-1}=\diag(\sigma)V^\top C_{k+1}$
	\EndFor
	\end{algorithmic}
	\caption{Algorithm to convert to output normal system}\label{alg:out_normal}
\end{algorithm}
This results in a output normal and observable system.
\begin{proof}
	From $\tilde{\Ob}_{k+1}^\top\tilde{\Ob}_{k+1} = \eye$ follows that $\tilde{\Ob}_k^\top\tilde{\Ob}_k = \eye$ as
	\begin{equation}\label{eq:induction_obs}
		\tilde{\Ob}_k^\top\tilde{\Ob}_k
		=
		U_k^\top
		\begin{bmatrix} \eye& 0 \\
			0& \Ob_{k+1}^\top \end{bmatrix}
		\begin{bmatrix} \eye& 0 \\
		0& \Ob_{k+1} \end{bmatrix}
		U_k
		=
		U_k^\top
		\begin{bmatrix} \eye& 0 \\
		0& \eye \end{bmatrix}
		U_k
		=
		\eye
	\end{equation}
	As $x_{K+1}$ is zero dimensional $\Ob_{k+1}$ vanishes and Equation\,\ref{eq:induction_obs} is true for $K$.
	By induction this proofs $\tilde{\Ob}_k^\top\tilde{\Ob}_k = \eye$ for all $k \in [1,\dots,K-1]$
	Reachability directly follows form $\Ob_k\Ob_k^\top = \eye$.
\end{proof}


\paragraph{Obtain sigmas}
By converting an input normal system to an output normal system it is possible to calculate the singlaur values of $H$.
This uses the factorization in Equation\,\ref{eq:H_def} and the recursion relation in Equation\,\ref{eq:decomp_O}
to get
 \begin{equation}
 H = \Ob_k \R_k = 
 \begin{bmatrix}
 \eye& 0 \\
 0& \Ob_{k+1}
 \end{bmatrix}
 \begin{bmatrix}
 C_k\\
 A_k
 \end{bmatrix}
 \R_k
 .
 \end{equation}
As the algorithm started with an input normal system, $\R_k\R_k^\top = \eye$ is still fulfilled.
And due to the fact the algorithm has transformed the states $x_i$ for $i>k$ to output normal form $\Ob_{k+1}^\top \Ob_{k+1} = \eye$ is already fulfilled.
When the SVD is applied to the matrix $[C_k^\top A_k^\top]^\top$ this results in the SVD of $H$
 \begin{equation}
 H = 
 \underbrace{
 	\begin{bmatrix}
 	\eye& 0 \\
 	0& \Ob_{k+1}
 	\end{bmatrix}
 	U_k}_{U_k} 
 \Sigma_k 
 \underbrace{\vphantom{\begin{matrix} 0 \\0\end{matrix}}
 	V_k^\top
 	\R_k}_{\breve{V}_k^\top}
 =\breve{U}_k\Sigma_k \breve{V}_k^\top
 \end{equation}

 \begin{proof}
 	To prove that $H = \breve{U}_k\Sigma_k\breve{V}^\top$ is the singluar value decomposition we have to prove that $\breve{U}_k^\top \breve{U}_k= \eye$ and $\breve{V}^\top \breve{V} = \eye$.
 	If $\Ob_{k+1}$, $R_k$, $U_k$ and $V$ are orthogonal the proof directly follows. For the sub unitary case we have 
 	$
 	\breve{V}_k^\top \breve{V}_k
 	=
 	V_k^\top
 	\R_k
 	\R_k^\top
 	V_k
 	=
 	V_k^\top
 	\eye
 	V_k
 	=
 	\eye
 	$
 	and
 	$
 	U_k^\top U_k
 	=
 	\eye 
 	$
 	has been proven in Equation\,\ref{eq:induction_obs}
 \end{proof}

The resulting system is ordered.
\begin{proof}
	The condition in Equation\,\ref{eq:orderd_obs} directly follows form the output normality $\tilde{\Ob}_k^\top \tilde{\Ob}_k = \eye$.
	The condition in Equation\,\ref{eq:orderd_reach} follows form the input normality of the intermediate system by
	\begin{equation}
		\tilde{\R}_k \tilde{\R}_k^\top 
		=
		\Sigma_k V_k \R_k \R_k^\top V_k^\top \Sigma_k 
		= \Sigma_k^2
	\end{equation}
	As $\Sigma_k$ is result form an SVD, the singular values are in decreasing order. 
\end{proof}
Therefore the system can be approximated using balanced truncation.

This algorithm is also a fast way to reduce the system to a minimal system.
The tolerance for the first transformation $\text{tol}_i$ is usually set to a value close to the machine precision to avoid errors due to the removed states.
The tolerance for the second transformation $\text{tol}_o$ is then set to the wanted $\epsilon$.
 
It is also possible to first convert the system to an output normal system and calculate the SVD when converting it to an input normal system.
The proof for this and the algorithms for the anticausal case are omitted for brevity.

\todo[inline]{Some notes on cumputational cost}
 
 
\todo[inline]{Is it clear if there is a difference between immediately cutting or cutting later?}

%\end{itemize}
\section{Cost of computation}
%\begin{itemize}
%\item Short section on the computational cost and number of parameters.
%\item Define it with and without additions.

\todo[inline]{Might also go in the Background, but I think it would be a bit to much and I did not find a detailed discussion in the literature. } 
%\end{itemize}

When using Time Varying Systems to approximate weight matrices, we are mainly interested in the cost of computing the product $y = Tu$ using time varying systems.
The cost is the number of Floating Point Operations(FLOPs).
The number of parameters is also important, as this determines the required memory.

First the cost for a causal stage is derived.
Without considering additions, the cost $C_k$ for the stage $k$ is 
\begin{equation}
	C_k = d_{k+1}d_k + d_{k+1}m_k+p_kd_k+p_km_k
	.
\end{equation}
We can also include the additions and get the cost
 \begin{equation}
 	C_k' = d_{k+1}(2d_k-1) + d_{k+1}(2m_k-1)+p_k(2d_k-1)+p_k(2m_k-1)
 	.
 \end{equation}
For anticausal system, the formulas are the same except a change in the indexing 
Without additions this gives the cost for one stage
\begin{equation}
C_k = \da_{k-1}\da_k + \da_{k-1}m_k+p_k \da_k+p_km_k
.
\end{equation}
If the additions are included this results in the cost
\begin{equation}
C_k' = \da_{k-1}(2\da_k-1) + \da_{k-1}(2m_k-1)+p_k(2d_k-1)+p_k(2m_k-1).
\end{equation}
When considering mixed systems the cost for the causal and the anticausal system are added.
As the $D$-matrices for the anticausal part are not needed the according FLOPs are irrelevant.
This gives the cost for a mixed stage
\begin{equation}
C_k = d_{k+1}d_k + \da_{k-1}\da_k + (d_{k+1}+\da_{k-1})m_k +(d_k+\da_k)p_k +p_km_k 
\end{equation}
Include the additions, the cost is
\begin{multline}
C_k' = d_{k-1}(2d_k-1)+ \da_{k-1}(2\da_k-1) 
+ (d_{k-1}+\da_{k+1})(2m_k-1)\\
+p_k(2d_k-1)+p_k(2\da_k+1)+p_k(2m_k-1)
.
\end{multline}
The total cost is the sum over all stages
\begin{equation}\label{eq:cost_total}
C = \sum_{k=1}^K C_k
.
\end{equation}
The number of parameters is equal to the number of multiplications.
In the following I will only use the number of multiplications, as it is easier to simplify.

\section{Number of stages}
To choose the appropriate number of stages $K$ I approximated how the cost depends on the number of stages.
For this the dimensions $d$ $\da$, $p$ and $m$ are considered as constant.

Then the cost in Equation\,\ref{eq:cost_total} can be simplified to the multiplication
\begin{equation}
C = K(d^2 + {\da}^2 + (d+\da)m +(d+\da)p +pm) 
.
\end{equation}
The input and output sizes are related to the number of stages by $m = M/K$ and $p=P/K$.
This results in
\begin{align}
C =& K(d^2 + {\da}^2 + (d+\da)\frac{M}{K} +(d+\da)\frac{P}{K} +\frac{PM}{K^2}) \\
=&
K(d^2 + {\da}^2) + (d+\da)M +(d+\da)P +\frac{1}{K}PM) 
.
\end{align}
This relation describes how the general computational cost depends on the number of stages.
This approximation is not helpful if we want to approximate matrices, as in these cases the state dimension are usually not constant.
Usually the state dimensions are small for the first and last states and larger for the states in the middle.
Here different approximations of the state dimensions have to be used.

I used an approximation based on the properties of the singular values and their relation to the Frobenius norm.
The singular values are related to the Frobenius norm by $\|H\|_F = \sqrt{\sum \sigma_i^2}$.
If we consider the elements of $H$ as constant on average, then $\|H\|_F \propto \text{size(H)}$.
This means that the Frobenius norm increases with the size of the matrix, which in turn should also result in increasing singular values.
This is not directly related to the number of states as it depends on the distribution of the singular values and the wanted accuracy.
I did choose to not make this more concrete using random matrix theory, as this is only a rough approximation.
Regardless, the approximation $\breve{d}_k \propto \text{size}(H_k)$ fits the behavior of the considered matrices fairly well as illustrated in Figure\,\ref{fig:approx_degree}.
\begin{figure}[htb]
	\centering
	\includegraphics[width=\textwidth]{Plots/approx_degree.pdf}
	\caption{Approximated degree of a matrix form the \emph{Mobilenet V2} model \cite{sandler_mobilenetv2_2019}.
	The actual degree is the number of singular values with $\sigma > \alpha \|A\|_H$ for every causal Hankel matrix. Here $\alpha$ is a hyperparameter that determines the accuracy.
	Additionally approximations of the rank with suitable parameters $\gamma$ are plotted.}
	\label{fig:approx_degree}
\end{figure}
I approximate the degree using the relation
\begin{equation}\label{eq:approx_d}
	\breve{d}_k = \gamma (K-k+1)(k-1)\frac{PM}{K^2\min(P,M)}.
\end{equation}
The variable $\gamma$ is a proportionality factor.
The factor $1/\min(P,M)$ is used to normalize the state dimensions in such a way that for $\gamma = 1$ the approximation is compatible to the maximal rank of $H_k$. 
As I suppose that the statedimensions for the anticausal case is equivalent to the statedimensions for the causal case, I can calculate the cost using
\begin{equation}
\sum_{k=1}^{K} \left(2 d_{k} d_{k+1} + \frac{2 M d_{k+1}}{K} + \frac{2 P d_{k}}{K} + \frac{M P}{K^{2}}\right)
\end{equation}
As the the matrix can be transposed, $\min(P,M)=M$ can be assumed without loss of generality.
By inserting the approximation form Equation\,\ref{eq:approx_d} and transforming the sums using Faulhaber's formula \cite{knuth_johann_1993} the cost is transformed to
\begin{equation}\label{eq:cost_of_K}
 \frac{K P^{2} \gamma^{2}}{15} + \frac{M P \gamma}{3} + \frac{P^{2} \gamma}{3} + \frac{M P - \frac{P^{2} \gamma^{2}}{3}}{K} + \frac{- \frac{M P \gamma}{3} - \frac{P^{2} \gamma}{3}}{K^{2}} + \frac{4 P^{2} \gamma^{2}}{15 K^{3}}
\end{equation}
This makes it possible, to calculate the cost for different values for the parameter $\gamma$.
In Figure\,\ref{fig:cost_parameters} 
this is done for a example with $M=N = 2^{10}$.
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.75\textwidth]{Plots/cost_parameters.pdf}
	\caption{Approxiamted cost for different numbers of stages. The system has $M=P=2^{10}$ input and output dimensions. 
	The cost is approximated for different values for the parameter $\gamma$.
	The optimal number of stages for every $\gamma$ is marked with an circle}
	\label{fig:cost_parameters}
\end{figure}

We can see that for one stage the cost is equivalent to the cost of a simple matrix product, as the whole matrix is stored as one $D$-matrix.
For $\gamma = 1$ the cost only increases for higher number of stages. 
For $\gamma < 0$ the cost first decreases before increasing later.
\todo{Is it usefull to give some details on the parameters of Equation\,\ref{eq:cost_of_K}} 
The figure shows that the optimal number of stages $\hat{K}$ increases as $\gamma$ decreases.
For very small $\gamma$ I get $\hat{K} = 1/2 N$.
For more reasonable values like $\gamma=0.16$ the optimal number of stages is $\hat{K}=N2^{-5}$. 
This gives an educated guess on the optimal number of stages $K$ for a weight matrix.

The approximation has the downsides, that the parameter $\gamma$ has to be estimated
and the input and output dimensions are modeled as constant, which is not true for the tested systems.
%This means that the number of stages can be higher if the stete diemsniuon 
%This is not surprising, as for a high number of stages and a high state dimension, the 
It is also possible to approximate the state dimension based on the the maximum possible rank of the Hankel matrix.
The Rank has to be lower than $\min(\text{width}(H_k),\text{height}(H_k))$.
This approximation might have some benefits for very tall or wide matrices, as this approximation is able to represent ranks that are not symmetric in $k$. 


\newpage
\section{Segmentation Adaptation}

In this section the segmentatation of the matrix is changed. 
This means adapting the input and output dimensions. 
First I will describe how to move the boundaries.
In Subsection\,\ref{subsec:move_sig} the algorithm is extended in a way that makes it possible to calculate the Hankel singular values.
In Subsection\,\ref{subsec:optim_segmentation} a strategy to improve the segmentation is presented.

\subsection{Moving Boundaries}
In this subsection an algorithm to move the boundaries of a causal system is introduced.
For this we have to move inputs $u_\m$ or outputs $y_\m$ from one stage to another.
The connections that are changed when we do this are drawn in color in the Figures\,\ref{fig:move_left}-\ref{fig:move_up}.
All other connections will remain unchanged.
This also means that the observability and reachability matrices for the previous and later states stay the same.
The adaptation of the segmentation changes the parts of the matrix that can be represented with a causal system.
When we move boundaries to the left or down, some connections are no longer possible in a causal system. These will be dropped.
Conversely when we move boundaries to the right or up new connections will be established.
These connections will be drawn with dotted lines in the Figures\,\ref{fig:move_left}-\ref{fig:move_up}.
For an mixed system these connections can be moved to the anticausal system and vice versa.
The algorithms are constructed such that they preserve minimality.

\paragraph{Move Left}
\begin{figure}[!htb]
	\centering
	\input{diagrams/Moves/Move_left.pgf}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{Original system}
		\label{fig:move_left_a}
	\end{subfigure}
	\hspace{0.8cm}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{Moved system}
		\label{fig:move_left_b}
	\end{subfigure}
	\caption{Illustration of a system where a boundary is moved to the left}
	\label{fig:move_left}
\end{figure}
\begin{algorithm}[!htb]
	\begin{algorithmic}
		\Input{System $[A,B,C,D]$, index $k$}
		\Output{System $[A,B,C,D]$ with changed input dimensions}
		\State $b \gets B_k[:,-1]$
		\State $D_k \gets D_k[:,:-2]$
		\State $B_k \gets B_k[:,:-2]$
		\State $D_{k+1} \gets [C_{k+1}b,D_{k+1}]$
		\State $B_{k+1} \gets [A_{k+1}b,B_{k+1}]$
		\State $U,\sigma,V^\top \gets$ SVD$([A_k,B_k])$ 
		\If{$\sigma[-1] < \text{tol}$}
		\State $U \gets U[:,:-2]$; $\sigma \gets \sigma[:-2]$; $V \gets V[:-2,:]$
		%\State $U \gets U[:,:-2]$
		%\State $\sigma \gets \sigma[:-2]$
		%\State $V \gets V[:-2,:]$
		\State$[A_k,B_k] \gets \diag(\sigma) V^\top$
		\State$A_{k+1} \gets A_{k+1}U$
		\State$C_{k+1} \gets C_{k+1}U$
		\EndIf
	\end{algorithmic}
	\caption{Algorithm to move a boundary between $u_k$ and $u_{k+1}$ to the left}\label{alg:move_left}
\end{algorithm}
First the boundary between two inputs is moved left.
This means that the last element of the input $u_k$ is removed and inserted as a new first element in $u_{k+1}$. 
The moved input is designated as $u_\m$.
This change is illustrated in Figure\,\ref{fig:move_left}.

The vector $b$ is the last column of $B_k$ and describes the connection from $u_\m$ to $x_{k+1}$. The vector $d$ is the last column of $D_k$ and describes the connection from $u_\m$ to $y_{k}$.
These are removed form the matrices and we get $\breve{B}_k$ and $\tilde{D}$.
We can see that the connection $d$ in the original system cannot by realized with an causal system as this would mean a connection from $\tilde{u}_{k+1}$ to $y_k$ in the transformed system.
Now we have to add the the input $u_\m$ to the next stage.
The connection form the input $u_\m$ to the output $y_{k+1}$ can be incorporated by adding the vector $C_{k+1}b$ to $D_{k+1}$ as a new column.
Analogously the connection form $u_\m$ to the state $x_{k+2}$ can be incorporated by attaching the vector $A_{k+1}b$ to $B_{k+1}$.

As the last column of $\R_{k+1}$ is removed, this might result in a non reachable system.
This is the case if the matrix $[A_b \: \breve{B}_k]$ no longer has full rank.
We can check this using the SVD. 
If the last singular value is equal to zero, the state has to be removed. 
This can be done using a state transform based on the reduced SVD. 
The matrices $[\tilde{A}_b \: \tilde{B}_k]$ are set to $\diag(\sigma) V^\top$ and 
the following stages are transformed according to $\tilde{A}_{k+1} = A_{k+1}U$ and $\tilde{C}_{k+1} = C_{k+1} U$.

The pseudocode to move the boundary to the left is given in Algorithm\,\ref{alg:move_left}.
 


\paragraph{Move Down}
\begin{figure}[!htb]
	\centering
	\input{diagrams/Moves/Move_down.pgf}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{Original system}
		\label{fig:move_down_a}
	\end{subfigure}
	\hspace{0.8cm}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{Moved system}
		\label{fig:move_down_b}
	\end{subfigure}
	\caption{Illustration of a system where a boundary is moved down}
	\label{fig:move_down}
\end{figure}
\begin{algorithm}[!htb]
	\begin{algorithmic}
		\Input{System $[A,B,C,D]$, index $k$}
		\Output{System $[A,B,C,D]$ with changed output dimensions}
		\State $c^\top \gets C_{k+1}[1,:]$
		\State $D_{k+1} \gets D_{k+1}[2:,]$
		\State $C_{k+1} \gets C_{k+1}[2:,:]$
		\State $D_k \gets \begin{bmatrix} D_k\\c^\top B_k	\end{bmatrix}$
		\State $C_k \gets \begin{bmatrix} C_k\\c^\top A_k	\end{bmatrix}$
		\State $U,\sigma,V^\top \gets$ SVD$\left(\begin{bmatrix} A_{k+1}\\C_{k+1}	\end{bmatrix}\right)$ 
		\If{$\sigma[-1] < \text{tol}$}
		\State $U \gets U[:,:-2]$; $\sigma \gets \sigma[:-2]$; $V \gets V[:-2,:]$
		%\State $U \gets U[:,:-2]$
		%\State $\sigma \gets \sigma[:-2]$
		%\State $V \gets V[:-2,:]$
		\State$\begin{bmatrix} A_{k+1}\\C_{k+1}	\end{bmatrix} \gets U\diag(\sigma)$
		\State$A_{k} \gets V^\top A_{k}$
		\State$C_{k} \gets V^\top C_{k}$
		\EndIf
	\end{algorithmic}
	\caption{Algorithm to move a boundary between $y_k$ and $y_{k+1}$ down}\label{alg:move_down}
\end{algorithm}
A similar strategy can be used to move a boundary down.
That means removing the first output $y_\m$ form $y_{k+1}$ and adding it to $y_k$.
This is illustrated in Figure\,\ref{fig:move_left}.


The vector $c^\top$ is the first row of $C_{k+1}$ and describes the connection form $x_k$ to $y_\m$.
This vector is removed from $C_{k+1}$.
We also remove the first row from $D_{k+1}$.
Then we attach the output $y_\m$ to the output vector $y_k$.
For this the vector $c^\top B_k$ is added as last row to $D_k$ and the vector $c^\top A_k$ is added as last row to $C_k$.

As the algorithm removes a row from $\Ob_{k+1}$ it might happen that the state $x_{k+1}$ is no longer observable.
This is the case if the kernel of $\Ob_{k+1}$ is not the zero subspace.
We can check this with the SVD of $[A_{k+1}^\top,\breve{C}_{k+1}^\top]^\top$.
If the last singular values is equal to zero the state has to be reduced.
This is done by using a state transform based on the reduced SVD. 
Again the inverse does not need to be calculated as it is implicitly calculated by the SVD.
An algorithm to move the state is given in Algorithm\,\ref{alg:move_down}

\FloatBarrier
\paragraph{Move Right}
\begin{figure}[!htb]
	\centering
	\input{diagrams/Moves/Move_right.pgf}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{Original system}
		\label{fig:move_right_a}
	\end{subfigure}
	\hspace{0.8cm}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{Moved system}
		\label{fig:move_right_b}
	\end{subfigure}
	\caption{Illustration of a system where a boundary is moved to the right}
	\label{fig:move_right}
\end{figure}
\begin{algorithm}[!htb]
	\begin{algorithmic}
		%		def move_right(sys,v,d_new=None):
		%		#function to move a boundary left 
		%		#v is the index to the left of the boundary
		\Input{System $[A,B,C,D]$, index $k$}
		\Output{System $[A,B,C,D]$ with changed input dimensions}
		\State $b \gets B_{k+1}[:,1]$
		\State $d \gets D_{k+1}[:,1]$
		\State $B_{k+1} \gets B_{k+1}[:,2:]$
		\State $D_{k+1} \gets D_{k+1}[:,2:]$
		%		#check if [d;b] in range(C_v+1;Av+1)
		\State $U,\sigma,V^\top \gets$ SVD$\left(\begin{bmatrix}
		C_{k+1}\\A_{k+1}
		\end{bmatrix}\right)$ Full matrices 
		
		\State $r \gets $ count$(\sigma>\text{tol}_i)$\Comment{TODO: is r needed?}
		\State $a \gets U^\top \begin{bmatrix}
		d\\b
		\end{bmatrix}$
		%\If{any $|a[r+1:]|>\text{tol}$}
		\If{$\|a[r+1:]\|>\text{tol}$}
		\Comment not in range
		\State $B_k \gets \begin{bmatrix}
		B_k & 0\\
		0 & 1
		\end{bmatrix}$
		\State $A_k \gets \begin{bmatrix}
		A_k \\ 0
		\end{bmatrix}$
		\State $A_{k+1} \gets [A_{k+1},b]$
		\State $C_{k+1} \gets [C_{k+1},d]$		
		\Else
		%			#in range -> no need for an additional dim
		\State $m \gets V[:,:r] \diag(\sigma[:r])^{-1} a[:r]$
		\State $B_k \gets \begin{bmatrix} B_k & m\end{bmatrix}$
		%		
		\EndIf
		%		#set the appropirate D_v
		\State $D_v \gets \begin{bmatrix}
		D_k&d_{\text{new}}
		\end{bmatrix}$
	\end{algorithmic}
	\caption{Algorithm to move a boundary between $u_k$ and $u_{k+1}$ to the right}\label{alg:move_right}
\end{algorithm}
It is also possible to reverse these changes.
When we move the boundary to the right, the input $x_\m$ is removed form the input $u_{k+1}$ and attached to the input vector $\tilde{u}_k$.
The connection form $u_\m$ is moved form the current stage to the previous stage.
We can do this by directly routing the input to a an additional state connection.
To attach the input $u_\m$ to the end of the state vector $x_{k+1}$, we set 
\begin{equation}
	\breve{B}_k = \begin{bmatrix}
	B_k & 0\\
	0 & 1
	\end{bmatrix}
\end{equation}
This makes the input $u_\m$ available in the next stage as a new state.
Now the connections can be reconnected.
For this we remove first row $d$ of $D_{k+1}$ and attach it to to $C_{k+1}$ as the new last row. 
Analogously the first row $b$ of $B_{k+1}$ is removed and attached to the end of $A_{k+1}$.
This is illustrated in Figure\,\ref{fig:move_left}.

This trivial algorithm results in a new Observability matrix $\tilde{\Ob}_{k+1}$ that has one additional column.
\begin{equation}\label{eq:strucure_mover}
	\tilde{\Ob}_{k+1}
	=
	\begin{bmatrix}
	\eye & 0\\ 0 &\Ob_{k+2}
	\end{bmatrix}
	\begin{bmatrix}
	C_{k+1} & d\\
	A_{k+1} & b
	\end{bmatrix}
\end{equation}
If the last row is not linearly independent, the system is no longer observable as $\Ob_{k+1}$ does not have full column rank.
This is the case if
\begin{equation}
	\begin{bmatrix}
	d\\b
	\end{bmatrix}
	\in
	\range\left(
	\begin{bmatrix}
	C_{k+1}\\A_{k+1}
	\end{bmatrix}\right).
\end{equation}
If the vector is in the range then there exits a state $m$ with the property that
\begin{equation}
	\begin{bmatrix}
	d\\b
	\end{bmatrix}
	=
	\begin{bmatrix}
	C_{k+1}\\A_{k+1}
	\end{bmatrix}
	m
\end{equation}
This also means that there is no new state dimension required.
In this case the vector $m$ is appended to the matrix $B_{k}$.
The algorithm uses the SVD of $[C_{k+1}^\top,A_{k+1}^\top]^\top$ to check the relations.
This uses the intermediate vector 
\begin{equation}
	a = U^\top 	
	\begin{bmatrix}
	d\\b
	\end{bmatrix}
\end{equation} 
If the vector $[d^\top,b^\top]^\top$ is in the range, the norm of 
\begin{equation}
	\|a_{[r+1:]} \| = 0
	.
\end{equation}
Here $r$ is the number of nonzero singular values.
The vector $m$ is computed with 
\begin{equation}
	m = V[:,:r] \diag(\sigma[:r])^{-1} a[:r]
\end{equation}
This is equivalent to calculating the vector $m$ using the pseudoniverse.  
Additionally the vector $d_\text{new}$ is attached $D_k$ to represent the new connection form $u_{k+1}$ to $y_\m$.




\paragraph{Move Up}
\begin{figure}[!htb]
	\centering
	\input{diagrams/Moves/Move_up.pgf}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{Original system}
		\label{fig:move_up_a}
	\end{subfigure}
	\hspace{0.8cm}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{Moved system}
		\label{fig:move_up_b}
	\end{subfigure}
	\caption{Illustration of a system where a boundary is moved up}
	\label{fig:move_up}
\end{figure}
\begin{algorithm}[!htb]
	\begin{algorithmic}
		\Input{System $[A,B,C,D]$, index $k$}
		\Output{System $[A,B,C,D]$ with changed  output dimensions}
		\State $c^\top \gets C_{k}[-1,:]$
		\State $d^\top \gets D_{k}[-1,:]$
		\State $C_{k} \gets C_{k}[:-2,:]$
		\State $D_{k} \gets D_{k}[:-2,:]$
		\State $U,\sigma,V^\top \gets$ SVD$\left(\begin{bmatrix}
		A_{k+1}^\top\\B_{k+1}^\top
		\end{bmatrix}\right)$ Full matrices 
		\State $r \gets $ count$(\sigma>\text{tol}_i)$
		\State $a \gets U^\top \begin{bmatrix}
		c\\d
		\end{bmatrix}$
		\If{$\|a[r+1:]\|>\text{tol}$} 
		\Comment not in range
		\State $A_k \gets \begin{bmatrix}
		A_k \\ c^\top
		\end{bmatrix}$
		\State $B_k \gets \begin{bmatrix}
		B_k \\ d^\top
		\end{bmatrix}$
		\State $C_{k+1} \gets \begin{bmatrix}
		0 & 1\\ C_{k+1} & 0
		\end{bmatrix}$
		\State $A_{k+1} \gets \begin{bmatrix}
		A_{k+1} & 0
		\end{bmatrix}$
		\Else
		\State $m \gets V[:,:r] \diag(\sigma[:r])^{-1} a[:r]$
		\State $C_{k+1} \gets \begin{bmatrix} m^\top\\C_{k+1} \end{bmatrix}$
		\EndIf
		\State $D_v \gets \begin{bmatrix}
		d_{\text{new}} \\D_k
		\end{bmatrix}$
	\end{algorithmic}
	\caption{Algorithm to move a boundary between $y_k$ and $y_{k+1}$ up}\label{alg:move_up}
\end{algorithm}

An similar algorithm, makes it possible to move up a boundary.
When moving the boundary up the output $y_\m$ is removed form the output $y_k$ and attached to the output vector $y_{k+1}$.
Here the output $y_\m$ is constructed in the stage $k$ and appended to the state $x_{k+1}$.
First the last column of $D_{k}$ is removed and stored in $d^\top$ and analogously the last column of $C_k$ is removed and stored in $c^\top$.
The vectors $c^\top$ is appended to the matrix $A_k$ and the vector $d^\top$ is attached to $B_k$.
Now we can route the new state to the new output.
This can be done by setting 
\begin{equation}
	\breve{C}_{k+1} = 
	\begin{bmatrix}
	0 & 1\\ C_{k+1} & 0
	\end{bmatrix}
\end{equation}
This is illustrated in Figure\,\ref{fig:move_up}.

The algorithm adds a new column to reachability matrix.
\begin{equation}
\tilde{\R}_{k+1}
=
\begin{bmatrix}
A_{k} & B_{k}\\
c^\top & d^\top
\end{bmatrix}
\begin{bmatrix}
\R_{k} &0\\
0& \eye
\end{bmatrix}
\end{equation}
If this new column is part of the co-range of $\R_{k+1}$, then the system is no longer reachable.
In this case no additional state is needed. 
Using a similar derivation as for the previous move we can check this using the SVD of $[A_{k+1} \: B_{k+1}]^\top$.
If no new state is needed, then we can simply attach the vector
\begin{equation}
	m = \text{pinv}\left(\begin{bmatrix}
	A_{k+1}^\top\\B_{k+1}^\top
	\end{bmatrix}\right) 
	\begin{bmatrix}
	c \\ d
	\end{bmatrix}
\end{equation}
to the matrix $C_{k+1}$.

The pseudocode can be found in Algorithm\,\ref{alg:move_up}.

\FloatBarrier

\subsection{Combine and Split stages}
It is also possible to split and combine stages. In \cite{chandrasekaran_fast_2005} an algorithm for this purpose is given.
\todo[inline]{The same algorithm is given in the other paper, but we could give a more detailed explaination on the combination.
Also I think the derivation of the splitting is clearer than the derivation given in the paper}
\FloatBarrier
\subsection{Moving Boundaries and calc sigmas}\label{subsec:move_sig}
In this section the ideas form Section\,\ref{sec:approx} are used to calculate the singular values for a moved system.
The main idea is that this algorithm iterates over the different stages and moves them if needed.


I will illustrate the algorithms to move the bounds left or right. 
\todo[inline]{The algorithms for moving up or down will be omitted for brevity/Can be found in the appendix}
In the following, the algorithm starts with an output normal system and transforms it into a ordered input normal system, by iterating over the stages from $k=2$ up to $K$.


\paragraph{Move Left}
When moving the boundary between $u_{k-1}$ and and $u_{k}$ to the left the distance $l$, the last $l$ columns of $\R_{k}$ are removed.
Based on this we have the new Hankel matrix
\begin{equation}
	\tilde{H}_k = \tilde{\Ob}_k \tilde{\R}_k = \Ob_{k}\R_{k}[:,:-(l+1)]
	.
\end{equation}
Using the decomposition form Equation\,\ref{eq:decomp_R} this becomes
\begin{equation}
	H_{k}=\Ob_{k}
	\begin{bmatrix}
	A_{k-1}&B_{k-1}[:,:-(l+1)]
	\end{bmatrix}
	\begin{bmatrix}
	\R_{k-1} & 0\\ 0& \eye
	\end{bmatrix}
\end{equation}
If $\Ob_k^\top \Ob_k = \eye$ and $\R_{k-1} \R_{k-1}^\top = \eye$ then the singular values of the SVD of $\begin{bmatrix}
A_{k-1}&B_{k-1}[:,:-(l+1)]
\end{bmatrix}$ are the singular values of $H_k$.
This is the case as the initial system in output normal and the previous stages where already transformed to input normal form.
Based on the singular value decomposition, the stage can be transformed such that $\R_{k} \R_{k}^\top = \eye$.
This also gives a ordered realization.

\begin{algorithm}[htb]
	\begin{algorithmic}
	%\For{$k\gets 2$ \textbf{to} $K$}
		\Input{System $[A,B,C,D]$ with $\Ob_k^\top \Ob_k = \eye$ and $\R_{k-1} \R_{k-1}^\top = \eye$,\\ \spaceIO
		Index $k$, distance to move $l$}
		\Output{System $[A,B,C,D]$ with changed input dimensions and $\R_{k} \R_{k}^\top = \eye$, \\ \spaceIO 
			Hankel singular values $\sigma$}
		\State $b \gets B_{k-1}[:,-l:]$
		\State $D_{k-1} \gets D_{k-1}[:,:-(l+1)]$
		\State $B_{k-1} \gets B_{k-1}[:,:-(l+1)]$
		\State $D_{k} \gets [C_{k}b,D_{k}]$
		\State $B_{k} \gets [A_{k}b,B_{k}]$
		\State $U,\sigma,V^\top \gets$ reducedSVD$([A_{k-1},B_{k-1}],\epsilon=\text{tol})$ 
%		\State $r \gets $ count$(\sigma>\text{tol})$
%		\State $U \gets U[:,:r]$
%		\State $\sigma \gets \sigma[:r]$
%		\State $V^\top \gets V^\top[:r,:]$
		\State $[A_{k-1},B_{k-1}] \gets V^\top$
		\State $A_{k}=A_{k}U\diag(\sigma)$
		\State $C_{k}=C_{k}U\diag(\sigma)$
	%\EndFor
	\end{algorithmic}
	\caption{Algorithm to move the boundary between $u_k$ and $u_{k+1}$ to the left by $l$ inputs}\label{alg:move_left_sig}
\end{algorithm}


\paragraph{Move Right}
When the bound is moved right by the distance $l$, this results in $l$ additional columns in $\R_k$.
Additional states are also required in most cases.
Using the decomposition form Equation\,\ref{eq:decomp_R} and the structure form Equation\,\ref{eq:strucure_mover} the new Hankel matrix can be expressed as
\begin{equation}
	H_k = 
	\begin{bmatrix}
	\eye & 0\\ 0 &\Ob_{k+1}
	\end{bmatrix}
	\begin{bmatrix}
	C_{k} & d\\
	A_{k} & b
	\end{bmatrix}
	\begin{bmatrix}
	\begin{bmatrix}
	A_{k-1} \\
	0
	\end{bmatrix}&
	\begin{bmatrix}
		B_{k-1}&0\\
		0&\eye
	\end{bmatrix}
	\end{bmatrix}
	\begin{bmatrix}
	\R_{k-1} & 0\\ 0& \eye
	\end{bmatrix}
\end{equation}
If $A_{k-1}$ and $B_{k-1}$ were transformed to input normal form analogous to Algorithm\,\ref{alg:inp_normal} this can be reduced to 
\begin{equation}
H_k = 
\begin{bmatrix}
\eye & 0\\ 0 &\Ob_{k+1}
\end{bmatrix}
\begin{bmatrix}
C_{k+1} & d\\
A_{k+1} & b
\end{bmatrix}
\begin{bmatrix}
\R_{k-1} & 0\\ 0& \eye
\end{bmatrix}
\end{equation}
with an sub orthogonal $\R_{k-1}$.
The singular values of the SVD of $\begin{bmatrix}
	C_{k+1} & d\\
	A_{k+1} & b
\end{bmatrix}$ are the singular values of the Hankel matrix.
Using the SVD the system can be reduced to a minimal system. 
To preserve the input normality $A_{k-1}$ and $B_{k-1}$ are multiplied with $V^\top$ and $[A_k^\top \quad C_k^\top]^\top$ is set to $U \Sigma$.
This results in the Algorithm\,\ref{alg:move_right_sig}.

\begin{algorithm}[htb]
	\begin{algorithmic}
	\Input{System $[A,B,C,D]$ with $\Ob_k^\top \Ob_k = \eye$ and $\R_{k-1} \R_{k-1}^\top = \eye$,\\ \spaceIO
		Index $k$, distance to move $l$}
	\Output{System $[A,B,C,D]$ with changed input dimensions and $\R_{k} \R_{k}^\top = \eye$,\\ \spaceIO
		 Hankel singular values $\sigma$}
	%\For{$k\gets 2$ \textbf{to} $K$}
		\State $[A,B,C,D] \gets \text{input normal}_k([A,B,C,D])$
		\Comment Transform state $x_k$
		\State $b \gets B_{k}[:,:l]$
		\State $d \gets D_{k}[:,:l]$
		\State $B_{k+1} \gets B_{k}[:,l+1:]$
		\State $D_{k+1} \gets D_{k}[:,l+1:]$
		\State $U,\sigma,V^\top \gets$ SVD$\left(\begin{bmatrix}
		C_{k+1}&d\\A_{k+1}&b
		\end{bmatrix}\right)$  
		\State $r \gets $ count$(\sigma>\text{tol}_i)$
		\State $U \gets U[:,:r]$
		\State $\sigma \gets \sigma[:r]$
		\State $V^\top \gets V^\top[:r,:]$
		\State $B_{k-1} \gets V^\top \begin{bmatrix}
		B_{k-1} & 0\\
		0 & \eye
		\end{bmatrix}$
		\State $A_{k-1} \gets V^\top \begin{bmatrix}
		A_{k-1} \\ 0
		\end{bmatrix}$
		\State $\begin{bmatrix}
		A_{k} \\ B_{k} 
		\end{bmatrix}  \gets U \diag(\sigma)$	
		\State $D_v \gets \begin{bmatrix}
		D_k&d_{\text{new}}
		\end{bmatrix}$
	%\EndFor
	\end{algorithmic}
	\caption{Algorithm to move the boundary between $u_k$ and $u_{k+1}$ to the right by $l$ inputs}\label{alg:move_right_sig}
\end{algorithm}

Analogous algorithms can be derived for the anticausal part.

\subsection{Algorithm to optimize Input and oputput dims}\label{subsec:optim_segmentation}

When we want to obtain the optimal segmentation of a system, 
we want to solve the optimization problem
\begin{equation}
min_\text{segmentation} f(\text{System})
\end{equation}
Where $f(\text{System})$ is some objective function.
The problem is discrete and in general nonconvex, which makes it hard for us to solve it.

To get an possible solution of the optimization problem an iterative algorithm is used.
For this both the input dimensions and the output dimensions have to be changed.
This algorithm alternately changes the input and output dimensions.

The general strategy is that we start with a input normal causal system in combination with an output normal anticausal system.
First we adapt the dimensions of the outputs.
For every boundary, the algorithm computes a preliminary system in which the boundary is moved to the down, not moved or moved up.
For these three options the objective function is calculated. 
Then the system with the lowest objective function is adopted.
This is done for every boundary.
By doing this using the Algorithms described in the previous subsection, the Systems are transformed to an causal input normal system and an anticausal output normal system.
These algorithms also compute the singular values of the Hankel matrices. This makes ist possible, to use these in the objective function.
After this, we adapt the input dimensions.
When this is done the bounds are moved to the left, not moved and moved to the right.
Again the option with the lowest objective function is used.
During this, the system is transformed back to an input normal causal system in combination with an output normal anticausal system.
This makes it possible to immediately start again with an adaption of the output dimensions.

The pseudocode for the adaptation of the segmentation is given in Algorithm\,\ref{alg:optimize segemtnation}.

\begin{algorithm}[htb]
	\begin{algorithmic}
		\Input{Input normal causal system, output normal anticausal system,\\ 
			\spaceIO sequences $\lsearchud$ and $\lsearchrl$}
		\Output{Ordered System $[A,B,C,D]$ with optimized input and output dimensions}
		%\State Start with input normal causal system and output normal anticausal system, 
		\For{$i\gets 1$ \textbf{to} Number of iterations}
		\State update output segmentation
		\For{$k \gets K$ \textbf{downto} $2$}
		\State \Comment{iterate over output dimensions 
			and transform causal system to output normal and anticausal to input normal}
		\State $S_d \gets $ move down$(S)$ by $\lsearchud_i$
		\State $S_n \gets $ no move $(S)$ 
		\State $S_u \gets $ move up$(S)$ by $\lsearchud_i$
		\State $S \gets \underset{S \in \{S_d,S_n,S_u\}}{\argmin} f(S)$
		\EndFor
		\For{$k \gets 2$ \textbf{to} $K$}
		\Comment iterate over output dimensions transform causal system back to input normal and anticausal to output normal 
		\State $S_r \gets $ move right$(S)$ by $\lsearchrl_i$
		\State $S_n \gets $ no move $(S)$ 
		\State $S_l \gets $ move left$(S)$ by $\lsearchrl_i$
		\State $S \gets \underset{S \in  \{S_r,S_n,S_l\}}{\argmin} f(S)$
		\EndFor
		\EndFor
	\end{algorithmic}
	\caption{Algorithm to optimize the segmentation}\label{alg:optimize segemtnation}
\end{algorithm}

To avoid that the algorithm gets stuck in local minima and to improve the speed, the algorithm does not change the segmentation by a fixed search distance $l$, but uses different search distances for every iteration of the algorithm.
If the matrices are non square it might also be needed to use a different search distances for the outputs $\lsearchud$ and and the input $\lsearchrl$ 

For most of the tests I used search distances of the form $l_i = l_02^{-i}$.
This gives a structure that is similar to a tree.
As the optimal segmentation of the input depends on the segmentation of the output, that is also subject to change, the sequence is constructed in a way that there is not a single way to reach a certain movement of a boundary.
%by gradually changing the segmentation but by facilitating a tree like structure.


As the unchanged version is also included, the sequence of objective functions is nonincreasing.
\begin{equation}
f(\sys_{l+1}) = \min(f(\sys_{l,\text{moved pos}}),f(\sys_{l}),f(\sys_{l,\text{moved neg}})) \leq f(\sys_{l})
\end{equation}

\section{Permutations}
if time allows add some notes on permutations here



\chapter{Experiments}



\section{Matrix Approximation}
\begin{itemize}
\item Test different approaches to represent matrices.
\item Have a look at the representation error in different norms for different algorithms
\end{itemize}
\section{Weight Matrix Approximation}
%\section{Setups}
eventually split this up in subsections

Setup: 
\begin{itemize}
	\item Some general notes on the tests.
	\item How define loss, images to features etc. 
	\item How measure accuracy
\end{itemize}

Tests:
\begin{itemize}
\item Test the different approaces in neural nets.
\end{itemize}
\chapter{Discussion}
4-5 4pages

\chapter{Conclusion}
1-2 pages



\the\textwidth



% Puts out the list of references
\printbibliography{}

\end{document}
