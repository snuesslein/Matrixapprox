% Possible types of documents/theses
%   doctype=bachelorsthesis
%   doctype=mastersthesis
%   doctype=idp
%   doctype=phdthesis
%   doctype=studienarbeit
%   doctype=diplomarbeit
%
% Document language
%   without 'lang' attribute: English
%   lang=ngerman:             German (new orthography)
%
% Binding correction
%   BCOR=<Längenangabe>
%   Additional margin, which is invisible due to binding the book
%   The usual binding by the Fachschaft has a thickness of 1,5 cm 
%
% biblatex (citations)
%   This requires 'biber' to be used instead of 'bibtex', please
%   adapt your editor's settings accordingly!
\documentclass[doctype=mastersthesis,BCOR=15mm,biblatex]{ldvbook}%lang=ngerman



\usepackage{pgfplots}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{matrix}

\usepackage{todonotes}


% Look for citation sources in the database "diplomarbeit.bib"
\addbibresource{thesis.bib}


%operator declarations
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\triu}{triu}
\DeclareMathOperator{\tril}{tril}

\begin{document}

% Bibliographic information about the thesis, please change accordingly!
\title{Combining Matrix Representations for Structured Approximation of Neural Network Weight Matrices}
\author{Stephan Nüßlein}
\license{CC-BY}
\supervisor{Matthias Kissel}


\maketitle[frontcover=Design1]


\chapter*{Abstract}

\begin{itemize}
	\item neural nets
	\item reduce cost of neural nets using structured matrix
	\item how to use Sequentially semisperable matrices in this context
	\item choose number of stages and tuning segmentation
	\item some sentence on results
\end{itemize}


\tableofcontents

60-80 pages total

% Please compile this example document including the bibliography
% database. Check the resulting document and the references for 
% correct appearance (especially the German Umlaute).
% Thus you ensure that LaTeX is detecting the character encoding
% correctly and your build chain is working.
% If it does not, please tell your supervisor.






\chapter{Introduction} 3p
Motivation Why

What others are dooing

What has been done here


Description structure


\chapter{Literature Review}


\section{Neural Nets}
%Special matrix structures are been explored for the use in neural nets.
%Here different matrices structures are used to improve the performance. 
In this section different approaches to used matrix structures in neural nets to reduce the computational are shortly introduced.
One widely known example are Convolutional neural networks (CNN).
The convolutions can be understood as a mutltiplication with a circulant matrix. 
Instead of using matrix vector products the underlying structure is used to replace the multiplications with convolutions.
One might analogously try to use the structure of a matrix to represent it with a different representation that has a reduced computational cost.
There are generally two approaches to obtain such a representation:
In one case the structure is predetermined and the parameters of the structure are trained by a regular training process.
Or the neural net is trained without a predetermined matrix structures and the matrices are later approximated with structured matrices. There are also approaches where the approximation is done during the training procedure. 

One widely known approach to reduce the complexity of neural nets is pruning. 
These are processes that set elements in neural net matrices to 0 to reduce the computational cost \cite{blalock_what_2020}.
A approach to remove parameters form a trained net based on the hessian was proposed in \cite{hassibi_optimal_1993}.
It is also possible to obtain sparse matrices while training.
In \cite{dettmers_sparse_2019} a pruning step is done during the training after every epoch.
A different approach is to obtain sparse matrices is including a regularization term.
In \cite{louizos_learning_2018,wen_learning_2016} this is done using a LASSO regularization.

Low Rank approximations for filters in convolutional layers were proposed in \cite{rigamonti_learning_2013}. Existing filters are approximated using low rank filters in \cite{jaderberg_speeding_2014}. 
In \cite{ioannou_training_2016} the low rank filters are learned directly and are later combined.
Recent work also use low rank and sparse decomposition. 
In \cite{yu_compressing_2017} matrices in deep models are represented as a sum of low rank and sparse matrices.
The computation of the decomposition is based on an algorithm form \cite{zhou_greedy_2013}.
To avoid an accumulation of the error due to the compression of multiple layers the decomposition are not done independently.
This is archived by defining the objective function as the difference between the outputs of the approximation for an collection of approximated input vectors and an the corresponding output vectors.

The structure of Hirarchical matrices will be explained in subsection\,\ref{subsec:H-mat}. These matrices have also been used in machine learning. 
In \cite{fan_multiscale_2019} $\mathcal{H}$-Matrices where used to represent solutions of partial differential equations. The structure of the neural net was designed in a way that makes use of the different scales in the system.
Related tensor decomposition methods were used in \cite{wu_hybrid_2020} to represent weight matrices and convolutional kernels.
In \cite{ithapu_decoding_2017} hierarchical factorization of covariance matrices is used to explore relationships between classes.

A different matrix structure explored in neural nets are butterfly factorization. 
These are products of sparse matrices that overall have a similar structure to the fast Fourier transform \cite{li_butterfly_2015,parker_random_1995}. These were used to represent dense layers in \cite{ailon_sparse_2021}.
In \cite{li_butterfly-net_2020} the structure of the butterfly factorization is combined with CNNs. These can be used to efficiently represent Fourier kernels.
A extension of butterfly factorization are kaleidoscope matrices (K-matrices) \cite{dao_kaleidoscope_2020}.
K-Matrices are product of factors of the shape $B_aB_b^\top$, where $B_a$ and $B_b$ are butterfly matrices. These can be used to represent a wide class of structured matrices.
The patterns in the factors are fixed. Therefore the parameters can be learned using gradient descent.
K-matrices can also be used to represent linear hand crafted structures like reprocessing steps. %example fft 

\todo[inline]{
Properties of dense matrices
Approxiamtions of dense Layer matricies}



\section{Matrix structures}
\todo[inline]{eventually move this section to Background}

\subsection{Matrix structures}
In the following certain matrix structures are presented.
These have in common that the rank of sub-matrices are important.
Here semiseperable, Hierarchical and Sequentially semiseparable matrices are presented.

\subsection{Semiseperable matricies}
Semiseperable matrices are not consistently defined in the literature. 
In this thesis the definitions described in \cite{vandebril_bibliography_2005,vandebril_matrix_2007} is used.
An important differentiation are generator representable semiseparable matrices and semiseparable matries.
\paragraph{Generator Representable Semiseparable Matrix}
A matrix $S$ is a generator representable semiseparable matrix if the lower and upper triangular parts of $S$ are taken from rank 1 matrices.
This can be expressed as 
\begin{align}
	\triu(S) &= \triu(pq^\top)\\
	\tril(S) &= \tril(uv^\top)
\end{align}
Where $\triu$ is the upper triangular matrix and $\tril$ is the lower triangular matrix. The vectors $p,q,u$ and $v$ are called the generators.
It is important to note here that the diagonal of $S$ is both included in the lower and upper triangular matrix.

\paragraph{Semiseparable Matrix}
In a semiseperable matrix every subblock selected from the lower triangular part of $S$ have rank 1. The analogous statement has to be fulfilled for the upper triangular part.
This can be formalized as 
\begin{align}
	\rank(S_{i:n,1:i}) &\leq 1 & \forall_i &\in\{i,\dots,n\}\\
	\rank(S_{1:i,i:n}) &\leq 1 & \forall_i &\in\{i,\dots,n\}
\end{align}
A extension of this matrix class are the semiseparable plus diagonal matrices.

\paragraph{Quasiseparable Matrix}
The quasiseparable matrices are similar to the semiseperable matries. In a quasiseparable matrix every subblock selected from the strictly lower strictly upper triangular part of $S$ have rank 1. 
This can be formalized as 
\begin{align}
\rank(S_{i+1:n,1:i}) &\leq 1 & \forall_i &\in\{i,\dots,n\}\\
\rank(S_{1:i,i+1:n}) &\leq 1 & \forall_i &\in\{i,\dots,n\}
\end{align}


If these matrices are invertible the semiseperable matrices are related to tridiagobal matrices.
The inverses of a generator representable semiseparable matrix is a irreducible tridiagonal matrix and vice versa. 
The inverse of a semiseparable matrix is a tridiagonal matrix and vice versa.
If a invertible quasiseparable is inverted, the inverse is again a quasiseparable matrix.
A quasiseperable matrix is illustrated in Figure\,\ref{fig:quasiseperable}. All the marked submatrices have the property that their rank is 1.
\begin{figure}
	\centering
	\input{diagrams/semiseperable}
	\caption{Illustration of a quasiseperable matrix}
	\label{fig:quasiseperable}
\end{figure}

These matrices classes can also be extended for higher ranks.
A matrix $S$ is a generator representable semiseparable matrix of semiseparability rank r if there exist the matrices $R_1$ and $R_2$ with $\rank(R_1)=r$ and $\rank(R_2)=r$ such that
\begin{align}
\triu(S) &= \triu(R_1)\\
\tril(S) &= \tril(R_2)
\end{align}

A similar definition for semiseparable matrices of semiseparability rank $r$ is given in \cite{vandebril_bibliography_2005}.
For this class of matrices and some slight alterations there are algorithms for the efficient calculation of different operations.

\subsection{Hirarchical Matrices}\label{subsec:H-mat}
The Hirarchical matrices are an approach to approximate large matrices. These were mainly introduced in \cite{grasedyck_theorie_2001,hackbusch_hierarchische_2009}.
The $\mathcal{H}$-Matrices where developed for the solution of PDEs.
The computational cost of matrix operations is polynomial. Even for a quadratic complexity the computational cost prohibits the use of large matrices for certain applications.

It PDEs are solved numerically, they have to be discreticed in order to obtain an approxiamted solution.
As the diescretizaon already introduces errors, it is advantageous to drop the requirement that the matrix representation is exact, if this results in a reduction of the computational cost.
This can be done by partitioning the matrix in segments. These blocks are represented by low rank matrices. It the rank is far smaller than the size of the matrix this representation is cheaper in terms of storage and in terms of computational cost.
For some small blockmatrices it might also be adeventagious to store the full blockmatrix.
To make this representation possible the size of the blockmatrices might have to be reduced.
This is done by dividing blocks that cannot be represented using a low rank representation.
To decide if a block has to be devided a admissabillity condition is introduced.
This is a way to predict the representability of a blockmatrix.
For discretizations of PDEs this admissibility condition is usually based on the geometrical diameter and distance of the involved sets.
For other applications different admissibility conditions have to be derived.


\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{diagrams/Struktur_H-matrix}
	\caption{Illustration of a $H$-Matrix and block cluster tree TODO: ggf Tikz-en}
	\label{fig:strukturh-matrix}
\end{figure}


The need for efficient operations also constraints the possible sizes of the blocks as these should be compatible.
The partition is done in a hierarchical fashion and the matrices are represented in a block-tree. 
If a block is not admissible, the block is divided in smaller block matrices.
If matrices are already small and still not admissible the matrices are stored directly.
Such a tree is illustrated in Figure\,\ref{fig:strukturh-matrix}.
In this case each non admissible block is divided in four subblocks. 
If a block is admissible, it is stored as a low rank representations.
These are illustrated as the white rectangles.
The filled rectangles illustrate the blocks that are stored directly.
The Hierarchical matrices make it possible to compute different matrix operations efficiently.


\subsection{Sequentially semiseperable}
Sequentially semiseperable matrices were introduced in \cite{dewilde_time-varying_1998}.
As the Sequentially semiseperable maticies will be used in this thesis a introduction will be given in chapter\,\ref{chap:background}. 
There it will be explained form the standpoint of time varying systems.
Here the commonalities and differences to the semiseperable and hirarchical matrices are explored.

The sequentially semiseperable matrices have the properties that submatrices have a low rank.
This is similar to semiseperable matrices. 
Unlike the semiseperable matrices this is not true for all matrices taken form the lower and upper triangular part.
The sequentially semiseperable matrices are divided into blocks.
Matrices taken form the strict lower and strict upper triangular blockmatrix have the condition that their rank is low.
This is illustrated in Figure\,\ref{fig:sequentiallysep}.

\begin{figure}
	\centering
	\input{diagrams/sequentially_sep}
	\caption{Illustration of a sequentially semiseperable matrix. The thick dotted lines mark the segemntation of the matrix. The thin lines represent submatrices with low rank.}
	\label{fig:sequentiallysep}
\end{figure}

This segmentation in blocks makes similar to the hirarchical Matrices that also have a segmentation.
Unlike semiseperable matrices the sequentially semiseperable matrices do not have to be quadratic. 

\chapter{Background/Introduction}\label{chap:background}
\begin{itemize}
	\item Sequentially semiseperable Matrices can be considered as descriptions of time varying systems.
	\item Illustrate system.
	
	\item Details on SSS systems
	
	\item Hankel Operator, Reachability, Observability Minimality
	\item Describe causal and anticausal part
	
	\item SSS as matrix factorization
\end{itemize}



\chapter{Methods} 15p

\section{Approximation of Matrices}
\subsection{Approximation algorithm}
\begin{itemize}
\item Discuss Approximation of a general system based on cutting the singular values of the Hankel matrix.

\item Discuss ordering of the sigmas, and cutting states if the sigmas are ordered.
\item Define ordered realization as extention of the ballanced realization

\item Describe how to get the sigmas for a system by transforming it.General idea: We do not have to compute the SVD of the total Hankel matrix, if we take a input-nomal system and convert it step by step to a output-normal.

\item Describe the overall approximation algorithm.
\end{itemize}
\subsection{Cost of computation}
\begin{itemize}
\item Short section on the computational cost and number of parameters.

\item Define it with and without additions.

\item Might also go in the Background, but I think it would be a bit to much and I did not find a detailed discussion in the literature. 
\end{itemize}
\subsection{Number of stages}
\begin{itemize}
\item Describe continuous surrogate problem for the cost depending on the number of parameters. 
\item Discuss educated guesses for the number of stages.
\end{itemize}

\section{Changing of Segmentation}
\begin{itemize}
\item Describe algorithm of moving boundaries.
\item First for causal system, then for mixed system.

\item Describe splitting and combining states.

\item Pseudocode for the causal systems

\item Move all the proves of minimality to the appendix for brevity.
\item Describe calculation of $\sigma$s
\end{itemize}
\section{Permutations}
if time allows add some notes on permutations here


\chapter{Experiment Setups}
4pages -> probably more 2 pages
\begin{itemize}
\item Some general notes on matrix representation.

\item How define loss, images to features etc. 
\item How measure accuracy
\end{itemize}
\chapter{Experiments}
10pages

\section{Tests of general approxiamtion}
\begin{itemize}
\item Test different approaches to represent matrices.
\item Have a look at the representation error in different norms for different algorithms
\end{itemize}
\section{Tests in neural Nets}
\begin{itemize}
\item Test the different approaces in neural nets.
\end{itemize}
\chapter{Discussion}
4-5 4pages

\chapter{Conclusion}
1-2 pages







% Puts out the list of references
\printbibliography{}

\end{document}
